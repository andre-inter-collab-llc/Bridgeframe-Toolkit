[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bridgeframe",
    "section": "",
    "text": "Welcome\nBridgeframe is a practical toolkit for professionals who work at the intersection of information technology and public health. Whether you are a business analyst stepping into your first health department project, or a public health professional learning to collaborate with software teams, this book provides the translation layer you need.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#the-challenge",
    "href": "index.html#the-challenge",
    "title": "Bridgeframe",
    "section": "0.1 The Challenge",
    "text": "0.1 The Challenge\nTwo disciplines. Two languages. One shared goal: building systems that improve health outcomes.\nBusiness analysts speak of user stories, sprints, and requirements traceability. Public health professionals speak of logic models, PDSA cycles, and program evaluation. Both are trying to define problems, design solutions, and measure success, yet their terminology creates friction rather than collaboration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#the-solution",
    "href": "index.html#the-solution",
    "title": "Bridgeframe",
    "section": "0.2 The Solution",
    "text": "0.2 The Solution\nLogic Model Components Mapped to Requirements Bridgeframe provides:\n\nA terminology dictionary mapping IT/Agile concepts to their public health equivalents\nPhase-by-phase guidance aligning the BABOK lifecycle with CDC frameworks\nA running case study (CancerSurv) demonstrating concepts in practice\nTemplates and tools for hybrid teams",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Bridgeframe",
    "section": "0.3 Who This Book Is For",
    "text": "0.3 Who This Book Is For\n\nIT Business Analysts entering the public health sector\nPublic Health Informaticians collaborating with software vendors\nProject Managers overseeing health IT implementations\nData Scientists working with epidemiological systems\nStudents in health informatics or public health programs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Bridgeframe",
    "section": "0.4 How to Use This Book",
    "text": "0.4 How to Use This Book\nThis book is organized into three parts:\n\nFoundations: Core concepts, terminology mapping, and the CancerSurv case study\nThe Analysis Process: Phase-by-phase guidance from planning through evaluation\nPutting It Into Practice: Tools comparison and implementation science frameworks\n\nEach chapter includes CancerSurv examples in callout boxes, making abstract concepts concrete.\n\n\n\n\n\n\nTipGetting Started\n\n\n\nIf you are new to this intersection, start with Chapter 1: Introduction and the Terminology Dictionary. If you are already working on a project, jump to the relevant phase chapter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Bridgeframe",
    "section": "0.5 About the Author",
    "text": "0.5 About the Author\nAndré van Zyl, MPH is an epidemiologist and data science professional with close to two decades of experience spanning public health, health informatics, and technical system development. His career has taken him across local, state, federal, tribal, and international health systems, from helping establish global surveillance systems at the CDC to implementing health interventions in resource-constrained settings.\nAs a Health Scientist at CDC’s National Center for Emerging and Zoonotic Infectious Diseases, André led data acquisition and reporting systems for global antimicrobial resistance surveillance spanning multiple continents. He pioneered the integration of artificial intelligence into public health workflows and modernized data processes using R, Python, Azure Databricks, and platforms like REDCap and DHIS2. A consistent theme throughout his work has been technical translation: bridging communication gaps between laboratory scientists, clinical teams, and technical developers.\nAndré holds a Master of Public Health from the University of Pretoria and a BA Honors in Psychology from Nelson Mandela University. This combination of behavioral science and technical expertise enables him to understand both user needs and system requirements when implementing solutions across diverse communities.\nHe is the founder of Intersect Collaborations LLC, a consultancy helping public health organizations transform data systems and analytics capabilities for improved decision-making and community health outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "2.1 The Translation Challenge\nThroughout my career, I’ve watched brilliant teams stumble, not because they lacked expertise, but because they were speaking entirely different professional languages.\nMy formal training is in social sciences and public health, where business analysis concepts weren’t part of the curriculum. But my career path led me through various technical roles (working for technology companies and alongside them) where I absorbed the terminology, tools, software, and processes that business analysts and project managers use daily. This dual exposure showed me both the value and the difficulty of blending these perspectives.\nWhen I joined projects bridging IT and public health, I saw the same friction points repeatedly:\nThe IT world optimizes for efficiency and profit. Public health optimizes for equity and outcomes. Both are rigorous disciplines with systematic frameworks. Yet when they collaborate, the translation gap creates delays, misaligned expectations, and sometimes outright project failure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#the-translation-challenge",
    "href": "preface.html#the-translation-challenge",
    "title": "2  Preface",
    "section": "",
    "text": "Business analysts asking for “user stories” while epidemiologists stared blankly\nDevelopers requesting “non-functional requirements” while program managers wondered if that related to grant compliance\nAgile sprints clashing with PDSA cycles or action research, even though both are iterative improvement frameworks\nStakeholder maps that missed community partners because they weren’t “decision-makers” in the traditional IT sense",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#lessons-from-both-sides",
    "href": "preface.html#lessons-from-both-sides",
    "title": "2  Preface",
    "section": "2.2 Lessons from Both Sides",
    "text": "2.2 Lessons from Both Sides\nI’ve experienced this challenge from multiple angles:\nIntroducing IT tools to public health teams. I’ve worked with groups who had strong public health backgrounds and wanted to streamline collaboration, tracking, and communication. I introduced project management software (including Azure DevOps, because it was the best tool available to us). But I ran into constant friction translating the software’s terminology (sprints, agile, user stories, backlogs) into concepts that made sense for public health professionals and their workflows.\nWatching tech-dominated engagements produce suboptimal products. I’ve also seen the reverse: public health organizations relying heavily on technology vendors where the frameworks, project management styles, and terminology of business analysts dominated the engagement. The public health perspective got lost, and the resulting products didn’t fit how health departments actually operate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#finding-common-ground",
    "href": "preface.html#finding-common-ground",
    "title": "2  Preface",
    "section": "2.3 Finding Common Ground",
    "text": "2.3 Finding Common Ground\nBridgeframe exists because I needed it. Every time I translated a clinical workflow into a requirements document, or explained why a “user story” doesn’t capture a patient journey, I wished for a reference guide that mapped these concepts clearly.\nThis toolkit distills what I’ve learned from CDC surveillance systems, COVID-19 contact tracing implementations, tuberculosis intervention trials, and countless cross-functional team meetings. It’s designed for the health informatician sitting between two worlds: ensuring the IT team’s technical specifications align with the health department’s programmatic goals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#a-work-in-progress",
    "href": "preface.html#a-work-in-progress",
    "title": "2  Preface",
    "section": "2.4 A Work in Progress",
    "text": "2.4 A Work in Progress\nI want to be clear: I am not positioning myself as an expert on business analysis, and Bridgeframe is not intended as an authoritative guide. This is a draft that I will continue to develop and refine.\nThe examples throughout this book (including the CancerSurv case study) are illustrative and fictitious. They are designed to be relatable, to help professionals in both domains recognize common patterns and challenges. The intent is educational and informative.\nMy hope is that Bridgeframe stimulates discussion and points readers toward the established frameworks and resources that offer deeper expertise: BABOK for business analysis, CDC evaluation frameworks for public health, CFIR for implementation science, and the many other rigorous methodologies developed by true experts in these fields.\nIf this toolkit helps even a few teams find common ground, or sparks conversations that improve how we build health information systems, it will have served its purpose. I welcome feedback, corrections, and contributions as this resource evolves.\nAndré van Zyl\nIntersect Collaborations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 Why Bridgeframe Exists\nPublic health and information technology are increasingly intertwined. Disease surveillance systems, immunization registries, electronic lab reporting, and health analytics platforms all require collaboration between two groups that often struggle to understand each other: business analysts and public health professionals.\nThis chapter explains the problem, introduces the Bridgeframe approach, and sets the stage for the detailed guidance that follows.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-bridgeframe-exists",
    "href": "chapters/01-introduction.html#why-bridgeframe-exists",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1.1 The Silo Problem\nConsider a typical scenario: A state health department receives funding to modernize its disease surveillance system. They contract with a software vendor whose project team includes experienced business analysts, developers, and testers. The health department brings epidemiologists, program managers, and data analysts. Both sides are competent in their respective domains. Yet within weeks, the project is mired in confusion.\nThe vendor’s BA asks for “user stories.” The epidemiologist provides a detailed case definition document. The BA politely explains that a case definition is not a user story. The epidemiologist wonders why the BA keeps asking about “acceptance criteria” when the CDC already publishes validation rules.\nThis is not a failure of intelligence or good faith. It is a failure of translation.\n\n\n3.1.2 Two Parallel Worlds\nBusiness analysis, as codified in the BABOK (Business Analysis Body of Knowledge), provides a rigorous framework for understanding needs, defining requirements, and ensuring solutions deliver value1. Public health analysis, guided by CDC frameworks and epidemiological methods, provides an equally rigorous approach to assessing community needs, designing interventions, and evaluating outcomes2,3.\nThese frameworks are parallel, not incompatible:\n\n\n\nBusiness Analysis\nPublic Health\n\n\n\n\nStakeholder Analysis\nCommunity Partner Mapping\n\n\nCurrent State Analysis\nEpidemiological Baseline\n\n\nRequirements Specification\nProgram Protocol\n\n\nSolution Design\nIntervention Design\n\n\nImplementation\nProgram Rollout\n\n\nEvaluation\nProgram Evaluation\n\n\n\nThe concepts align; the terminology diverges. Bridgeframe provides the translation layer.\n\n\n3.1.3 The Cost of Miscommunication\nWhen BA and PH professionals cannot communicate effectively, projects suffer:\nInability to prove impact: Systems that cannot demonstrate public health value\nMore importantly, health outcomes suffer. A delayed surveillance system means delayed outbreak response. A poorly designed immunization registry means children missing vaccines. And increasingly, programs that cannot demonstrate measurable public health impact risk losing funding.\nBoth technical and public health teams share responsibility for proving the value of their work. Technology investments must translate to demonstrable improvements in population health, not just features delivered on time and on budget.\n\n\n3.1.4 The Bridgeframe Solution\nBridgeframe offers a structured approach to bridging these domains:\n\nCommon Vocabulary: Chapter 3 provides a comprehensive dictionary mapping BA terms to PH equivalents\nPhase Alignment: Chapters 4-9 walk through each lifecycle phase, showing how BA activities map to PH frameworks\nPractical Examples: The CancerSurv case study (Chapter 2) demonstrates concepts in action\nImplementation Science: Chapter 11 introduces CFIR and other frameworks for addressing adoption barriers\n\n\n\n3.1.5 Start with the Client’s Framework\nBefore diving into requirements gathering or sprint planning, a critical first question must be answered: What frameworks does the client already use?\nPublic health organizations operate within established methodological traditions. They may follow CDC evaluation frameworks, use logic models for program planning, apply CFIR for implementation readiness, or adhere to agency-specific protocols. These are not obstacles to be worked around; they are assets to be leveraged.\nWhen engaging with a public health client:\n\nAsk early: In the first meetings, explicitly ask what frameworks, methodologies, or standards guide their work\nPrioritize existing frameworks: If the client has established approaches, adapt your deliverables to align with them rather than imposing unfamiliar structures\nTranslate, do not replace: When BA frameworks offer value the client lacks, introduce them by mapping to familiar concepts. Present a “logic model with software requirements” rather than demanding they learn BABOK terminology\nDocument the bridge: Create explicit crosswalks showing how your deliverables map to their frameworks\n\n\n\n\n\n\n\nTipPractical Guidance\n\n\n\nIf you are the business analyst or developer, your role is translation, not conversion. A requirements document that aligns with the client’s existing CDC program evaluation framework will gain faster buy-in than one that requires the client to learn Agile terminology. Meet them where they are.\n\n\nThis principle works both ways. Public health professionals engaging with IT vendors should communicate their frameworks early, providing documentation and context so the technical team can adapt their approach accordingly.\n\n\n3.1.6 When to Introduce the Translation\nA common pattern across public health IT projects: translation challenges emerge during software requirements gathering, by which point significant momentum (and sometimes conflict) has already built up. The teams discover they are speaking different languages only after the project is well underway.\nThe earlier translation happens, the better the outcome.\nIdeally, translation should begin during the business case development that justifies funding. When writing grant applications or funding proposals, explicitly address:\n\nHow IT terminology maps to programmatic outcomes\nWhat resources (training, facilitation, documentation) will be needed for cross-domain communication\nWho will serve as translators or bridges between teams\n\nThis early attention accomplishes two things: it surfaces potential friction points before they become costly, and it ensures that resource discovery includes the human and process elements needed for successful collaboration, not just technical requirements.\n\n\n\n\n\n\nTipPractical Guidance\n\n\n\nIf you are writing a grant proposal or business case for a health IT project, include a line item for “cross-domain facilitation” or “terminology alignment workshops.” This signals awareness of the challenge and secures resources to address it.\n\n\nWhen translation training happens before or alongside business case development, teams enter requirements gathering with shared vocabulary and mutual understanding. When it happens only during software requirements definition, teams must untangle miscommunication while simultaneously trying to make progress.\n\n\n3.1.7 Who Should Use This Book\nThis book serves multiple audiences:\nFor IT Business Analysts entering public health:\n\nLearn the regulatory context (HIPAA, CDC reporting requirements)\nUnderstand grant-driven funding and its implications\nAdapt Agile practices for public health workflows\n\nFor Public Health Professionals working with IT teams:\n\nTranslate your needs into requirements language\nParticipate effectively in sprint planning and reviews\nEvaluate vendor proposals with confidence\n\nFor Project Managers and Leaders:\n\nBuild teams with shared vocabulary\nAnticipate common friction points\nStructure projects for hybrid success\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nThroughout this book, we follow the CancerSurv project: a state health department modernizing its cancer registry system. In the Introduction, the challenge is framed as both a Business Need (replace aging mainframe with modern cloud platform) and a Public Health Challenge (ensure timely, complete cancer data for prevention planning and disparity identification).\n\n\n\n\n3.1.8 What Bridgeframe Is Not\nThis book does not replace domain-specific training. Business analysts should still study the BABOK and pursue relevant certifications. Public health professionals should still learn epidemiology, biostatistics, and program evaluation. Bridgeframe provides the bridge between these bodies of knowledge, not a substitute for them.\nSimilarly, this book does not cover software development practices, database design, or clinical medicine in depth. It focuses specifically on the analysis phase: understanding needs, defining requirements, and evaluating solutions.\n\n\n3.1.9 Moving Forward\nThe following chapter introduces CancerSurv in detail, providing the context you will need to engage with examples throughout the book. Chapter 3 then provides the terminology dictionary, a reference you will return to often. From there, we proceed phase by phase through the analysis lifecycle.\nWelcome to Bridgeframe. Let us build this bridge together.\n\n\n\n\n\n\n1. Iiba. Babok: A Guide to the Business Analysis Body of Knowledge. International Institute of Business Analysis; 2015. \n\n\n2. CDC. CDC Program Evaluation Framework [Internet]. CDC Approach to Program Evaluation. 2025 [cited 2026 Jan 9]. Available from: https://www.cdc.gov/evaluation/php/evaluation-framework/index.html\n\n\n3. Kidder DP. CDC Program Evaluation Framework, 2024. MMWR Recommendations and Reports [Internet]. 2024 [cited 2026 Jan 9];73. Available from: https://www.cdc.gov/mmwr/volumes/73/rr/rr7306a1.htm",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-case-study.html",
    "href": "chapters/02-case-study.html",
    "title": "4  Meet CancerSurv",
    "section": "",
    "text": "4.1 The CancerSurv Case Study\nThroughout this book, we use a single comprehensive case study to illustrate concepts: CancerSurv, a cloud-based cancer surveillance and registry system. This chapter introduces the project context, stakeholders, and objectives that will appear in examples across all subsequent chapters.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meet CancerSurv</span>"
    ]
  },
  {
    "objectID": "chapters/02-case-study.html#the-cancersurv-case-study",
    "href": "chapters/02-case-study.html#the-cancersurv-case-study",
    "title": "4  Meet CancerSurv",
    "section": "",
    "text": "4.1.1 Project Overview\nA state public health department partners with TechHealth Solutions, a health IT company, to develop CancerSurv: a modern, cloud-based platform for cancer surveillance and registry operations.\n\n4.1.1.1 The Business Context\n\n\n\n\n\n\n\nAttribute\nDetail\n\n\n\n\nPublic Health Partner\nState Cancer Registry (Department of Health)\n\n\nTechnology Partner\nTechHealth Solutions (cloud software vendor)\n\n\nFunding Source\nCDC National Program of Cancer Registries (NPCR) grant\n\n\nTimeline\n18-month phased implementation\n\n\nScope\nReplace legacy mainframe system with modern cloud solution\n\n\n\n\n\n4.1.1.2 The Public Health Context\nCancer registries serve a critical public health function. They collect, process, and analyze data on cancer incidence, treatment, and outcomes. This data informs:\n\nPrevention program targeting\nEarly detection initiatives\nHealth disparity identification\nResearch and clinical trials\nHealthcare resource planning\n\nThe state’s current system, built on 1990s mainframe technology, cannot meet modern demands for interoperability, real-time analytics, and remote access.\n\n\n\n4.1.2 Stakeholder Landscape\nUnderstanding who participates in this project requires seeing stakeholders through both BA and PH lenses:\n\n\n\n\n\n\n\n\n\nRole\nBA Term\nPH Term\nKey Concerns\n\n\n\n\nProject Sponsor\nExecutive Sponsor\nRegistry Director / State Epidemiologist\nBudget, timeline, CDC compliance\n\n\nEnd Users\nSystem Users\nCancer Registrars, Epidemiologists, Data Analysts\nUsability, efficiency, data quality\n\n\nSubject Matter Experts\nBusiness SMEs\nOncologists, Pathologists, Tumor Board Members\nClinical accuracy, coding standards\n\n\nExternal Partners\nVendors / Integrators\nHospitals, Laboratories, Vital Records\nData submission, interoperability\n\n\nOversight Bodies\nGovernance Board\nNPCR Program, NAACCR Standards Committee\nStandards compliance, data quality metrics\n\n\n\n\n\n\n\n\n\nImportantTerminology Note\n\n\n\nWhile we use “stakeholder” here for its familiarity in BA contexts, public health practitioners often prefer alternatives: community partners, interest holders, rights holders, or beneficiaries. The choice of term signals values: “stakeholder” implies an interest or stake, while “rights holder” acknowledges inherent claims and dignity. See Chapter 3 for detailed discussion.\n\n\n\n\n4.1.3 System Functions\nCancerSurv must support five core functional areas:\n\n4.1.3.1 Case Abstraction\nCancer registrars enter and code cancer cases from medical records. This involves:\n\nExtracting relevant data from pathology reports, discharge summaries, and treatment records\nCoding diagnoses using ICD-O-3 (International Classification of Diseases for Oncology)\nStaging tumors using TNM and SEER summary staging\nLinking cases to patients across multiple treatment facilities\n\n\n\n4.1.3.2 Data Quality\nAutomated processes ensure data completeness and accuracy:\n\nEdit checks flagging inconsistent or missing data\nDuplicate detection identifying potential duplicate case records\nLinkage to vital records for death clearance\nInter-rater reliability monitoring\n\n\n\n4.1.3.3 Reporting\nThe registry must meet external reporting requirements:\n\nAnnual submissions to NPCR (National Program of Cancer Registries)\nData exchange with SEER (Surveillance, Epidemiology, and End Results Program)\nAd-hoc queries for researchers (with appropriate IRB approval)\nPublic health reports for state legislature and media\n\n\n\n4.1.3.4 Analytics Dashboard\nModern surveillance requires real-time analytics:\n\nCancer incidence trends by site, stage, and demographics\nGeographic mapping of cancer clusters\nSurvival analysis and outcomes tracking\nHealth disparity indicators\n\n\n\n4.1.3.5 Interoperability\nCancerSurv must integrate with external systems:\n\nHL7 FHIR APIs for hospital EHR integration\nElectronic pathology reporting from laboratories\nVital records linkage for death data\nNational data exchange protocols\n\n\n\n\n4.1.4 Project Phases\nThe 18-month implementation follows a phased approach, which we will revisit throughout the book:\n\n\n\n\n\n\ngantt\n    title CancerSurv Implementation Timeline\n    dateFormat  YYYY-MM\n    section Planning\n    Needs Assessment       :2025-01, 2M\n    Requirements Gathering :2025-02, 3M\n    section Development\n    Core Platform          :2025-04, 4M\n    Interoperability       :2025-07, 3M\n    Analytics              :2025-09, 2M\n    section Deployment\n    Pilot Testing          :2025-10, 2M\n    Training & Rollout     :2025-11, 3M\n    section Evaluation\n    Post-Implementation    :2026-01, 3M\n\n\n\n\nFigure 4.1: CancerSurv Implementation Phases\n\n\n\n\n\n\n\n4.1.5 How CancerSurv Appears in This Book\nEach chapter includes CancerSurv examples demonstrating concepts in practice:\n\nPlanning (Chapter 4): Needs assessment comparing cancer data gaps with CDC reporting requirements\nElicitation (Chapter 5): User stories from registrars; clinical guidelines from oncologists\nRequirements (Chapter 6): Functional specifications for case entry; NFRs for HIPAA compliance\nDesign (Chapter 7): System architecture; CFIR implementation readiness assessment\nImplementation (Chapter 8): Agile sprints mapped to grant milestones; PDSA cycles for workflow adoption\nEvaluation (Chapter 9): KPIs (data completeness ≥95%) mapped to health outcomes\n\n\n\n\n\n\n\nTipUsing the Case Study\n\n\n\nWhen reading subsequent chapters, refer back to this overview to ground abstract concepts in the CancerSurv context. The case study makes the BA-PH bridge tangible.\n\n\n\n\n4.1.6 The Dual Mandate\nCancerSurv illustrates the fundamental tension bridged by this book. The project must simultaneously satisfy:\nTechnology Requirements:\n\nModern cloud architecture\nAPI-first design\nAgile delivery methodology\nVendor best practices\n\nPublic Health Requirements:\n\nCDC/NPCR compliance\nNAACCR data standards\nHIPAA security\nHealth equity focus\n\nSuccess requires translation between these worldviews. That is what Bridgeframe provides.\n\n\n4.1.7 Shared Responsibility: Proving Public Health Value\nBoth teams in the CancerSurv project (TechHealth Solutions’ business analysts and the State Cancer Registry’s public health professionals) share a critical responsibility: proving the public health value and impact of the solution they develop.\nThis is not merely about meeting technical specifications. Both teams must prove the public health value of their work through measurable outcomes and return on investment. In an era of constrained budgets, demonstrating tangible health impact has become essential for program survival.\nChapter 4 explores how to establish these success metrics during the Planning phase, ensuring that both technical deliverables and programmatic outcomes are measured from the start.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meet CancerSurv</span>"
    ]
  },
  {
    "objectID": "chapters/03-terminology.html",
    "href": "chapters/03-terminology.html",
    "title": "5  Terminology Dictionary",
    "section": "",
    "text": "5.1 The BA-PH Translation Guide\nThis chapter provides a comprehensive mapping between Business Analysis (BA) terminology and Public Health (PH) equivalents. Use this as a reference throughout your work on hybrid projects.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Terminology Dictionary</span>"
    ]
  },
  {
    "objectID": "chapters/03-terminology.html#the-ba-ph-translation-guide",
    "href": "chapters/03-terminology.html#the-ba-ph-translation-guide",
    "title": "5  Terminology Dictionary",
    "section": "",
    "text": "5.1.1 Core Process Mapping\nThe BA lifecycle aligns with public health program phases:\n\n\n\n\n\n\n\n\nBA / BABOK Phase\nPublic Health Equivalent\nKey Activities\n\n\n\n\nStrategy Analysis\nCommunity Health Assessment\nDefine the problem using epidemiological data vs business metrics\n\n\nRequirements Analysis\nData Analysis & Logic Models\nModel processes, define indicators\n\n\nSolution Evaluation\nProgram Evaluation (CDC Framework)\nMeasure outcomes against targets\n\n\nChange Management\nImplementation Science\nCFIR, RE-AIM for adoption barriers\n\n\n\n\n\n5.1.2 The Complete BA-PH Process Workflow\nThe following diagram illustrates the six-phase analysis process covered in Part II of this book, showing how each BA phase maps to its Public Health equivalent. Note the iterative feedback loop from evaluation back to planning, reflecting the continuous improvement philosophy shared by both domains.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#2494f7', 'primaryTextColor': '#ffffff', 'primaryBorderColor': '#01272f', 'lineColor': '#777777', 'secondaryColor': '#00a4bb', 'tertiaryColor': '#01272f', 'edgeLabelBackground': '#ffffff'}}}%%\nflowchart TB\n    P1[\"&lt;b&gt;Ch 6: Planning & Strategy (BA) | Needs Assessment (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: business need, current state, future state, stakeholder identification&lt;br/&gt;PH Analog: public health challenge, epidemiological baseline, program goals&lt;br/&gt;Frameworks: SMART, RACI, SIPOC, 5 Whys, Fishbone&lt;br/&gt;Artifacts: business case, needs assessment, stakeholder map, current-state analysis\"]\n\n    P2[\"&lt;b&gt;Ch 7: Elicitation (BA) | Stakeholder Engagement (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: interviews, workshops, document analysis, observation, prototyping&lt;br/&gt;PH Analog: key informant interviews, focus groups, community engagement&lt;br/&gt;Frameworks: facilitation techniques, participatory research methods&lt;br/&gt;Artifacts: interview notes, workshop outputs, elicitation findings\"]\n\n    P3[\"&lt;b&gt;Ch 8: Requirements Analysis (BA) | Data Analysis & Logic Models (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: functional requirements, NFRs, data requirements, business rules&lt;br/&gt;PH Analog: program activities, implementation characteristics, case definitions&lt;br/&gt;Frameworks: MoSCoW, INVEST, UML, ERD; Logic Model, Theory of Change&lt;br/&gt;Artifacts: requirements specification, data dictionary, logic model, RTM\"]\n\n    P4[\"&lt;b&gt;Ch 9: Design & Solution Definition (BA) | Intervention Design (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: solution architecture, interface design, integration design&lt;br/&gt;PH Analog: intervention framework, program design, service delivery model&lt;br/&gt;Frameworks: UML, CFIR implementation planning&lt;br/&gt;Artifacts: architecture diagrams, prototypes, implementation strategy\"]\n\n    P5[\"&lt;b&gt;Ch 10: Implementation (BA) | Program Execution (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: sprint, release management, UAT, go-live, defect management&lt;br/&gt;PH Analog: PDSA cycle, phased rollout, pilot evaluation, program launch&lt;br/&gt;Frameworks: Agile/Scrum, Kanban; PDSA, implementation science&lt;br/&gt;Artifacts: release notes, test results, training materials, change log\"]\n\n    P6[\"&lt;b&gt;Ch 11: Evaluation (BA) | Program Evaluation & QI (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: solution evaluation, KPI tracking, ROI, lessons learned&lt;br/&gt;PH Analog: program evaluation, health indicators, cost-effectiveness, QI&lt;br/&gt;Frameworks: CDC Evaluation Framework, RE-AIM, PDCA/PDSA&lt;br/&gt;Artifacts: KPI dashboard, evaluation report, after-action review\"]\n\n    P1 --&gt; P2\n    P2 --&gt; P3\n    P3 --&gt; P4\n    P4 --&gt; P5\n    P5 --&gt; P6\n    P6 -.-&gt;|\"Iterate and Improve\"| P1\n\n    linkStyle 5 stroke:#333333\n\n\n\n\nFigure 5.1: Business Analyst Process Mapped to Public Health Analyst Workflow\n\n\n\n\n\nEach phase produces artifacts and applies frameworks that translate across domains. The following sections detail the terminology mappings for each phase.\n\n\n5.1.3 Terminology Translation Table\n\n5.1.3.1 Planning & Strategy Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nBusiness Need / Business Case\nPublic Health Challenge / Health Need\nIn PH, the driver for change is health equity or social determinants, not profit\n\n\nCurrent State Analysis\nEpidemiological Baseline\nDocument existing conditions: disease incidence, service gaps, health disparities\n\n\nFuture State / Vision\nProgram Goals & Intended Outcomes\nDefine success via improved health indicators, not market share\n\n\nConstraints / Assumptions\nSocial Determinants / Policy Constraints\nInclude funding limitations, regulatory requirements (HIPAA), cultural barriers, equity considerations\n\n\n\n\n\n\n5.1.4 Stakeholder & Communication Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nStakeholder\nInterest Holder / Community Partner / Rights Holder\nSee detailed note below on terminology considerations\n\n\nStakeholder Analysis\nCommunity Partner Mapping\nIdentify power dynamics, health equity implications\n\n\nRequirements Workshop\nCommunity Engagement Session\nPH emphasizes participatory approaches and cultural competency\n\n\n\n\n\n\n\n\n\nWarningA Note on ‘Stakeholder’ Terminology\n\n\n\nWhile “stakeholder” is standard in BA/Agile contexts (and used throughout this book for clarity), it carries colonial connotations in public health settings. The term evokes land claims and power imbalances, potentially disempowering Indigenous peoples and marginalized communities.\nPreferred alternatives in public health contexts:\n\nGeneral Use: Interest Holders, Parties/Affected Parties, Beneficiaries, Actants\nAction-Oriented: Constituents, Key Informants, Knowledge Users, Rights Holders\n\nWhen writing for mixed audiences, acknowledge both terms. When writing for public health audiences exclusively, favor the alternatives.\n\n\n\n5.1.4.1 Requirements & Analysis Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nUser Story\nService-User Scenario / GPS Format\nGPS = “Given [context], the Person [role] Should [action]” for clinical settings\n\n\nEpic\nGrant Objective / Program Goal\nHigh-level outcome (e.g., “Reduce TB incidence by 10%”)\n\n\nRequirements\nProgram Protocols / Clinical Guidelines\nBusiness rules are often legally or clinically mandated in PH\n\n\nNFRs (Non-Functional Requirements)\nImplementation Characteristics (CFIR)\nScalability = Outbreak Resilience; Security = HIPAA/Trust\n\n\nProcess Model (BPMN)\nIntervention Flowchart / Logic Model\nVisualize Inputs → Activities → Outputs → Outcomes\n\n\nData Model / Schema\nCase Definition / Data Dictionary\nER diagrams vs epidemiological case criteria\n\n\nUse Case Diagram\nPatient Journey Map\nUML diagrams vs mapping patient experience through care continuum\n\n\n\n\n\n5.1.4.2 Agile & Iteration Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nSprint\nPDSA Cycle / Adaptive Management\nPlan-Do-Study-Act: cyclic improvement framework\n\n\nBacklog\nWorkplan / Action Items\nPrioritized list of features vs outreach tasks\n\n\nSprint Review / Demo\nProgress Reporting\nOften aligned with grant reporting periods\n\n\nRetrospective\nAfter-Action Review\nSystematic reflection on what worked and what needs improvement\n\n\n\n\n\n5.1.4.3 Quality & Evaluation Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nKPI (Key Performance Indicator)\nHealth Indicator\n“15% improvement in customer satisfaction” vs “10% reduction in infection rate”\n\n\nAcceptance Test Plan\nEvaluation Protocol\nTest cases vs data collection and analysis plan\n\n\nQuality Assurance\nQuality Improvement (QI)\nSystematic checks, continuous improvement cycles\n\n\nBug / Defect\nAdverse Event / Variance\nSystem error vs deviation from expected health outcome\n\n\nLessons Learned\nAfter-Action Review\nRetrospective analysis, sharing successes and gaps across organization\n\n\n\n\n\n5.1.4.4 Design & Implementation Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nPrototype / Mockup\nPilot Study / Field Test\nSoftware wireframe vs PH intervention pilot in limited population\n\n\nRisk Analysis\nCommunity Risk Assessment\nStandard risk assessment vs PH frameworks (CFIR, RE-AIM)\n\n\nGo-Live\nProgram Launch / Rollout\nSystem deployment vs intervention implementation\n\n\nTraining Plan\nCapacity Building\nEnd-user training vs workforce development\n\n\n\n\n\n\n5.1.5 Alternative User Story Formats for Public Health\nThe standard “As a [user], I want [feature], so that [benefit]” format often fails in clinical contexts. Use these alternatives:\n\n5.1.5.1 GPS Format (Given-Person-Should)\n\n“Given [clinical context], the [health worker role] should [specific action] to [health outcome].”\n\nExample:\n\n“Given a positive TB test result, the contact tracer should initiate household investigation within 48 hours to prevent secondary transmission.”\n\n\n\n5.1.5.2 Service-User Scenario\nA narrative vignette describing a patient’s journey through the system:\n\n“Maria, a 45-year-old farmworker, visits a mobile clinic for diabetes screening. She speaks primarily Spanish and has no regular primary care provider. The system must support her preferred language, connect her to follow-up care, and track her screening results for population health reporting.”\n\n\n\n5.1.5.3 Situational Protocol\nContext-specific workflow tied to clinical guidelines:\n\n“When a laboratory reports a confirmed measles case, the system shall generate a contact list and notify the assigned epidemiologist within 4 hours.”\n\n\n\n\n5.1.6 The Logic Model as Requirements Framework\nIn public health, the Logic Model serves a similar purpose to a requirements specification. Understanding this mapping helps BA professionals communicate with PH colleagues:\n\n\n\n\n\n\nflowchart LR\n    A[Inputs&lt;br/&gt;Resources] --&gt; B[Activities&lt;br/&gt;What we do]\n    B --&gt; C[Outputs&lt;br/&gt;Direct products]\n    C --&gt; D[Outcomes&lt;br/&gt;Short-term]\n    D --&gt; E[Impact&lt;br/&gt;Long-term]\n\n\n\n\nFigure 5.2: Logic Model Structure\n\n\n\n\n\n\n\n\nLogic Model Component\nBA Equivalent\n\n\n\n\nInputs\nResources, Constraints, Assumptions\n\n\nActivities\nFunctional Requirements, Use Cases\n\n\nOutputs\nDeliverables, System Features\n\n\nOutcomes\nSuccess Metrics, KPIs\n\n\nImpact\nBusiness Value, Strategic Objectives\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nIn CancerSurv, the Logic Model framing helped translate between teams:\n\nInput: Grant funding, registrar staff, hospital data feeds\nActivity: Case abstraction, data quality checks, interoperability\nOutput: Complete case records, quality reports, data submissions\nOutcome: 95% data completeness, timely NPCR submissions\nImpact: Accurate survival statistics, targeted prevention resources\n\n\n\n\n\n5.1.7 Data Standards as Requirements\nIn public health IT, data standards are non-negotiable requirements, not optional “technical details”:\n\n\n\n\n\n\n\n\nStandard\nPurpose\nBA Implication\n\n\n\n\nHL7\nMessaging between systems\nDefine trigger events (e.g., “ADT A01, Admit Patient”)\n\n\nFHIR\nModern API-based exchange\nSpecify FHIR Resources (Patient, Observation)\n\n\nUSCDI\nFederal data interoperability\nRequired for ONC certification\n\n\nICD-10 / ICD-O-3\nDiagnosis coding\nValidation rules in requirements\n\n\nSNOMED CT\nClinical terminology\nConcept mapping specifications\n\n\nLOINC\nLab test coding\nInterface specifications\n\n\n\n\n\n5.1.8 Data Architecture: Medallion Architecture for Public Health\nModern data platforms often use a medallion architecture, a design pattern that organizes data into progressively refined layers: Bronze, Silver, and Gold. Originally popularized by Databricks around 2020 for their “Lakehouse” architecture, the term uses the metaphor of precious metal refinement to represent the purification of raw data into actionable insights.\nWhile the specific “medallion” terminology is relatively new, the underlying concept evolved from traditional data warehousing layers (staging → cleansed → presentation) used since the late 1990s. Although often discussed in cloud computing contexts, medallion architecture applies equally to desktop and local server environments.\n\n\n\n\n\n\nWarningTranslation Required: Avoid ‘Medallion’ Jargon with Public Health Colleagues\n\n\n\nThe terms “Bronze,” “Silver,” and “Gold” layers are IT/data engineering jargon that will cause confusion when speaking with public health professionals. Most epidemiologists and program staff have never encountered this terminology and will not understand what you mean.\nWhen working with public health clients, use familiar terms instead:\n\n\n\n\n\n\n\nInstead of saying…\nSay this…\n\n\n\n\n“Bronze layer”\n“Raw data,” “source files,” “incoming data”\n\n\n“Silver layer”\n“Cleaned data,” “standardized data,” “processed data”\n\n\n“Gold layer”\n“Final reports,” “line lists,” “analysis-ready data,” “dashboards”\n\n\n“Land it in Bronze”\n“Store the raw file first”\n\n\n“Promote to Gold”\n“Move it to the final/reporting layer”\n\n\n\nReserve medallion terminology for conversations with data engineers, IT architects, and other technical staff who share this vocabulary. The underlying concepts are universal; only the labels differ.\n\n\n\n5.1.8.1 Mapping Medallion Layers to Public Health\nThe medallion layers map directly to standard stages of public health surveillance and clinical data management:\n\n\n\n\n\n\n\n\n\n\nMedallion Layer\nCloud Concept\nDesktop/Local Equivalent\nPublic Health Equivalent\nPractical Application\n\n\n\n\nBronze (Raw)\nData Lake, blob storage\nRaw data folder, incoming file directory, source database tables\nIngestion / Source Systems\nOriginal, unprocessed data from EHRs, medical devices, lab results, vital records, and external APIs\n\n\nSilver (Cleansed)\nData warehouse staging, transformed datasets\nCleaned spreadsheets, normalized database tables, staging folders\nNormalization / Harmonization\nStandardizing data to common formats (FHIR, OMOP), de-identifying PHI for HIPAA compliance, harmonizing lab units and coding schemes\n\n\nGold (Curated)\nAnalytics layer, data marts, OLAP cubes\nFinal reports, pivot tables, analysis-ready datasets, exported summaries\nActionable Insights / Reporting\nLine lists for contact tracing, outbreak predictive models, patient cohorts for research, operational dashboards, CDC/NPCR submissions\n\n\n\n\n\n\n\n\n\nTipDesktop Analogy\n\n\n\nThink of medallion architecture like organizing files on your computer:\n\nBronze = Your “Downloads” or “Inbox” folder with raw files exactly as received\nSilver = A “Working” folder where you’ve cleaned up, renamed, and standardized files\nGold = Your “Final Reports” folder with polished, ready-to-share documents\n\nEven an Excel workbook can implement this pattern: raw data on a “Source” tab (Bronze), cleaned data on a “Processed” tab (Silver), and summary tables/charts on a “Dashboard” tab (Gold).\n\n\n\n\n5.1.8.2 User Roles by Layer\nDifferent team members interact with different layers based on their roles and responsibilities. In well-resourced organizations, distinct roles handle each layer; in smaller programs, a single person may be responsible for all three.\nWho Creates Each Layer?\n\n\n\n\n\n\n\n\nLayer\nBA/IT Role (Creates)\nPublic Health Role (Creates)\n\n\n\n\nBronze\nData engineers, ETL developers, integration specialists\nData managers, IT staff, interface analysts\n\n\nSilver\nData engineers, data analysts, data quality specialists\nEpidemiologists, data quality analysts, cancer registrars\n\n\nGold\nBI developers, analytics engineers, data scientists\nEpidemiologists, biostatisticians, program analysts\n\n\n\nWho Consumes Each Layer?\n\n\n\n\n\n\n\n\nLayer\nBA/IT Role (Consumes)\nPublic Health Role (Consumes)\n\n\n\n\nBronze\nData engineers (for troubleshooting), compliance/audit teams\nData managers (validation), security officers, auditors\n\n\nSilver\nData analysts, data scientists (for detailed analysis)\nEpidemiologists (case-level analysis), registrars, researchers\n\n\nGold\nExecutives, business users, ML engineers (for models)\nContact tracers, program managers, leadership, partner agencies, the public\n\n\n\n\n\n\n\n\n\nWarningReality in Resource-Constrained Settings\n\n\n\nThe role separation above represents an ideal state. In many public health settings, particularly local health departments or small programs, a single epidemiologist or data manager may be responsible for all three layers: receiving raw data files (Bronze), cleaning and standardizing them (Silver), and producing reports and line lists (Gold).\nThis is common and acceptable. The medallion architecture is a logical framework, not an organizational mandate. What matters is maintaining the conceptual separation of data stages, even when one person performs all the work. This separation enables:\n\nClear documentation of what transformations occurred\nAbility to reprocess from raw data if errors are discovered\nEasier handoff if responsibilities change\n\n\n\n\n\n5.1.8.3 What Layer Does a Line List Belong To?\nA line list, the tabular record of cases used for outbreak investigation and contact tracing, is a Gold layer artifact. Here’s why:\n\nIt serves a specific operational purpose (contact tracing, outbreak response)\nIt draws from cleansed, deduplicated Silver layer data\nIt is structured for end-user consumption by epidemiologists and contact tracers\nIt represents a curated, analysis-ready dataset rather than raw source data\n\nThe raw lab reports and case notifications are Bronze; the standardized, deduplicated case records are Silver; the line list exported for the contact tracing team is Gold.\n\n\n5.1.8.4 Strategic Benefits for Public Health\n\nImproved Data Quality and Trust\n\nEach layer acts as a governance checkpoint. Data in the Bronze layer may contain duplicates, inconsistencies, and errors. By the time data reaches Gold, it has been validated, deduplicated, and enriched. This is critical for clinical decision-making and regulatory reporting.\n\nSecurity and Compliance\n\nThe layered architecture supports role-based access control. Data engineers may access Bronze (raw PHI for integration work), while analysts work primarily in Silver (de-identified or limited datasets), and program staff view only Gold (aggregated, anonymized dashboards). This minimizes unnecessary exposure of sensitive patient information.\n\nScalability for Outbreak Response\n\nThe architecture handles varying data volumes, from routine surveillance to pandemic surge. Bronze ingests high-velocity real-time streams; Silver performs complex transformations at scale; Gold delivers rapid insights to decision-makers.\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nIn CancerSurv, the medallion architecture maps to existing workflows:\n\nBronze: Raw HL7 messages from hospital pathology systems, vital records death certificates, lab reports\nSilver: Deduplicated patient records, standardized ICD-O-3 codes, NAACCR-compliant case abstracts\nGold: Annual incidence reports for NPCR, survival analysis dashboards, geographic cancer cluster maps for epidemiologists\n\n\n\n\n\n\n\n\n\nTipTranslation Tip\n\n\n\nWhen a data engineer says “we need to land this in Bronze first,” they mean the data should be ingested in its raw form before any processing. This is equivalent to a public health analyst saying “we need to see the source data before any transformations.”\n\n\n\n\n\n5.1.9 Quick Reference Card\nFor easy access during meetings, here are the most commonly needed translations:\n\n\n\n\n\n\n\nWhen they say…\nThey might mean…\n\n\n\n\n“What’s the user story?”\n“What is the service-user scenario or clinical workflow?”\n\n\n“Let’s do a sprint”\n“Let’s run a PDSA cycle”\n\n\n“What are the NFRs?”\n“What are the implementation characteristics?”\n\n\n“Stakeholder meeting”\n“Community partner engagement session”\n\n\n“Business case”\n“Public health challenge / needs assessment”\n\n\n“Acceptance criteria”\n“Evaluation protocol measures”\n\n\n“Technical debt”\n“System sustainability issues”\n\n\n“MVP (Minimum Viable Product)”\n“Pilot intervention”\n\n\n“Land it in Bronze”\n“Ingest raw data before any processing”\n\n\n“Promote to Gold”\n“Data is ready for reporting and analytics”",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Terminology Dictionary</span>"
    ]
  },
  {
    "objectID": "chapters/04-planning.html",
    "href": "chapters/04-planning.html",
    "title": "6  Planning & Needs Assessment",
    "section": "",
    "text": "6.1 Planning & Strategy / Needs Assessment\nEvery successful project begins with understanding the problem. In business analysis, this phase is called Strategy Analysis or Planning. In public health, it is the Needs Assessment or Community Health Assessment. Both seek to answer the same fundamental question: What problem are we solving, and for whom?",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Planning & Needs Assessment</span>"
    ]
  },
  {
    "objectID": "chapters/04-planning.html#planning-strategy-needs-assessment",
    "href": "chapters/04-planning.html#planning-strategy-needs-assessment",
    "title": "6  Planning & Needs Assessment",
    "section": "",
    "text": "6.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nBusiness Need\nPublic Health Challenge\n\n\nCurrent State Analysis\nEpidemiological Baseline\n\n\nFuture State Vision\nProgram Goals & Intended Outcomes\n\n\nStakeholder Identification\nCommunity Partner Mapping\n\n\nFeasibility Assessment\nResource & Capacity Analysis\n\n\n\n\n\n6.1.2 Start Translation Early\nMany health IT projects encounter translation challenges only when they reach software requirements definition. By then, both teams have invested time and developed expectations using their own terminology. Untangling miscommunication while simultaneously trying to make progress creates unnecessary friction.\nThe business case is the ideal moment to introduce translation.\nWhen building the justification for funding (whether a grant application, budget request, or vendor RFP), explicitly consider:\n\nWhat terminology gaps exist between the technical and programmatic teams?\nWhat training or facilitation will help teams communicate effectively?\nWho will serve as translators or bridges between domains?\nWhat documentation will help each team understand the other’s frameworks?\n\nIncluding these considerations in the business case accomplishes two things:\n\nResource discovery: The funding request captures the full scope of what the project needs, including human and process elements, not just technical deliverables\nEarly alignment: Teams enter requirements gathering with shared vocabulary, reducing costly clarification loops later\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nThe CancerSurv grant application included a budget line for “cross-domain facilitation workshops” and allocated time for the project manager to develop a terminology crosswalk document before vendor selection. When TechHealth Solutions came on board, the state team provided this crosswalk in the kickoff meeting, establishing shared language from day one.\n\n\n\n\n6.1.3 Defining the Problem\n\n6.1.3.1 Business Analysis Approach\nIn traditional BA, the business need emerges from organizational pain points:\n\nRevenue decline\nOperational inefficiency\nCompliance gaps\nCompetitive pressure\nTechnology obsolescence\n\nThe BA documents this in a Business Case that quantifies the problem, proposes solutions, and projects return on investment.\n\n\n6.1.3.2 Public Health Approach\nIn public health, the need emerges from population health data:\n\nDisease incidence and prevalence\nHealth disparities across demographics\nService access gaps\nOutbreak patterns\nUnmet community needs\n\nThe epidemiologist documents this in a Needs Assessment that quantifies health burden, identifies determinants, and prioritizes interventions.\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nBusiness Need (BA framing): The legacy mainframe system is approaching end-of-life. Maintenance costs have increased 40% over three years. The system cannot support modern interoperability requirements or remote work.\nPublic Health Challenge (PH framing): Cancer data completeness has declined to 89%, below the CDC target of 95%. Late-stage diagnoses are increasing in rural counties, suggesting gaps in early detection. The current system cannot support the real-time analytics needed to identify and address disparities.\nBoth framings describe the same project. The BA framing emphasizes operational efficiency; the PH framing emphasizes health outcomes. Effective planning addresses both.\n\n\n\n\n\n6.1.4 Current State Analysis\n\n6.1.4.1 Documenting What Exists\nBefore defining requirements, understand what currently exists. The approaches differ in emphasis but serve the same purpose:\nBA Current State Analysis:\n\nProcess maps (BPMN diagrams)\nSystem inventories\nPain point interviews\nPerformance metrics\nTechnical debt assessment\n\nPH Epidemiological Baseline:\n\nDisease surveillance data\nDemographic health profiles\nService utilization patterns\nHealth equity indicators\nEnvironmental and social determinants\n\n\n\n6.1.4.2 Data Sources for Current State\n\n\n\nBA Data Sources\nPH Data Sources\n\n\n\n\nSystem logs, usage analytics\nDisease registries, vital records\n\n\nUser surveys, interviews\nCommunity health surveys (BRFSS)\n\n\nProcess documentation\nClinical guidelines, protocols\n\n\nFinancial reports\nGrant reports, program evaluations\n\n\nVendor assessments\nCDC/state health department data\n\n\n\n\n\n\n6.1.5 Future State Vision\n\n6.1.5.1 Defining Success\nThe future state describes what success looks like. Again, the framing differs:\nBA Future State:\n\nSystem capabilities and features\nProcess improvements\nPerformance targets\nTechnical architecture\nIntegration landscape\n\nPH Program Goals:\n\nHealth outcome improvements\nDisparity reductions\nService access expansion\nQuality metrics\nPopulation health indicators\n\n\n\n6.1.5.2 SMART Objectives\nBoth domains benefit from SMART objective setting:\n\n\n\n\n\n\n\n\nComponent\nBA Example\nPH Example\n\n\n\n\nSpecific\nReduce case entry time\nIncrease early-stage cancer detection\n\n\nMeasurable\nFrom 15 to 8 minutes per case\nFrom 45% to 55% of cases\n\n\nAchievable\nBased on vendor benchmarks\nBased on peer state performance\n\n\nRelevant\nSupports registrar productivity\nSupports prevention targeting\n\n\nTime-bound\nWithin 6 months of go-live\nWithin 2 years of program launch\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSMART Objective (Dual Framing):\nBA: “Within 6 months of CancerSurv deployment, average case abstraction time will decrease from 15 minutes to 8 minutes, as measured by system audit logs.”\nPH: “Within 2 years of CancerSurv deployment, data completeness will increase from 89% to 95%, enabling accurate survival analysis and disparity identification for the state cancer plan.”\nBoth objectives are valid; both should appear in project documentation.\n\n\n\n\n\n6.1.6 Proving Impact: Defining Success Metrics That Matter\nDuring the Planning phase, both teams must establish not just technical deliverables but proof points that demonstrate public health value and return on investment. This is not an afterthought for the Evaluation phase; impact measurement must be designed into the project from the beginning.\n\n6.1.6.1 Why This Matters Now\nWhen funding is constrained, every technology investment must justify its worth. Programs that cannot demonstrate measurable health outcomes or cost savings risk elimination. Grant proposals that lack data-driven proof of impact are increasingly non-competitive.\nThis creates a shared responsibility between technical and public health teams:\n\nTechnical teams (BAs, developers, product managers) must design systems that capture and report impact metrics automatically\nPublic health teams (epidemiologists, program managers, evaluators) must define what “success” looks like in measurable, system-capturable terms\n\nNeither can succeed without the other. A system that tracks technical performance but not health outcomes cannot justify continued funding. A program with strong outcomes but no data to prove them cannot compete for resources.\n\n\n6.1.6.2 Focus on Outcomes, Not Just Outputs\nDuring planning, resist the temptation to define success solely through system features or process metrics. Instead, establish clear lines of sight from technical deliverables to health outcomes:\n\n\n\n\n\n\n\nDon’t Just Measure…\nInstead, Translate To…\n\n\n\n\nSystem processes 12,000 records annually\nFaster outbreak detection through real-time analytics\n\n\nAchieved 95% data quality scores\nEnabled accurate survival statistics that guide treatment protocols\n\n\nImplemented HL7 FHIR integration\nReduced registrar burden by 40%, allowing focus on complex cases\n\n\nBuilt analytics dashboard\nIdentified cancer disparities 6 months faster, enabling targeted screening\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nDuring the Planning phase, the CancerSurv project team established paired metrics that satisfy both technical and programmatic accountability:\nBaseline Metrics: - Legacy system processes 12,000 cases annually with 87% data completeness - 6-month lag between data collection and annual reporting - Manual processes consume 15 minutes per case for abstraction - Limited ability to identify geographic or demographic disparities\nTarget Metrics (Technical + Health Outcomes):\n\nData Completeness: Increase from 87% to 95% within 18 months\n\nTechnical benefit: Meets CDC/NPCR reporting standards\nHealth outcome: Approximately 960 additional cases fully documented, strengthening survival analyses and disparity assessments\n\nReporting Timeliness: Reduce lag from 6 months to 3 months\n\nTechnical benefit: Automated data pipelines and validation\nHealth outcome: Policy decisions about screening programs informed by more current data\n\nWorkflow Efficiency: Reduce abstraction time from 15 to 8 minutes per case\n\nTechnical benefit: Improved UI/UX and automated coding suggestions\nHealth outcome: Registrars can process more cases or dedicate time to complex case resolution\n\nDisparity Identification: Enable real-time geographic and demographic analysis\n\nTechnical benefit: Analytics dashboard with interactive mapping\nHealth outcome: Target screening and prevention resources to underserved populations faster\n\n\nThese metrics were not arbitrary technical goals. They directly map to CDC reporting requirements, state strategic health objectives, and competitive grant criteria. By establishing them during Planning, the team ensured that both the business case and the system requirements aligned around demonstrable impact.\n\n\n\n\n6.1.6.3 Making the Case to Multiple Audiences\nSuccess metrics serve different stakeholders with different priorities. Plan for multiple reporting formats:\nFor CDC/NPCR Program: - Annual performance reports demonstrating compliance with NPCR standards - Data quality metrics (completeness, timeliness, validity) - Comparison to national benchmarks\nFor State Legislature and Budget Offices: - Budget justifications showing public health value per dollar invested - Efficiency gains (e.g., “Automation saves 140 hours/month, equivalent to $X salary”) - Health outcomes (e.g., “Earlier detection in rural counties reduced late-stage diagnoses by 12%”)\nFor Hospital Partners and Data Submitters: - Evidence that data submission yields insights valuable to their quality improvement efforts - Feedback reports showing how their data contributes to statewide cancer prevention - Reduced burden through automated electronic reporting\nFor Registry Staff: - Tangible improvements in workflow efficiency and job satisfaction - Recognition of their work’s impact on community health - Career development opportunities through new technical skills\n\n\n\n\n\n\nImportantDesign for Impact from Day One\n\n\n\nProving impact is not solely the responsibility of the public health team’s evaluation unit. Business analysts, developers, and product managers must design impact measurement into the system architecture from requirements gathering forward. This means:\n\nIncluding “evaluation metrics capture” as functional requirements, not optional features\nDesigning data models that support both operational reporting and outcome analysis\nBuilding dashboards and export functions that generate grant-ready reports automatically\nArchitecting audit logs that track not just system usage but workflow improvements\n\nLikewise, epidemiologists and registry staff must collaborate with technical partners early to define what “success” looks like in measurable, system-capturable terms. Vague goals like “improve cancer outcomes” must translate to specific, quantifiable indicators the system can track.\n\n\n\n\n6.1.6.4 Building Impact into the Business Case\nThe business case or grant application developed during this Planning phase should explicitly address impact demonstration:\nResource Allocation: - Budget line items for evaluation tools, data visualization platforms, or analytics staff - Time allocated for defining metrics and establishing baselines - Training for staff on using system-generated reports for grant writing\nLong-term Sustainability: - How will the system prove its value over time to justify continued funding? - What ongoing performance indicators will be tracked and reported? - Who is responsible for translating technical metrics into health outcome narratives?\nCompetitive Positioning: - How does this project’s approach to impact measurement differentiate the proposal? - What evidence-based outcomes make the case more compelling than competing priorities?\nWhen both technical and public health teams embrace this shared responsibility for proving impact, technology investments do more than deliver features on time and on budget. They deliver demonstrable improvements in population health, which justifies existing funding and unlocks new opportunities. Public health leaders and elected officials are far more likely to protect and expand programs that show tangible results: reduced cancer mortality, eliminated disparities, dollars saved through prevention.\n\n\n\n6.1.7 Stakeholder / Community Partner Identification\n\n6.1.7.1 Mapping the Landscape\nIdentifying who participates in the project requires understanding both organizational and community perspectives:\n\n\n\n\n\n\nflowchart TB\n    subgraph Internal[\"Internal Partners\"]\n        A[Registry Director]\n        B[Cancer Registrars]\n        C[Epidemiologists]\n        D[IT Department]\n    end\n    \n    subgraph External[\"External Partners\"]\n        E[Hospitals]\n        F[Laboratories]\n        G[CDC/NPCR]\n        H[Vital Records]\n    end\n    \n    subgraph Community[\"Community\"]\n        I[Cancer Survivors]\n        J[Advocacy Groups]\n        K[Researchers]\n    end\n    \n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    E --&gt; B\n    F --&gt; B\n    G --&gt; A\n    H --&gt; C\n    I --&gt; J\n    J --&gt; A\n    K --&gt; C\n\n\n\n\nFigure 6.1: Stakeholder/Community Partner Landscape\n\n\n\n\n\n\n\n6.1.7.2 Power-Interest Analysis\nThe classic BA power-interest grid maps to PH community engagement levels:\n\n\n\nQuadrant\nBA Approach\nPH Approach\n\n\n\n\nHigh Power, High Interest\nManage closely\nActive partnership\n\n\nHigh Power, Low Interest\nKeep satisfied\nInform and consult\n\n\nLow Power, High Interest\nKeep informed\nEmpower and involve\n\n\nLow Power, Low Interest\nMonitor\nEnsure representation\n\n\n\n\n\n\n6.1.8 Feasibility Assessment\n\n6.1.8.1 Can We Do This?\nBoth domains assess feasibility before committing resources:\nBA Feasibility Dimensions:\n\nTechnical feasibility (Can we build it?)\nEconomic feasibility (Can we afford it?)\nOperational feasibility (Can we run it?)\nSchedule feasibility (Can we deliver on time?)\n\nPH Feasibility Dimensions:\n\nEvidence base (Does the intervention work?)\nResource availability (Do we have funding, staff?)\nPolitical will (Is there leadership support?)\nCommunity readiness (Will the population engage?)\nEthical considerations (Is it equitable?)\n\n\n\n\n\n\n\nTipBridging Tip\n\n\n\nWhen presenting feasibility to mixed audiences, address both technical and programmatic dimensions. A system that is technically feasible but lacks community buy-in will fail. An intervention with strong evidence but no technical infrastructure cannot scale.\n\n\n\n\n\n6.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nBusiness Case\nNeeds Assessment Report\nJustify the project\n\n\nStakeholder Register\nCommunity Partner Map\nIdentify participants\n\n\nCurrent State Analysis\nEpidemiological Baseline\nDocument starting point\n\n\nFuture State Vision\nProgram Goals\nDefine success\n\n\nFeasibility Study\nReadiness Assessment\nConfirm viability\n\n\n\n\n\n6.1.10 Common Pitfalls\nFor BA professionals entering PH:\n\nUnderestimating regulatory constraints (HIPAA, IRB)\nIgnoring health equity implications\nTreating clinical workflows like business processes\nMissing grant-cycle dependencies\n\nFor PH professionals working with BA:\n\nProviding needs assessments instead of requirements\nAssuming IT teams understand clinical context\nUnderspecifying data quality needs\nIgnoring change management complexity\n\n\n\n6.1.11 Moving Forward\nWith the problem defined and feasibility confirmed, the next phase focuses on Elicitation: gathering detailed information from stakeholders and community partners about their specific needs and constraints.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Planning & Needs Assessment</span>"
    ]
  },
  {
    "objectID": "chapters/05-elicitation.html",
    "href": "chapters/05-elicitation.html",
    "title": "7  Elicitation & Engagement",
    "section": "",
    "text": "7.1 Elicitation & Stakeholder Engagement\nOnce the problem is defined, we must gather detailed information about needs, constraints, and context. In business analysis, this is Elicitation. In public health, it is Stakeholder Engagement or Community-Based Participatory Research. Both involve systematic approaches to learning from people who will use, be affected by, or govern the solution.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Elicitation & Engagement</span>"
    ]
  },
  {
    "objectID": "chapters/05-elicitation.html#elicitation-stakeholder-engagement",
    "href": "chapters/05-elicitation.html#elicitation-stakeholder-engagement",
    "title": "7  Elicitation & Engagement",
    "section": "",
    "text": "7.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nElicitation Techniques\nCommunity Engagement Methods\n\n\nRequirements Workshops\nFocus Groups, Town Halls\n\n\nUser Interviews\nKey Informant Interviews\n\n\nDocument Analysis\nLiterature Review, Policy Analysis\n\n\nObservation\nEthnography, Site Visits\n\n\nPrototyping\nPilot Testing, Formative Research\n\n\n\n\n\n7.1.2 Elicitation Techniques Mapped\n\n7.1.2.1 Interviews\nBoth domains rely heavily on one-on-one conversations with knowledgeable individuals:\nBA User Interviews:\n\nFocus on workflow, pain points, desired features\nStructured around use cases or process steps\nDocument functional and non-functional requirements\n\nPH Key Informant Interviews:\n\nFocus on community needs, barriers, facilitators\nMay explore cultural context, health beliefs\nInform intervention design and implementation strategy\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nBA Interview with Registrar:\n“Walk me through your typical day. What tasks take the most time? Where do you encounter errors? What features would make your work easier?”\nPH Interview with Oncologist:\n“How do you currently receive staging information? What data quality issues affect your treatment decisions? How could better surveillance data support tumor board discussions?”\nBoth interviews inform CancerSurv requirements, but from different perspectives.\n\n\n\n\n\n7.1.3 Workshops and Focus Groups\nGroup sessions enable collaborative discovery:\nBA Requirements Workshop:\n\nFacilitated session with defined agenda\nUses techniques like brainstorming, affinity mapping\nProduces prioritized requirement lists\nResolves conflicts through negotiation\n\nPH Focus Group:\n\nSemi-structured group discussion\nExplores shared experiences and diverse perspectives\nMay surface unanticipated needs or concerns\nEmphasizes inclusive participation\n\n\n7.1.3.1 Document Analysis\nExisting documentation provides essential context:\n\n\n\nBA Documents\nPH Documents\n\n\n\n\nSystem specifications\nClinical protocols\n\n\nProcess manuals\nCDC guidelines\n\n\nVendor contracts\nGrant requirements\n\n\nTraining materials\nHealth education materials\n\n\nAudit reports\nProgram evaluations\n\n\n\n\n\n7.1.3.2 Observation\nWatching work happen reveals what interviews miss:\nBA Observation (Job Shadowing):\n\nWatch users perform tasks\nNote workarounds and inefficiencies\nIdentify undocumented processes\nTime critical workflows\n\nPH Observation (Site Visits, Ethnography):\n\nVisit clinics, community settings\nUnderstand context and constraints\nObserve patient-provider interactions\nIdentify environmental factors\n\n\n\n\n\n\n\nTipGemba Walks\n\n\n\nThe Lean concept of “Gemba” (going to the actual place where work happens) applies to both domains. For CancerSurv, this means spending time in the registry office watching abstractors work, not just interviewing them in a conference room.\nHowever, Gemba walks often fall short when designers and programmers only observe a single point in the workflow. A critical gap in many software projects is the failure to understand the complete data and document flow across all users and use cases.\n\n\n\n\n7.1.3.3 Understanding the Full Workflow\nA frequent and costly mistake is that software designers speak primarily with decision-makers and budget holders—often located far from the actual work. These gatekeepers may not themselves use the system daily, and they may not see the entire operational flow. The result: software that fails to support the messy, multi-stage reality of work.\nThe Complete Workflow Includes:\n\nData Generation & Collection (field operations)\n\nHow data originates: inspections, tests, patient encounters\nWho collects it and what constraints they face\n\nData Intake & System Processing (data entry, administration)\n\nHow raw data enters the system\nWhat validation, coding, and quality checks occur\nWho performs these tasks and their priorities\n\nData Preparation for External Use (records, reporting)\n\nHow data is processed for public requests, legal discovery, or external reporting\nAdmin support and customer service functions\nRegulatory and compliance requirements\n\nData Use by Multiple Stakeholders (analysis, management, operations)\n\nHow frontline workers (field inspectors, registrars) use data for daily decisions\nHow managers use the same data for productivity metrics, resource allocation, and accountability\nThese uses often conflict—what serves operational efficiency may not serve field worker needs\n\n\nWhy This Matters:\nEach stage has different outcomes, different focuses, and different definitions of success. Data collected by one stakeholder is processed by another and used by a third—each with distinct priorities. A system designed to satisfy the decision-maker but not these three operational groups will fail in practice.\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nThe Problem:\nA software vendor designs CancerSurv by consulting the State Cancer Registry Director (the budget holder). The director prioritizes:\n\nNPCR reporting compliance\nCost efficiency\nCentralized system control\n\nBut the director rarely enters data or manages daily workflows. The actual system involves:\n\nData Generation: Hospital pathologists and surgical oncologists generate reports (often outside the system, in hospital EHRs)\nData Intake: Registrars abstract cases from hospital records into CancerSurv—manually reviewing pathology, staging, treatment\nData Processing: Admin staff validate data, perform quality checks, request missing information, deduplicate records, and prepare annual reports for CDC\nData Use (Inspection/Operations): Registrars use the system to identify missing follow-up information and flag data quality issues\nData Use (Management): The director uses dashboards to track incidence trends and ensure grant milestones are met; registry managers use the same data to evaluate registrar productivity and workload\n\nThe Disconnect:\nIf programmers and designers only shadow the director or attend leadership meetings, they miss critical needs:\n\nRegistrars need efficient batch workflows for large case imports (not addressed in director-focused design)\nAdmin staff need audit trails and error logs (not a director priority)\nRegistrars’ productivity data goes to managers for performance reviews—creating tension between doing quality work and doing work fast\nHospital pathologists need interoperability to send structured reports directly into CancerSurv (if this step is never observed, a manual workaround becomes permanent)\n\nThe Solution:\nBefore programming begins, the development team—including programmers and architects—must shadow all four phases:\n\nSpend time in the hospital observing how pathology reports are generated and accessed by registrars\nSit with registrars as they abstract cases and manage quality issues\nWork with admin staff processing data for CDC submission and responding to records requests\nInterview both field registrars and managers to understand how the same data supports different decisions\n\nOnly then can the team design a system that truly serves the work, not just the budget narrative.\n\n\nA Critical Principle for Both Domains:\nWhether in public health or IT, the programmer or analyst who designs the system must see the complete workflow before design begins. This is not optional; it is the foundation of requirements elicitation. Skipping this step guarantees expensive redesigns, user dissatisfaction, and workarounds that undermine system integrity.\n\n\n\n7.1.4 Engaging Diverse Voices\n\n7.1.4.1 The Equity Imperative\nPublic health emphasizes inclusive engagement, ensuring marginalized voices are heard. This principle benefits IT projects as well:\nQuestions to Ask:\n\nWho is missing from our stakeholder list?\nWhose needs might be overlooked by “typical” users?\nWhat barriers prevent participation (language, location, schedule)?\nHow do we ensure power imbalances do not silence important perspectives?\n\n\n\n7.1.4.2 Community-Based Participatory Research (CBPR)\nCBPR principles can strengthen BA elicitation:\n\n\n\n\n\n\n\nCBPR Principle\nBA Application\n\n\n\n\nCommunity as equal partner\nUsers co-design, not just provide input\n\n\nBuild on community strengths\nLeverage existing workflows that work\n\n\nBalance research and action\nDeliver incremental value during elicitation\n\n\nLong-term commitment\nMaintain relationships beyond project end\n\n\n\n\n\n\n7.1.5 Translating What You Hear\n\n7.1.5.1 From Needs to Requirements\nElicitation produces raw material that must be translated into actionable requirements:\n\n\n\n\n\n\nflowchart LR\n    A[Interviews&lt;br/&gt;Observations] --&gt; B[Elicitation Notes]\n    B --&gt; C[Analysis &&lt;br/&gt;Synthesis]\n    C --&gt; D[Requirements&lt;br/&gt;Documentation]\n    D --&gt; E[Validation with&lt;br/&gt;Stakeholders]\n    E --&gt; F[Approved&lt;br/&gt;Requirements]\n\n\n\n\nFigure 7.1: From Elicitation to Requirements\n\n\n\n\n\n\n\n7.1.5.2 Common Translation Challenges\n\n\n\n\n\n\n\n\nWhat Stakeholders Say\nWhat They Might Mean\nRequirement Implication\n\n\n\n\n“It should be easy to use”\nCurrent system requires too many clicks\nReduce clicks per task by 50%\n\n\n“We need real-time data”\nCurrent reports are weeks old\nDashboard updates within 24 hours\n\n\n“Make it like Excel”\nUsers are comfortable with Excel\nFamiliar grid-based interface\n\n\n“We need better reports”\nCurrent reports lack specific metrics\nAdd [specific metric] to [report]\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStakeholder statement: “We need the system to be faster.”\nProbing questions:\n\nWhich specific tasks feel slow?\nHow long do those tasks take now?\nWhat would be an acceptable time?\nWhat happens when the system is slow?\n\nRefined requirement: “The case search function shall return results within 3 seconds for queries returning up to 1,000 records, to support efficient case lookup during abstraction.”\n\n\n\n\n\n7.1.6 Documentation Approaches\n\n7.1.6.1 BA Requirements Documentation\n\nUser Stories (Agile)\nUse Cases (UML)\nRequirements Specifications (Waterfall)\nAcceptance Criteria\n\n\n\n7.1.6.2 PH Program Documentation\n\nLogic Models\nIntervention Protocols\nEvaluation Plans\nImplementation Guides\n\n\n\n7.1.6.3 Bridging the Formats\nFor hybrid projects, consider dual documentation:\n\n\n\n\n\n\n\n\nAudience\nFormat\nContent\n\n\n\n\nDevelopment team\nUser Stories\nFunctional requirements in Agile format\n\n\nClinical stakeholders\nService-User Scenarios\nNarrative descriptions of clinical workflows\n\n\nFunders (CDC, grants)\nLogic Model\nInputs, activities, outputs, outcomes\n\n\nGovernance\nRequirements Traceability Matrix\nLinks requirements to objectives\n\n\n\n\n\n7.1.6.4 When Standard User Stories Fall Short\nThe standard Agile user story format (“As a [user], I want [feature], so that [benefit]”) works well for many software contexts. However, it often fails to capture the nuances of clinical workflows, regulatory requirements, and public health scenarios.\nWhy standard user stories may not fit clinical contexts:\n\nClinical workflows involve conditional logic (“if this lab result, then that action”)\nRegulatory requirements mandate specific timeframes and actions\nPatient safety considerations require explicit protocols, not just user preferences\nPublic health surveillance involves system-initiated actions, not just user-initiated features\n\nAlternative formats worth considering:\nGiven-Person-Should (GPS) Format:\nThis format emphasizes context and obligation rather than desire:\n\n“Given [clinical/situational context], the [health worker role] should [specific action] to [health outcome].”\n\nExample:\n\n“Given a positive TB test result, the contact tracer should initiate household investigation within 48 hours to prevent secondary transmission.”\n\nSituational Protocol Format:\nThis format ties system behavior to clinical guidelines or regulatory requirements:\n\n“When [triggering event], the system shall [required action] within [timeframe].”\n\nExample:\n\n“When a laboratory reports a confirmed measles case, the system shall generate a contact list and notify the assigned epidemiologist within 4 hours.”\n\nService-User Scenario Format:\nThis narrative format describes a patient or client journey through the system:\n\n“Maria, a 45-year-old farmworker, visits a mobile clinic for diabetes screening. She speaks primarily Spanish and has no regular primary care provider. The system must support her preferred language, connect her to follow-up care, and track her screening results for population health reporting.”\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStandard user story:\n“As a registrar, I want to search for existing cases, so that I can avoid creating duplicates.”\nGPS format (adding clinical context):\n“Given a new pathology report for a patient who may already be in the registry, the registrar should be able to search by name, SSN, and diagnosis within 3 seconds to prevent duplicate case creation and ensure accurate incidence counts.”\nSituational protocol (system-initiated):\n“When a new case is entered, the system shall automatically search for potential duplicates using probabilistic matching and present candidates to the registrar for review before saving.”\n\n\nChoose the format that best communicates intent to your audience. Development teams may still translate these into standard user stories for sprint planning, but starting with clinical context ensures nothing gets lost in translation.\n\n\n\n7.1.7 Validation and Confirmation\n\n7.1.7.1 Ensuring Accuracy\nElicitation is iterative. Validate what you heard:\nBA Validation Techniques:\n\nRequirements reviews\nPrototype walkthroughs\nStructured walkthroughs\nSign-off meetings\n\nPH Validation Techniques:\n\nMember checking (returning findings to participants)\nCommunity review sessions\nPilot testing\nExpert review panels\n\n\n\n\n7.1.8 Managing Conflicting Needs\n\n7.1.8.1 When Stakeholders Disagree\nConflicts are inevitable. Resolution approaches include:\n\n\n\n\n\n\n\nApproach\nWhen to Use\n\n\n\n\nPrioritization\nWhen resources are limited; use MoSCoW or weighted scoring\n\n\nNegotiation\nWhen compromise is possible without losing value\n\n\nEscalation\nWhen authority must resolve; use governance structure\n\n\nPhasing\nWhen both needs are valid; address in different releases\n\n\nAlternatives Analysis\nWhen creative solutions can satisfy both parties\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nConflict: Registrars want a simple, streamlined interface. Epidemiologists want comprehensive data fields for analysis.\nResolution: Implement a tiered interface:\n\nCore fields (required): Streamlined view for registrars\nExtended fields (optional): Available when needed\nAnalytics fields: Populated from other sources, not requiring registrar entry\n\n\n\n\n\n\n7.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nElicitation Results\nEngagement Summary\nRaw findings documentation\n\n\nRequirements Document\nProgram Protocol\nDetailed specifications\n\n\nUser Stories / Use Cases\nService-User Scenarios\nActionable descriptions\n\n\nStakeholder Feedback Log\nCommunity Input Register\nTrack all input received\n\n\n\n\n\n7.1.10 Moving Forward\nWith needs elicited and documented, the next phase focuses on Requirements Analysis: organizing, prioritizing, and specifying the detailed requirements that will guide solution design.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Elicitation & Engagement</span>"
    ]
  },
  {
    "objectID": "chapters/06-requirements.html",
    "href": "chapters/06-requirements.html",
    "title": "8  Requirements & Data Analysis",
    "section": "",
    "text": "8.1 Requirements Analysis & Data Analysis\nRaw elicitation findings must be organized, analyzed, and specified in detail. In business analysis, this is Requirements Analysis and Design Definition. In public health, it maps to Data Analysis and Logic Model Development. Both processes transform unstructured input into structured, actionable specifications.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Requirements & Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/06-requirements.html#requirements-analysis-data-analysis",
    "href": "chapters/06-requirements.html#requirements-analysis-data-analysis",
    "title": "8  Requirements & Data Analysis",
    "section": "",
    "text": "8.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nRequirements Analysis\nData Analysis\n\n\nRequirements Specification\nLogic Model / Theory of Change\n\n\nFunctional Requirements\nProgram Activities\n\n\nNon-Functional Requirements\nImplementation Characteristics\n\n\nData Requirements\nCase Definitions, Data Dictionaries\n\n\nBusiness Rules\nClinical Guidelines, Protocols\n\n\n\n\n\n8.1.2 Types of Requirements\n\n8.1.2.1 Functional Requirements\nBA Definition: What the system must do. Capabilities, features, functions.\nPH Equivalent: Program activities, intervention components, service delivery specifications.\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nFunctional Requirement (BA format):\n\nFR-101: The system shall allow users to search for cases by patient name, medical record number, or social security number.\n\nProgram Activity (PH format):\n\nCancer registrars will abstract and code incident cases from hospital pathology reports within 6 months of diagnosis date.\n\nBoth describe “what happens” but at different levels of specificity.\n\n\n\n\n8.1.2.2 Non-Functional Requirements (NFRs)\nBA Definition: Quality attributes, constraints, performance characteristics.\nPH Equivalent: Implementation characteristics (per CFIR framework).\n\n\n\n\n\n\n\n\nNFR Category\nBA Focus\nPH Focus (CFIR Domain)\n\n\n\n\nPerformance\nResponse time, throughput\nEfficiency of intervention delivery\n\n\nSecurity\nAccess control, encryption\nHIPAA compliance, trust\n\n\nScalability\nGrowth capacity\nOutbreak surge response\n\n\nUsability\nUser interface design\nComplexity, ease of adoption\n\n\nReliability\nUptime, fault tolerance\nService continuity\n\n\nInteroperability\nAPI standards, data exchange\nHealth information exchange\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nNFR (BA format):\n\nNFR-201: The system shall maintain 99.9% uptime during business hours (8 AM to 6 PM Eastern).\n\nImplementation Characteristic (PH format):\n\nThe CancerSurv platform must demonstrate high reliability to maintain registrar confidence and ensure continuous data collection, critical during cancer awareness campaigns when reporting volumes increase.\n\n\n\n\n\n8.1.2.3 Data Requirements\nData specifications are central to both domains:\nBA Data Model:\n\nEntity-Relationship diagrams\nDatabase schemas\nData dictionaries\nValidation rules\n\nPH Case Definitions:\n\nDiagnostic criteria\nInclusion/exclusion criteria\nCoding standards (ICD-O-3, TNM)\nData quality metrics\n\n\n\n\n\n\n\nerDiagram\n    PATIENT ||--o{ TUMOR : has\n    TUMOR ||--o{ TREATMENT : receives\n    TUMOR ||--|| DIAGNOSIS : \"classified by\"\n    FACILITY ||--o{ TUMOR : reports\n    \n    PATIENT {\n        string patient_id PK\n        string name\n        date birth_date\n        string ssn\n        string address\n    }\n    \n    TUMOR {\n        string tumor_id PK\n        string patient_id FK\n        date diagnosis_date\n        string primary_site\n        string histology\n        string stage\n    }\n\n\n\n\nFigure 8.1: CancerSurv Simplified Data Model\n\n\n\n\n\n\n\n8.1.2.4 Data Architecture Requirements\nModern public health data systems require architecture that handles data from ingestion through analytics. The medallion architecture provides a framework for specifying data flow requirements across three progressive layers.\nSpecifying Requirements by Layer\nWhen documenting data requirements, specify which layer each requirement applies to:\n\n\n\n\n\n\n\n\n\nRequirement Type\nBronze Layer\nSilver Layer\nGold Layer\n\n\n\n\nPrimary Focus\nCompleteness, lineage\nAccuracy, consistency\nTimeliness, usability\n\n\nData State\nRaw, as-received\nCleansed, standardized\nAggregated, analytics-ready\n\n\nSchema\nSchema-on-read (flexible)\nEnforced schema\nDimensional models\n\n\nRetention\nLong-term archive\nMedium-term\nPurpose-specific\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nBronze Layer Requirements:\n\nREQ-DATA-001: The system shall ingest HL7 v2.x ADT messages from hospital interfaces within 15 minutes of receipt\nREQ-DATA-002: The system shall preserve original message content with timestamp and source metadata for audit purposes\nREQ-DATA-003: The system shall support ingestion of CSV files from facilities without HL7 capability\n\nSilver Layer Requirements:\n\nREQ-DATA-010: The system shall deduplicate patient records using probabilistic matching (≥95% precision)\nREQ-DATA-011: The system shall map incoming diagnosis codes to ICD-O-3 standard within 24 hours\nREQ-DATA-012: The system shall apply NAACCR edit checks and flag records failing validation\n\nGold Layer Requirements:\n\nREQ-DATA-020: The system shall generate NPCR-compliant annual submission files by January 31\nREQ-DATA-021: The system shall calculate age-adjusted incidence rates by county, updated monthly\nREQ-DATA-022: The system shall provide self-service query access for approved epidemiologists\n\n\n\nData Lineage and Traceability\nPublic health reporting requires demonstrable data provenance. Requirements should specify:\n\nHow data flows from source to final output\nWhich transformations are applied at each layer\nHow to trace any Gold-layer value back to its Bronze-layer source\n\nThis is equivalent to the “chain of custody” concept in laboratory settings.\n\n\n\n\n\n\nflowchart LR\n    subgraph Bronze[\"Bronze (Raw)\"]\n        B1[\"HL7 Messages\"]\n        B2[\"Lab Reports\"]\n        B3[\"Vital Records\"]\n    end\n    \n    subgraph Silver[\"Silver (Cleansed)\"]\n        S1[\"Deduplicated&lt;br/&gt;Patient Records\"]\n        S2[\"Standardized&lt;br/&gt;Case Abstracts\"]\n    end\n    \n    subgraph Gold[\"Gold (Curated)\"]\n        G1[\"Incidence&lt;br/&gt;Reports\"]\n        G2[\"Analytics&lt;br/&gt;Dashboards\"]\n        G3[\"Research&lt;br/&gt;Datasets\"]\n    end\n    \n    B1 --&gt; S1\n    B2 --&gt; S1\n    B3 --&gt; S1\n    S1 --&gt; S2\n    S2 --&gt; G1\n    S2 --&gt; G2\n    S2 --&gt; G3\n\n\n\n\nFigure 8.2: Data Flow Through Medallion Layers\n\n\n\n\n\n\n\n8.1.2.5 Business Rules / Clinical Guidelines\nRules governing system behavior and data processing:\n\n\n\n\n\n\n\nBA Business Rule\nPH Clinical Guideline\n\n\n\n\n“Order cannot be placed if credit limit exceeded”\n“Case is reportable if primary site is within state jurisdiction”\n\n\n“Discount applies if quantity &gt; 100”\n“Stage is unknown if pathology report unavailable within 4 months”\n\n\n“Manager approval required for refunds &gt; $500”\n“Multiple primary rules apply per SEER guidelines”\n\n\n\n\n\n\n8.1.3 Data Standards as Primary Requirements\nIn commercial software projects, data standards (file formats, API specifications, integration protocols) are often treated as technical details to be resolved by developers during implementation. In public health IT, this approach fails.\nData standards are primary business requirements, not optional technical details.\nHealth information systems operate within a regulatory and interoperability landscape where specific standards are mandated, not merely preferred. These standards should be identified and documented early, during requirements analysis, not deferred to design or implementation.\n\n\n\n\n\n\n\n\nStandard\nPurpose\nRequirement Implication\n\n\n\n\nHIPAA\nPrivacy and security\nSecurity architecture, access controls, audit logging\n\n\nHL7 v2\nMessage-based data exchange\nInterface specifications for lab results, ADT events\n\n\nHL7 FHIR\nModern API-based exchange\nRESTful API design for EHR integration\n\n\nUSCDI\nFederal data interoperability\nRequired data classes for ONC certification\n\n\nICD-10 / ICD-O-3\nDiagnosis and oncology coding\nValidation rules, lookup tables, code mapping\n\n\nSNOMED CT\nClinical terminology\nConcept mapping specifications\n\n\nLOINC\nLaboratory test coding\nInterface specifications for electronic lab reporting\n\n\nNAACCR\nCancer registry standards\nData dictionary, edit checks, submission formats\n\n\n\n\n\n\n\n\n\nWarningCommon Pitfall\n\n\n\nWhen data standards are not identified as requirements, projects encounter costly surprises during integration testing. A system that functions correctly in isolation may fail when connected to external systems that expect specific data formats, codes, or protocols.\nFor business analysts entering public health IT: treat data standards as \"Must Have\" requirements from day one. Interview stakeholders about external data exchanges early, and document the specific standards each interface requires.\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStandards-Based Requirements for CancerSurv:\n\n\n\n\n\n\n\nStandard\nCancerSurv Requirement\n\n\n\n\nHIPAA\nAll PHI encrypted at rest and in transit; role-based access; 6-year audit log retention\n\n\nHL7 FHIR\nPatient, Condition, and Observation resources for hospital EHR integration\n\n\nNAACCR v24\nAll required data items; automated EDITS validation; annual submission file generation\n\n\nICD-O-3\nValidated primary site and histology codes with cross-validation rules\n\n\nLOINC\nMapping table for incoming electronic pathology reports\n\n\n\nThese standards-based requirements appeared in the CancerSurv requirements specification alongside functional requirements, with the same priority and traceability as any other \"Must Have\" item.\n\n\n\n\n8.1.4 The Logic Model as Requirements Framework\nPublic health uses the Logic Model to specify program components. This structure maps directly to requirements categories:\n\n\n\n\n\n\nflowchart LR\n    subgraph Inputs[\" \"]\n        I[\"**Inputs**&lt;br/&gt;(Resources)&lt;br/&gt;───────&lt;br/&gt;Funding&lt;br/&gt;Staff&lt;br/&gt;Data feeds&lt;br/&gt;Infrastructure\"]\n    end\n    \n    subgraph Activities[\" \"]\n        A[\"**Activities**&lt;br/&gt;(Functions)&lt;br/&gt;───────&lt;br/&gt;Case abstraction&lt;br/&gt;Data quality&lt;br/&gt;Reporting&lt;br/&gt;Analytics\"]\n    end\n    \n    subgraph Outputs[\" \"]\n        O[\"**Outputs**&lt;br/&gt;(Deliverables)&lt;br/&gt;───────&lt;br/&gt;Case records&lt;br/&gt;Quality reports&lt;br/&gt;NPCR submissions&lt;br/&gt;Dashboards\"]\n    end\n    \n    subgraph Outcomes[\" \"]\n        OC[\"**Outcomes**&lt;br/&gt;(Success Metrics)&lt;br/&gt;───────&lt;br/&gt;95% completeness&lt;br/&gt;Timely reporting&lt;br/&gt;User satisfaction\"]\n    end\n    \n    I --&gt; A --&gt; O --&gt; OC\n\n\n\n\nFigure 8.3: Logic Model Components Mapped to Requirements\n\n\n\n\n\n\n\n\nLogic Model Component\nRequirements Category\n\n\n\n\nInputs\nConstraints, Assumptions, Dependencies\n\n\nActivities\nFunctional Requirements\n\n\nOutputs\nSystem Deliverables, Features\n\n\nOutcomes\nSuccess Metrics, Acceptance Criteria\n\n\nImpact\nStrategic Objectives, Business Value\n\n\n\n\n\n8.1.5 Prioritization\n\n8.1.5.1 Methods for Ranking Requirements\nNot all requirements are equal. Prioritization ensures critical needs are addressed first:\nMoSCoW Method:\n\nMust have: Essential for go-live\nShould have: Important but not critical\nCould have: Desirable if time permits\nWon’t have: Out of scope for this release\n\nWeighted Scoring:\nAssign weights to criteria (business value, regulatory requirement, user impact) and score each requirement.\nKano Model:\n\nBasic needs (expected, cause dissatisfaction if missing)\nPerformance needs (more is better)\nDelighters (unexpected features that excite)\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nMust Have:\n\nCase entry and coding functionality\nHIPAA-compliant security\nNPCR data submission capability\n\nShould Have:\n\nReal-time analytics dashboard\nMobile-friendly interface\nAutomated duplicate detection\n\nCould Have:\n\nMachine learning for coding assistance\nPatient portal for self-reported outcomes\nIntegration with research databases\n\n\n\n\n\n\n8.1.6 Requirements Traceability\n\n8.1.6.1 Linking Requirements to Objectives\nTraceability ensures every requirement connects to a business need or program goal:\n\n\n\n\n\n\nflowchart TB\n    BN[Business Need /&lt;br/&gt;Program Goal] --&gt; FR[Functional&lt;br/&gt;Requirement]\n    BN --&gt; NFR[Non-Functional&lt;br/&gt;Requirement]\n    FR --&gt; TC[Test Case]\n    NFR --&gt; TC\n    FR --&gt; US[User Story]\n    TC --&gt; TR[Test Result]\n\n\n\n\nFigure 8.4: Requirements Traceability Structure\n\n\n\n\n\nTraceability Matrix Example:\n\n\n\n\n\n\n\n\n\n\nRequirement ID\nDescription\nSource\nPriority\nTest Case\n\n\n\n\nFR-101\nCase search functionality\nRegistrar interviews\nMust\nTC-101, TC-102\n\n\nFR-102\nICD-O-3 coding validation\nNAACCR standards\nMust\nTC-103\n\n\nNFR-201\n99.9% uptime\nSLA requirements\nMust\nTC-201\n\n\n\n\n\n\n8.1.7 Specification Formats\n\n8.1.7.1 Writing Good Requirements\nRegardless of format, good requirements share characteristics:\n\n\n\n\n\n\n\n\nCharacteristic\nDescription\nExample\n\n\n\n\nComplete\nContains all necessary information\nIncludes error handling, edge cases\n\n\nConsistent\nDoes not contradict other requirements\nUses standard terminology\n\n\nUnambiguous\nOnly one interpretation possible\n“Within 3 seconds” not “quickly”\n\n\nVerifiable\nCan be tested\nMeasurable acceptance criteria\n\n\nTraceable\nLinks to source and test\nIncludes requirement ID\n\n\n\n\n\n8.1.7.2 User Story Format\nFor Agile projects:\n\nAs a [role], I want [feature], so that [benefit].\n\nAcceptance Criteria:\n\nGiven [context], when [action], then [result]\n\n\n\n8.1.7.3 GPS Format for Clinical Contexts\n\nGiven [clinical context], the [health worker role] should [specific action] to [health outcome].\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nUser Story:\n\nAs a cancer registrar, I want to search for existing cases before creating a new record, so that I avoid creating duplicate entries.\n\nGPS Format:\n\nGiven a new pathology report, the registrar should search existing cases by patient identifiers before abstracting, to maintain data integrity and accurate incidence counts.\n\nAcceptance Criteria:\n\nGiven a patient name, when the registrar searches, then matching cases display within 3 seconds\nGiven a patient with no existing cases, when the registrar searches, then a “No matches found” message displays with option to create new case\n\n\n\n\n\n\n8.1.8 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nRequirements Specification\nLogic Model\nDocument what must be built\n\n\nData Dictionary\nCase Definition / Data Standards\nSpecify data structures\n\n\nBusiness Rules Catalog\nClinical Protocol\nDefine processing rules\n\n\nTraceability Matrix\nEvaluation Framework\nLink requirements to objectives\n\n\nPrioritized Backlog\nWorkplan\nOrder implementation work\n\n\n\n\n\n8.1.9 Moving Forward\nWith requirements analyzed, prioritized, and specified, the next phase focuses on Design: defining how the solution will be built to meet these requirements.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Requirements & Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/07-design.html",
    "href": "chapters/07-design.html",
    "title": "9  Design & Solution Definition",
    "section": "",
    "text": "9.1 Design & Solution Definition / Intervention Design\nWith requirements defined, we move to designing the solution. In business analysis, this is Solution Design or Design Definition. In public health, it parallels Intervention Design and Implementation Planning. Both involve translating requirements into a blueprint for action.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design & Solution Definition</span>"
    ]
  },
  {
    "objectID": "chapters/07-design.html#design-solution-definition-intervention-design",
    "href": "chapters/07-design.html#design-solution-definition-intervention-design",
    "title": "9  Design & Solution Definition",
    "section": "",
    "text": "9.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSolution Architecture\nIntervention Framework\n\n\nSystem Design\nProgram Design\n\n\nInterface Design\nService Delivery Model\n\n\nIntegration Design\nHealth Information Exchange\n\n\nChange Management Plan\nImplementation Strategy (CFIR)\n\n\n\n\n\n9.1.2 Architecture and Framework\n\n9.1.2.1 Solution Architecture\nBA solution architecture defines:\n\nSystem components and their relationships\nTechnology stack selection\nIntegration points with existing systems\nData flow between components\nSecurity architecture\n\n\n\n9.1.2.2 Intervention Framework\nPH intervention design defines:\n\nCore intervention components\nDelivery mechanisms\nTarget populations\nAdaptable vs. core elements\nContextual considerations\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSolution Architecture (BA):\n┌─────────────────────────────────────────────────────────┐\n│                    CancerSurv Platform                   │\n├──────────────┬──────────────┬──────────────┬────────────┤\n│  Web UI      │  API Layer   │  Analytics   │  Reporting │\n│  (React)     │  (REST/FHIR) │  (R/Python)  │  Engine    │\n├──────────────┴──────────────┴──────────────┴────────────┤\n│                    Core Services                         │\n│  Case Management │ Data Quality │ User Management        │\n├─────────────────────────────────────────────────────────┤\n│                    Data Layer                            │\n│  PostgreSQL │ Document Store │ Data Warehouse           │\n└─────────────────────────────────────────────────────────┘\nIntervention Framework (PH):\nCancerSurv delivers the surveillance intervention through:\n\nCore components: Case abstraction, data quality, NPCR reporting (non-negotiable)\nAdaptable elements: Dashboard customization, local report templates\nDelivery mechanism: Cloud-based SaaS with local training support\nTarget population: State cancer registries, hospital tumor registrars\n\n\n\n\n\n\n9.1.3 Design Patterns\n\n9.1.3.1 User Interface Design\nBoth domains emphasize user-centered design:\nBA UI/UX Approach:\n\nWireframes and mockups\nUser journey mapping\nUsability testing\nAccessibility compliance (WCAG)\n\nPH Service Design Approach:\n\nPatient journey mapping\nCultural competency review\nHealth literacy assessment\nEquity impact analysis\n\n\n\n9.1.3.2 Key Design Principles\n\n\n\n\n\n\n\n\nPrinciple\nBA Application\nPH Application\n\n\n\n\nSimplicity\nMinimize clicks, clear navigation\nReduce complexity for adoption\n\n\nConsistency\nStandard UI patterns\nConsistent with clinical workflows\n\n\nFeedback\nVisual confirmation of actions\nClear outcome indicators\n\n\nError Prevention\nValidation before submission\nBuilt-in clinical decision support\n\n\nFlexibility\nCustomizable views, workflows\nAdaptable to local context\n\n\n\n\n\n\n9.1.4 Integration Design\n\n9.1.4.1 Connecting Systems\nHealth IT projects require extensive integration:\n\n\n\n\n\n\nflowchart LR\n    subgraph External[\"External Systems\"]\n        H[Hospital EHRs]\n        L[Lab Systems]\n        V[Vital Records]\n        N[NPCR/CDC]\n    end\n    \n    subgraph CancerSurv[\"CancerSurv Platform\"]\n        API[Integration Layer]\n        Core[Core System]\n    end\n    \n    H --&gt;|HL7 FHIR| API\n    L --&gt;|HL7 v2| API\n    V --&gt;|Batch| API\n    API --&gt; Core\n    Core --&gt;|XML| N\n\n\n\n\nFigure 9.1: CancerSurv Integration Landscape\n\n\n\n\n\n\n\n9.1.4.2 Integration Standards\n\n\n\n\n\n\n\n\nStandard\nUse Case\nDesign Consideration\n\n\n\n\nHL7 FHIR\nReal-time EHR integration\nREST APIs, JSON payloads\n\n\nHL7 v2.x\nLegacy lab interfaces\nMessage parsing, acknowledgments\n\n\nNAACCR XML\nCancer registry exchange\nSchema validation, field mapping\n\n\nDirect Protocol\nSecure health messaging\nCertificate management\n\n\n\n\n\n\n9.1.5 Implementation Readiness Assessment\n\n9.1.5.1 CFIR for Design Validation\nThe Consolidated Framework for Implementation Research (CFIR) provides a lens for evaluating design decisions:\n\n\n\n\n\n\n\nCFIR Domain\nDesign Questions\n\n\n\n\nIntervention Characteristics\nIs the design evidence-based? Is it adaptable?\n\n\nOuter Setting\nDoes it meet regulatory requirements? Does it connect to external systems?\n\n\nInner Setting\nDoes it fit organizational workflows? Is infrastructure adequate?\n\n\nIndividuals\nWill users accept it? What training is needed?\n\n\nProcess\nHow will it be implemented? Who champions it?\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nCFIR-Informed Design Review:\n\n\n\n\n\n\n\n\nCFIR Construct\nCancerSurv Design Decision\nRationale\n\n\n\n\nRelative Advantage\nModern UI, mobile access\nClear improvement over mainframe\n\n\nComplexity\nPhased rollout, role-based views\nReduce cognitive load\n\n\nAdaptability\nConfigurable data fields\nSupport local registry needs\n\n\nAvailable Resources\nCloud-hosted, vendor support\nMinimize IT infrastructure burden\n\n\nSelf-Efficacy\nEmbedded training, help system\nBuild user confidence\n\n\n\n\n\n\n\n\n9.1.6 Prototyping and Validation\n\n9.1.6.1 Iterative Design\nDesign should be validated before full development:\nBA Prototyping:\n\nLow-fidelity wireframes for concept validation\nHigh-fidelity mockups for detailed feedback\nInteractive prototypes for workflow testing\nMVP (Minimum Viable Product) for market validation\n\nPH Piloting:\n\nFormative research with target population\nPilot testing in representative sites\nRapid cycle evaluation (PDSA)\nFidelity assessment\n\n\n\n9.1.6.2 Prototype Fidelity Levels\n\n\n\n\n\n\n\n\n\nLevel\nBA Artifact\nPH Artifact\nPurpose\n\n\n\n\nLow\nPaper sketches, Balsamiq\nConcept paper, draft protocol\nConcept validation\n\n\nMedium\nClickable mockups\nPilot at 1-2 sites\nWorkflow validation\n\n\nHigh\nWorking prototype\nMulti-site pilot\nFull process validation\n\n\n\n\n\n\n9.1.7 Change Management Planning\n\n9.1.7.1 Preparing for Transition\nDesign must include plans for organizational change:\nBA Change Management:\n\nStakeholder impact analysis\nCommunication plan\nTraining plan\nResistance management\nTransition strategy\n\nPH Implementation Planning:\n\nCapacity building\nTechnical assistance model\nSustainability planning\nScale-up strategy\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nChange Management Elements:\n\n\n\n\n\n\n\nElement\nPlan\n\n\n\n\nTraining\n3-tier approach: super-users (in-person), all users (webinar), ongoing (self-paced modules)\n\n\nCommunication\nMonthly newsletters, demo sessions at registrar conferences\n\n\nSupport\nHelp desk during business hours; online knowledge base; user community forum\n\n\nRollout\nPhase 1: High-volume hospitals; Phase 2: Remaining facilities; Phase 3: Full operation\n\n\n\n\n\n\n\n\n9.1.8 Design Documentation\n\n9.1.8.1 What to Document\nDesign documentation bridges requirements and implementation:\n\n\n\n\n\n\n\n\nDocument\nBA Content\nPH Content\n\n\n\n\nArchitecture Document\nSystem components, technology stack\nIntervention components, delivery model\n\n\nInterface Specifications\nScreen layouts, navigation flows\nService user touchpoints\n\n\nIntegration Specifications\nAPIs, message formats\nData exchange protocols\n\n\nData Design\nDatabase schema, data flows\nData collection instruments\n\n\nSecurity Design\nAccess controls, encryption\nPrivacy protections, consent\n\n\nTransition Plan\nDeployment, training, support\nImplementation, capacity building\n\n\n\n\n\n\n9.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nSolution Architecture\nIntervention Framework\nDefine solution structure\n\n\nUI/UX Design\nService Delivery Model\nSpecify user experience\n\n\nIntegration Design\nHIE Specifications\nDefine system connections\n\n\nPrototype\nPilot Protocol\nValidate design\n\n\nChange Management Plan\nImplementation Strategy\nPrepare organization\n\n\n\n\n\n9.1.10 Moving Forward\nWith design complete, the next phase focuses on Implementation: building, deploying, and rolling out the solution while managing the organizational change required for adoption.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design & Solution Definition</span>"
    ]
  },
  {
    "objectID": "chapters/08-implementation.html",
    "href": "chapters/08-implementation.html",
    "title": "10  Implementation & Execution",
    "section": "",
    "text": "10.1 Implementation & Program Execution\nDesign becomes reality through implementation. In business analysis, this phase involves Solution Delivery and Change Management. In public health, it is Program Implementation guided by frameworks like PDSA (Plan-Do-Study-Act). Both require managing complexity while maintaining focus on outcomes.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implementation & Execution</span>"
    ]
  },
  {
    "objectID": "chapters/08-implementation.html#implementation-program-execution",
    "href": "chapters/08-implementation.html#implementation-program-execution",
    "title": "10  Implementation & Execution",
    "section": "",
    "text": "10.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSprint/Iteration\nPDSA Cycle\n\n\nRelease Management\nPhased Rollout\n\n\nUser Acceptance Testing\nPilot Evaluation\n\n\nGo-Live\nProgram Launch\n\n\nDefect Management\nVariance/Adverse Event Tracking\n\n\n\n\n\n10.1.2 The Double Loop of Agile in Public Health\nStandard Agile focuses on product improvement: Build → Measure → Learn. Public health adds a second loop: Surveillance → Intervention → Evaluation.\n\n\n\n\n\n\nflowchart LR\n    subgraph Agile[\"Agile Loop\"]\n        A1[Plan Sprint] --&gt; A2[Develop]\n        A2 --&gt; A3[Demo/Review]\n        A3 --&gt; A4[Retrospective]\n        A4 --&gt; A1\n    end\n    \n    subgraph PH[\"Public Health Loop\"]\n        P1[Plan] --&gt; P2[Do]\n        P2 --&gt; P3[Study]\n        P3 --&gt; P4[Act]\n        P4 --&gt; P1\n    end\n    \n    A3 &lt;-.-&gt;|Sync| P3\n\n\n\n\nFigure 10.1: The Double Loop: Agile + Public Health\n\n\n\n\n\nThe BA must ensure both loops are synchronized: software releases should align with epidemiological reporting cycles.\n\n\n10.1.3 Agile Practices Adapted\n\n10.1.3.1 Sprint Planning\nTraditional Agile:\n\nProduct owner prioritizes backlog\nTeam selects stories for sprint\nStories estimated in points\nSprint goal defined\n\nPublic Health Adaptation:\n\nProgram manager prioritizes based on grant milestones\nTeam considers reporting deadlines\nStories linked to program objectives\nSprint goal aligned with public health impact\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSprint Goal (BA): Complete case search functionality and duplicate detection module.\nProgram Alignment (PH): This sprint supports NPCR Milestone 2: “Data quality infrastructure operational.” Completion enables Q2 data submission with duplicate resolution.\nSprint Backlog:\n\n\n\nStory\nPoints\nGrant Milestone\n\n\n\n\nCase search by patient ID\n5\nM2\n\n\nCase search by name/DOB\n3\nM2\n\n\nDuplicate candidate display\n5\nM2\n\n\nMerge duplicate records\n8\nM2\n\n\nAudit log for merges\n3\nCompliance\n\n\n\n\n\n\n\n10.1.3.2 PDSA Cycles\nPDSA provides a structured approach to continuous improvement:\n\n\n\n\n\n\n\n\nPhase\nActivities\nCancerSurv Example\n\n\n\n\nPlan\nDefine change, predict outcomes\n“Adding auto-population of demographics will reduce entry time by 2 minutes”\n\n\nDo\nImplement on small scale\nEnable feature for 3 pilot registrars\n\n\nStudy\nAnalyze results\nCompare entry times before/after; gather feedback\n\n\nAct\nAdopt, adapt, or abandon\nFeature reduced time by 1.5 minutes; adopt with UI adjustments\n\n\n\n\n\n10.1.3.3 Mapping Sprints to PDSA\n\n\n\nSprint Element\nPDSA Element\n\n\n\n\nSprint Planning\nPlan\n\n\nDevelopment\nDo\n\n\nSprint Review\nStudy\n\n\nRetrospective\nAct\n\n\nBacklog Refinement\nNext Plan cycle\n\n\n\n\n\n\n10.1.4 Testing in Health IT\n\n10.1.4.1 Testing Levels\n\n\n\n\n\n\n\n\nLevel\nBA Focus\nPH Focus\n\n\n\n\nUnit Testing\nCode functions correctly\nN/A (technical)\n\n\nIntegration Testing\nSystems connect properly\nData flows between systems\n\n\nSystem Testing\nFull system works\nEnd-to-end workflows function\n\n\nUser Acceptance Testing\nUsers approve functionality\nClinical workflows validated\n\n\nOperational Testing\nSystem performs under load\nHandles reporting surge periods\n\n\n\n\n\n10.1.4.2 UAT for Clinical Systems\nUser Acceptance Testing in public health requires clinical validation:\nStandard UAT:\n\nDoes the system do what was specified?\nDo users accept the interface?\nAre performance requirements met?\n\nClinical UAT Additions:\n\nDo clinical workflows function correctly?\nDoes data quality meet standards?\nDo edits align with NAACCR rules?\nIs patient safety protected?\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nUAT Test Case:\n\n\n\n\n\n\n\n\n\n\nID\nScenario\nSteps\nExpected Result\nPH Validation\n\n\n\n\nUAT-101\nDuplicate detection\nEnter case matching existing patient\nSystem flags potential duplicate\nMatches NAACCR duplicate resolution rules\n\n\nUAT-102\nStage validation\nEnter invalid stage combination\nSystem prevents save with error message\nError references SEER staging manual\n\n\nUAT-103\nNPCR export\nGenerate submission file\nValid NAACCR XML produced\nPasses CDC validator tool\n\n\n\n\n\n\n\n\n10.1.5 Managing Change\n\n10.1.5.1 Organizational Readiness\nImplementation fails without organizational change management:\nReadiness Assessment:\n\nLeadership commitment\nStaff capacity\nInfrastructure availability\nWorkflow adaptability\nCultural alignment\n\nCommon Barriers:\n\n\n\n\n\n\n\n\nBarrier\nBA Perspective\nPH Perspective\n\n\n\n\nResistance\nUsers prefer old system\n“Not how we’ve always done it”\n\n\nCapacity\nTraining time unavailable\nStaff already overburdened\n\n\nInfrastructure\nHardware/network issues\nRural sites lack bandwidth\n\n\nWorkflow\nProcess changes required\nClinical protocols affected\n\n\n\n\n\n10.1.5.2 Training Approaches\n\n\n\n\n\n\n\n\nApproach\nWhen to Use\nCancerSurv Example\n\n\n\n\nTrain-the-Trainer\nLarge, distributed user base\nRegistry supervisors trained first\n\n\nJust-in-Time\nComplex, infrequent tasks\nContext-sensitive help for staging\n\n\nSimulation\nHigh-stakes processes\nPractice mode for case abstraction\n\n\nPeer Support\nOngoing questions\nSuper-user network\n\n\n\n\n\n\n10.1.6 Deployment Strategies\n\n10.1.6.1 Phased vs. Big Bang\n\n\n\n\n\n\n\n\n\nStrategy\nPros\nCons\nWhen to Use\n\n\n\n\nBig Bang\nSingle cutover, consistent\nHigh risk, no rollback\nSimple systems, urgent deadlines\n\n\nPhased\nLower risk, lessons learned\nLonger timeline, parallel systems\nComplex systems, distributed users\n\n\nPilot\nReal-world validation\nLimited initial impact\nNew interventions, uncertain adoption\n\n\nParallel\nSafety net available\nResource intensive\nCritical systems, high risk tolerance\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nDeployment Strategy: Phased with Pilot\n\n\n\n\n\n\n\n\n\nPhase\nScope\nDuration\nSuccess Criteria\n\n\n\n\nPilot\n3 high-volume hospitals\n8 weeks\n&gt;90% user satisfaction; &lt;5% error rate\n\n\nPhase 1\nRemaining hospitals (20)\n12 weeks\nAll hospitals submitting data\n\n\nPhase 2\nPhysician offices, labs\n8 weeks\nELR feeds operational\n\n\nPhase 3\nFull operation, legacy decommission\n4 weeks\nLegacy system retired\n\n\n\n\n\n\n\n\n10.1.7 Monitoring During Implementation\n\n10.1.7.1 What to Track\n\n\n\n\n\n\n\n\nCategory\nBA Metrics\nPH Metrics\n\n\n\n\nAdoption\nLogin counts, feature usage\nSites trained, go-live completion\n\n\nPerformance\nResponse times, error rates\nData submission timeliness\n\n\nQuality\nDefect counts, resolution time\nData completeness, accuracy\n\n\nSatisfaction\nUser surveys, support tickets\nRegistrar feedback, NPS scores\n\n\nOutcomes\nFeature delivery, velocity\nGrant milestone achievement\n\n\n\n\n\n10.1.7.2 Issue Escalation\n\n\n\n\n\n\nflowchart TB\n    I[Issue Identified] --&gt; T{Severity?}\n    T --&gt;|Low| S1[Support Team]\n    T --&gt;|Medium| S2[Project Team]\n    T --&gt;|High| S3[Steering Committee]\n    T --&gt;|Critical| S4[Executive Sponsor]\n    \n    S1 --&gt; R[Resolution]\n    S2 --&gt; R\n    S3 --&gt; R\n    S4 --&gt; R\n\n\n\n\nFigure 10.2: Issue Escalation Path\n\n\n\n\n\n\n\n\n10.1.8 Communication During Implementation\n\n10.1.8.1 Stakeholder Updates\n\n\n\n\n\n\n\n\n\nAudience\nFrequency\nContent\nChannel\n\n\n\n\nExecutive Sponsors\nBi-weekly\nMilestone status, risks, decisions needed\nMeeting, dashboard\n\n\nProject Team\nDaily\nProgress, blockers, coordination\nStandup, chat\n\n\nEnd Users\nWeekly during rollout\nTraining, go-live dates, support\nEmail, newsletter\n\n\nExternal Partners\nAs needed\nIntegration status, requirements\nMeeting, documentation\n\n\n\n\n\n\n10.1.9 Deliverables from This Phase\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nWorking Software\nOperational Program\nDeliver the solution\n\n\nTest Results\nPilot Evaluation\nValidate quality\n\n\nTraining Materials\nCapacity Building Resources\nEnable users\n\n\nRelease Notes\nImplementation Updates\nCommunicate changes\n\n\nSupport Documentation\nOperational Guides\nEnable ongoing use\n\n\n\n\n\n10.1.10 Moving Forward\nWith the solution implemented, the next phase focuses on Evaluation: measuring outcomes, assessing value delivered, and identifying opportunities for continuous improvement.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implementation & Execution</span>"
    ]
  },
  {
    "objectID": "chapters/09-evaluation.html",
    "href": "chapters/09-evaluation.html",
    "title": "11  Evaluation & Improvement",
    "section": "",
    "text": "11.1 Evaluation & Continuous Improvement\nDid we solve the problem? Are outcomes improving? What should we do differently? In business analysis, this phase encompasses Solution Evaluation and Continuous Improvement. In public health, it maps to Program Evaluation using frameworks like the CDC Evaluation Framework. Both seek to measure value delivered and inform future action.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluation & Improvement</span>"
    ]
  },
  {
    "objectID": "chapters/09-evaluation.html#evaluation-continuous-improvement",
    "href": "chapters/09-evaluation.html#evaluation-continuous-improvement",
    "title": "11  Evaluation & Improvement",
    "section": "",
    "text": "11.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSolution Evaluation\nProgram Evaluation\n\n\nKPI Tracking\nHealth Indicator Monitoring\n\n\nROI Analysis\nCost-Effectiveness Analysis\n\n\nLessons Learned\nAfter-Action Review\n\n\nContinuous Improvement\nQuality Improvement (QI)\n\n\n\n\n\n11.1.2 The CDC Evaluation Framework\nPublic health evaluation follows a well-established framework that parallels BA evaluation practices:\n\n\n\n\n\n\nflowchart LR\n    A[Engage&lt;br/&gt;Stakeholders] --&gt; B[Describe the&lt;br/&gt;Program]\n    B --&gt; C[Focus the&lt;br/&gt;Evaluation]\n    C --&gt; D[Gather Credible&lt;br/&gt;Evidence]\n    D --&gt; E[Justify&lt;br/&gt;Conclusions]\n    E --&gt; F[Ensure Use &&lt;br/&gt;Share Lessons]\n    F -.-&gt; A\n\n\n\n\nFigure 11.1: CDC Evaluation Framework Steps\n\n\n\n\n\n\n\n\nCDC Step\nBA Equivalent\n\n\n\n\nEngage Stakeholders\nIdentify evaluation stakeholders\n\n\nDescribe the Program\nDocument solution scope and objectives\n\n\nFocus the Evaluation\nDefine evaluation questions and scope\n\n\nGather Credible Evidence\nCollect metrics and feedback\n\n\nJustify Conclusions\nAnalyze data, determine value delivered\n\n\nEnsure Use and Share Lessons\nCommunicate results, inform decisions\n\n\n\n\n\n11.1.3 Types of Evaluation\n\n11.1.3.1 Formative vs. Summative\n\n\n\n\n\n\n\n\n\n\nType\nWhen\nPurpose\nBA Example\nPH Example\n\n\n\n\nFormative\nDuring implementation\nImprove the intervention\nSprint reviews, usability testing\nPDSA cycles, pilot feedback\n\n\nSummative\nAfter implementation\nJudge overall value\nPost-implementation review\nAnnual program evaluation\n\n\n\n\n\n11.1.3.2 Process vs. Outcome\n\n\n\n\n\n\n\n\n\nType\nFocus\nQuestions\nMetrics\n\n\n\n\nProcess\nHow well did we implement?\nWas the solution delivered as designed?\nAdoption rates, fidelity measures\n\n\nOutcome\nWhat difference did it make?\nDid we achieve intended results?\nHealth indicators, KPIs\n\n\nImpact\nLong-term effects\nWhat is the lasting change?\nPopulation health trends\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nProcess Evaluation:\n\nWere all registrars trained? (Target: 100%)\nAre hospitals submitting data electronically? (Target: 90%)\nIs the system meeting uptime requirements? (Target: 99.9%)\n\nOutcome Evaluation:\n\nHas data completeness improved? (Target: 89% → 95%)\nHas case abstraction time decreased? (Target: 15 min → 8 min)\nAre NPCR submissions timely? (Target: 100% on-time)\n\nImpact Evaluation:\n\nAre survival statistics more accurate?\nCan disparities be identified at the county level?\nHas the data informed state cancer plan priorities?\n\n\n\n\n\n\n11.1.4 Defining Metrics\n\n11.1.4.1 KPIs and Health Indicators\nMetrics should be SMART (Specific, Measurable, Achievable, Relevant, Time-bound):\n\n\n\nBA KPI\nPH Health Indicator\nMeasurement\n\n\n\n\nSystem uptime\nService availability\n% time operational\n\n\nUser adoption\nProgram reach\n% target users active\n\n\nTask completion time\nEfficiency\nMinutes per case abstraction\n\n\nError rate\nData quality\n% records with errors\n\n\nUser satisfaction\nAcceptability\nSurvey scores\n\n\n\n\n\n11.1.4.2 Building a Balanced Scorecard\nConsider multiple dimensions of value:\n\n\n\n\n\n\n\n\nDimension\nBA Metrics\nPH Metrics\n\n\n\n\nFinancial\nCost savings, ROI\nCost per case, grant compliance\n\n\nCustomer\nUser satisfaction, NPS\nRegistrar satisfaction, partner feedback\n\n\nInternal Process\nEfficiency gains, quality\nData completeness, timeliness\n\n\nLearning & Growth\nSkill development, innovation\nWorkforce capacity, continuous improvement\n\n\n\n\n\n\n11.1.5 Data Collection for Evaluation\n\n11.1.5.1 Sources of Evidence\n\n\n\n\n\n\n\n\nSource\nBA Application\nPH Application\n\n\n\n\nSystem Logs\nUsage analytics, performance data\nData submission tracking\n\n\nSurveys\nUser satisfaction, feature requests\nRegistrar feedback, partner surveys\n\n\nInterviews\nDetailed user feedback\nKey informant perspectives\n\n\nDocument Review\nProject artifacts, change logs\nReports, protocols\n\n\nObservation\nUsability testing\nWorkflow observation\n\n\nAdministrative Data\nSupport tickets, defects\nProgram records, health data\n\n\n\n\n\n11.1.5.2 Evaluation Plan Components\n\n\n\n\n\n\n\n\nComponent\nDescription\nCancerSurv Example\n\n\n\n\nQuestions\nWhat do we want to know?\nHas data quality improved?\n\n\nIndicators\nHow will we measure?\n% records passing NAACCR edits\n\n\nData Sources\nWhere will we get data?\nCancerSurv quality reports\n\n\nMethods\nHow will we collect?\nAutomated monthly extraction\n\n\nTimeline\nWhen will we measure?\nBaseline, 6 months, 12 months\n\n\nResponsibilities\nWho will do it?\nRegistry data quality manager\n\n\n\n\n\n\n11.1.6 Analysis and Interpretation\n\n11.1.6.1 Comparing to Baseline\nEffective evaluation requires baseline data:\n\n\n\n\n\n\nxychart-beta\n    title \"CancerSurv Data Completeness\"\n    x-axis [Baseline, Q1, Q2, Q3, Q4]\n    y-axis \"Completeness (%)\" 85 --&gt; 100\n    line [89, 91, 93, 94, 96]\n    line [95, 95, 95, 95, 95]\n\n\n\n\nFigure 11.2: Data Completeness Trend\n\n\n\n\n\n\n\n11.1.6.2 Interpreting Results\n\n\n\n\n\n\n\n\nResult\nInterpretation\nAction\n\n\n\n\nExceeds target\nSuccess; potential to raise bar\nDocument best practices; set stretch goals\n\n\nMeets target\nSuccess; sustain performance\nContinue current approach; monitor\n\n\nBelow target, improving\nProgress; maintain effort\nIdentify accelerators; address barriers\n\n\nBelow target, flat/declining\nConcern; intervention needed\nRoot cause analysis; corrective action\n\n\n\n\n\n\n11.1.7 Communicating Results\n\n11.1.7.1 Tailoring Messages\n\n\n\n\n\n\n\n\n\nAudience\nInterest\nFormat\nContent Emphasis\n\n\n\n\nExecutive sponsors\nBottom line, strategic alignment\nExecutive summary, dashboard\nROI, milestone achievement\n\n\nFunders (CDC, grants)\nCompliance, outcomes\nFormal reports\nGrant objective progress\n\n\nProject team\nDetailed performance\nWorking reports, retrospectives\nSpecific metrics, lessons learned\n\n\nEnd users\nHow it helps them\nNewsletters, town halls\nEfficiency gains, new features\n\n\n\n\n\n11.1.7.2 Visualization Best Practices\n\nUse clear, simple charts\nShow trends, not just snapshots\nCompare to targets/benchmarks\nHighlight key takeaways\nMake data accessible\n\n\n\n\n11.1.8 Continuous Improvement\n\n11.1.8.1 The QI Cycle\nEvaluation feeds continuous improvement:\n\n\n\n\n\n\nflowchart LR\n    A[Identify&lt;br/&gt;Opportunity] --&gt; B[Analyze Root&lt;br/&gt;Cause]\n    B --&gt; C[Design&lt;br/&gt;Improvement]\n    C --&gt; D[Implement&lt;br/&gt;Change]\n    D --&gt; E[Evaluate&lt;br/&gt;Results]\n    E --&gt; F{Successful?}\n    F --&gt;|Yes| G[Standardize]\n    F --&gt;|No| A\n    G --&gt; H[Monitor]\n    H --&gt; A\n\n\n\n\nFigure 11.3: Continuous Improvement Cycle\n\n\n\n\n\n\n\n11.1.8.2 Retrospectives and After-Action Reviews\n\n\n\n\n\n\n\n\nElement\nAgile Retrospective\nPH After-Action Review\n\n\n\n\nWhat went well?\nSprint successes\nProgram strengths\n\n\nWhat could improve?\nSprint challenges\nProgram gaps\n\n\nWhat will we do differently?\nAction items for next sprint\nRecommendations\n\n\nWho is responsible?\nTeam member assignments\nAction owners\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\n12-Month Evaluation Summary:\n\n\n\nMetric\nBaseline\nTarget\nActual\nStatus\n\n\n\n\nData completeness\n89%\n95%\n96%\n✅ Exceeded\n\n\nAbstraction time\n15 min\n8 min\n9 min\n⚠️ Close\n\n\nUser satisfaction\nN/A\n80%\n85%\n✅ Exceeded\n\n\nNPCR submission\n85% on-time\n100%\n100%\n✅ Met\n\n\nSystem uptime\nN/A\n99.9%\n99.7%\n⚠️ Close\n\n\n\nKey Findings:\n\nData quality improvements exceeded expectations\nAbstraction time reduced but not to target; workflow analysis needed\nTwo outages impacted uptime; infrastructure improvements planned\n\nRecommendations:\n\nContinue current data quality processes\nConduct workflow study to identify remaining abstraction bottlenecks\nImplement redundant infrastructure for high availability\n\n\n\n\n\n\n11.1.9 Sustaining Value\n\n11.1.9.1 From Project to Operations\nEvaluation supports the transition from project mode to operations:\n\n\n\nProject Phase\nOperational Phase\n\n\n\n\nProject team manages\nOperations team manages\n\n\nChange requests\nEnhancement requests\n\n\nImplementation metrics\nOperational metrics\n\n\nGo-live success\nOngoing performance\n\n\nProject budget\nOperating budget\n\n\n\n\n\n11.1.9.2 Governance for Continuous Improvement\nEstablish ongoing governance:\n\nRegular metric reviews (monthly/quarterly)\nUser feedback channels\nEnhancement prioritization process\nPerformance monitoring\nPeriodic comprehensive evaluations\n\n\n\n\n11.1.10 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nSolution Evaluation Report\nProgram Evaluation Report\nDocument outcomes\n\n\nLessons Learned\nAfter-Action Review\nCapture knowledge\n\n\nPerformance Dashboard\nHealth Indicator Dashboard\nMonitor ongoing performance\n\n\nImprovement Recommendations\nQI Action Plan\nDrive continuous improvement\n\n\nTransition Documentation\nSustainability Plan\nEnable long-term success\n\n\n\n\n\n11.1.11 Moving Forward\nWith the core analysis process complete, the following chapters provide additional resources: tools comparison (Chapter 10), implementation science frameworks (Chapter 11), templates (Appendix A), and a comprehensive glossary (Appendix C).",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluation & Improvement</span>"
    ]
  },
  {
    "objectID": "chapters/10-tools.html",
    "href": "chapters/10-tools.html",
    "title": "12  Tools Comparison",
    "section": "",
    "text": "12.1 Commercial vs. Open Source/Public Health Tools\nPublic health agencies often operate with constrained budgets while managing sensitive health data. This chapter compares commercial enterprise tools with open source and public health-specific alternatives, helping you select the right tools for your context.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tools Comparison</span>"
    ]
  },
  {
    "objectID": "chapters/10-tools.html#commercial-vs.-open-sourcepublic-health-tools",
    "href": "chapters/10-tools.html#commercial-vs.-open-sourcepublic-health-tools",
    "title": "12  Tools Comparison",
    "section": "",
    "text": "12.1.1 Selection Criteria\nWhen evaluating tools, consider:\n\n\n\nCriterion\nCommercial Advantage\nOSS/PH Advantage\n\n\n\n\nCost\nPredictable licensing\nNo license fees\n\n\nSupport\nVendor SLAs\nCommunity + self-reliance\n\n\nFeatures\nPolished, integrated\nCustomizable, extensible\n\n\nCompliance\nOften pre-certified\nFull control over data\n\n\nData Sovereignty\nVendor-managed\nOrganization-controlled\n\n\nSustainability\nVendor roadmap\nCommunity-driven\n\n\n\n\n\n12.1.2 Tool Categories\n\n12.1.2.1 Project Management\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nFull PM Suite\nJira, Azure DevOps, MS Project\nOpenProject, Redmine, Taiga\n\n\nKanban Boards\nTrello (paid), Monday.com\nTrello (free tier), Wekan, Kanboard\n\n\nAgile Planning\nJira, Rally, VersionOne\nTaiga, OpenProject\n\n\nGrant Management\nSmartsheet, Asana\nOpenProject with custom fields\n\n\n\nRecommendation for Public Health:\n\nSmall teams (&lt;10): Trello free tier or Taiga for simple Kanban/Scrum\nLarger programs: OpenProject for full PM capabilities with data sovereignty\nCDC/Federal projects: Often require Azure DevOps or Jira per contract\n\n\n\n\n\n\n\nTipWhen to Choose OSS\n\n\n\nChoose open source when:\n\nBudget is constrained\nData sovereignty is critical (cannot store project data externally)\nTechnical staff can support installation and maintenance\nCustomization is needed beyond commercial options\n\n\n\n\n\n12.1.2.2 Requirements and Documentation\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nWiki/Docs\nConfluence, SharePoint\nBookStack, MediaWiki, GitHub Wiki\n\n\nRequirements Management\nJama, Helix RM, DOORS\nGitHub Issues, GitLab, Notion (free)\n\n\nCollaborative Editing\nMS 365, Google Workspace\nNextcloud, CryptPad, HedgeDoc\n\n\n\nRecommendation for Public Health:\n\nDocumentation: BookStack provides Confluence-like experience without licensing\nRequirements: GitHub Issues sufficient for most projects; integrates with development\nCollaboration: Consider data sensitivity; Nextcloud for on-premise control\n\n\n\n12.1.2.3 Diagramming\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nGeneral Diagramming\nVisio, Lucidchart\ndiagrams.net (draw.io), Mermaid\n\n\nProcess Modeling (BPMN)\nVisio, Bizagi\ndiagrams.net, Camunda Modeler\n\n\nArchitecture\nLucidchart, Visio\ndiagrams.net, PlantUML\n\n\n\nRecommendation for Public Health:\n\ndiagrams.net is the de facto standard in public sector: free, web-based, exports to multiple formats, works offline\nMermaid for diagrams in documentation (renders from text, version-controllable)\n\n\n\n12.1.2.4 Data Collection\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nSurveys\nQualtrics, SurveyMonkey\nLimeSurvey, KoBoToolbox\n\n\nClinical/Research Data\nREDCap (free for research)\nREDCap, ODK, DHIS2\n\n\nForms\nMicrosoft Forms, Google Forms\nKoBoToolbox, ODK Collect\n\n\nCase Management\nSalesforce\nDHIS2, CommCare\n\n\n\nREDCap: The Public Health Standard\nREDCap (Research Electronic Data Capture) deserves special mention:\n\nFree for non-profit research institutions\nHIPAA-compliant, 21 CFR Part 11 capable\nSupports complex branching logic, validation\nBuilt-in audit trails\nConsortium of 6,000+ institutions\nCDC and NIH approved\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nFor the CancerSurv project, data collection tools include:\n\nREDCap: Pilot site feedback surveys, user satisfaction assessments\nKoBoToolbox: Field data collection for mobile cancer screening events\nNative CancerSurv: Case abstraction (built into the platform)\n\n\n\n\n\n12.1.2.5 Data Analysis\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nStatistical Analysis\nSAS, SPSS, Stata\nR, Python (pandas, scipy)\n\n\nEpidemiological Analysis\nSAS, Stata\nR (epitools), Epi Info\n\n\nData Wrangling\nAlteryx, Trifacta\nR (tidyverse), Python (pandas)\n\n\nNotebooks\nDatabricks, SAS Studio\nJupyter, RStudio, Quarto\n\n\n\nEpi Info: CDC’s Free Epidemiology Tool\nEpi Info is developed by CDC specifically for outbreak investigation:\n\nFree download, no installation fees\nBuilt-in epidemiological statistics (odds ratios, relative risks)\nEpidemic curve generation\nGeographic mapping\nSurvey development and analysis\n7-day moving averages, case fatality rates\n\nR for Public Health\nR has become the standard for public health analytics:\n# Example: Calculate age-adjusted incidence rate\nlibrary(epitools)\nlibrary(tidyverse)\n\ncancer_data %&gt;%\n  group_by(county, year) %&gt;%\n  summarize(\n    cases = n(),\n    population = first(population),\n    crude_rate = cases / population * 100000\n  ) %&gt;%\n  # Age adjustment using standard population\n  ageadjust.direct(count = cases, pop = population, stdpop = us_std_pop)\n\n\n12.1.2.6 Data Platform Architecture\nModern public health data systems benefit from structured data architectures that organize information from raw ingestion through analytics-ready outputs. The medallion architecture (Bronze → Silver → Gold) provides a framework for designing scalable, maintainable data platforms.\nWhile often discussed in cloud contexts, medallion architecture works equally well on desktop computers and local servers. The key is the logical separation of data by refinement stage, not the specific technology.\n\n\n\n\n\n\nWarningCommunication Tip\n\n\n\n“Medallion architecture” and “Bronze/Silver/Gold” are IT jargon unfamiliar to most public health professionals. When discussing data workflows with epidemiologists or program staff, use terms like “raw data,” “cleaned data,” and “final reports” instead. See the Terminology Dictionary for a complete translation guide.\n\n\nCommercial vs. Open Source Data Platforms\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nData Lake / Lakehouse\nDatabricks, Snowflake, Azure Synapse\nApache Spark + Delta Lake, Apache Iceberg, DuckDB\n\n\nETL/Orchestration\nAzure Data Factory, Informatica, Talend\nApache Airflow, Dagster, Prefect, dbt\n\n\nData Catalog\nAlation, Collibra\nApache Atlas, DataHub, Amundsen\n\n\nData Quality\nInformatica DQ, Talend\nGreat Expectations, dbt tests, Soda\n\n\n\nImplementing Medallion Architecture\nThe medallion architecture can be implemented with various tool combinations, from enterprise cloud platforms to desktop applications:\n\n\n\n\n\n\n\n\n\nLayer\nPurpose\nCloud/Server Options\nDesktop/Local Options\n\n\n\n\nBronze\nRaw data landing, preserve source fidelity\nObject storage (S3, Azure Blob), PostgreSQL staging tables\nFile folders, SQLite database, Excel “Raw Data” sheets\n\n\nSilver\nCleansing, standardization, deduplication\ndbt transformations, Apache Spark, Python/pandas\nExcel Power Query, Python scripts, Access queries\n\n\nGold\nAnalytics-ready datasets, aggregations\nDimensional models, materialized views, OLAP cubes\nPivot tables, final Excel reports, exported CSVs for tools\n\n\n\n\n\n\n\n\n\nTipStarting Small\n\n\n\nYou don’t need Databricks or Snowflake to implement medallion architecture. Even a well-organized folder structure with clear naming conventions implements the same principle:\nproject/\n├── 01_bronze/          # Raw files as received\n│   ├── lab_results_2024-01-15.csv\n│   └── ehr_export_raw.xlsx\n├── 02_silver/          # Cleaned and standardized\n│   ├── cases_cleaned.csv\n│   └── patients_deduplicated.xlsx\n└── 03_gold/            # Ready for analysis/reporting\n    ├── outbreak_line_list.xlsx\n    └── monthly_summary_report.xlsx\nMany state health departments successfully run medallion architectures on modest infrastructure, including single PostgreSQL databases with three schemas or even organized Excel workbooks.\n\n\nOpen Source Lakehouse Stack for Public Health\nFor organizations seeking data sovereignty and cost control:\n\n\n\n\n\n\n\n\nComponent\nTool\nNotes\n\n\n\n\nStorage\nMinIO or local filesystem\nS3-compatible object storage\n\n\nTable format\nDelta Lake or Apache Iceberg\nACID transactions, time travel\n\n\nCompute\nApache Spark or DuckDB\nDuckDB for smaller workloads\n\n\nOrchestration\nApache Airflow\nWorkflow scheduling\n\n\nTransformation\ndbt\nSQL-based transformations\n\n\nQuality\nGreat Expectations\nData validation\n\n\nCatalog\nDataHub\nMetadata management\n\n\n\nData Architecture for Different Scales\n\n\n\n\n\n\n\n\n\nOrganization Size\nRecommended Approach\nKey Tools\nTypical Staffing\n\n\n\n\nIndividual analyst\nOrganized folders with naming conventions\nExcel, Python/R scripts, SQLite\nSingle epidemiologist or data manager handles all layers\n\n\nSmall program\nSingle PostgreSQL database with layered schemas\nPostgreSQL, dbt, Python\n1-2 staff share responsibilities across layers\n\n\nMedium health department\nData warehouse with ETL pipeline\nPostgreSQL/Snowflake, Airflow, dbt\nDedicated data team with some role specialization\n\n\nLarge state/federal\nFull lakehouse architecture\nSpark/Databricks, Delta Lake, Airflow, dbt\nSpecialized roles: data engineers (Bronze/Silver), analysts (Silver/Gold), BI developers (Gold)\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nCancerSurv implements a medallion architecture using open source tools:\n\n\n\n\n\n\n\n\nLayer\nImplementation\nGold Layer Outputs\n\n\n\n\nBronze\nPostgreSQL raw schema; HL7 messages stored as JSON; CSV uploads preserved verbatim\n—\n\n\nSilver\nPostgreSQL staging schema; dbt models for deduplication and ICD-O-3 standardization\n—\n\n\nGold\nPostgreSQL analytics schema; pre-computed incidence rates, survival metrics, NPCR submission tables\nLine lists for case follow-up, incidence reports, survival dashboards\n\n\nOrchestration\nApache Airflow schedules daily Bronze→Silver→Gold pipeline\n—\n\n\nQuality\nGreat Expectations validates data at Silver layer before promotion to Gold\n—\n\n\n\n\n\n\n\n12.1.2.7 Visualization\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nDashboards\nTableau, Power BI\nR Shiny, Dash (Python), Apache Superset\n\n\nStatic Visualization\nTableau, Excel\nR (ggplot2), Python (matplotlib, plotly)\n\n\nInteractive Charts\nTableau, Power BI\nPlotly, Highcharts (free for non-commercial)\n\n\n\nR Shiny for Public Health Dashboards\nR Shiny enables interactive dashboards without JavaScript expertise:\n\nFree and open source\nIntegrates with R analysis pipelines\nCan be deployed on-premise or Shinyapps.io\nMany public health templates available\n\n\n\n12.1.2.8 GIS and Mapping\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nDesktop GIS\nArcGIS Pro\nQGIS\n\n\nWeb Mapping\nArcGIS Online, Mapbox\nLeaflet, OpenLayers\n\n\nSpatial Analysis\nArcGIS, ESRI\nQGIS, R (sf package), PostGIS\n\n\nGeocoding\nGoogle, ESRI\nNominatim, US Census Geocoder\n\n\n\nQGIS for Disease Mapping\nQGIS is essential for spatial epidemiology:\n\nFree, cross-platform\nFull-featured GIS capabilities\nDisease mapping and cluster detection\nIntegrates with R for spatial statistics\nActive public health user community\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nCancerSurv analytics stack:\n\n\n\n\n\n\n\n\nFunction\nTool\nRationale\n\n\n\n\nCase data storage\nPostgreSQL\nOpen source, HIPAA-capable\n\n\nETL/Data pipeline\nApache Airflow\nOrchestration of data flows\n\n\nStatistical analysis\nR (tidyverse, survival)\nStandard for cancer epidemiology\n\n\nDashboards\nR Shiny\nInteractive, deployable on-premise\n\n\nGeographic mapping\nQGIS + Leaflet\nCancer cluster visualization\n\n\nAd-hoc queries\nApache Superset\nSelf-service for epidemiologists\n\n\n\n\n\n\n\n12.1.2.9 Data Standards and Interoperability\n\n\n\nStandard\nCommercial Tools\nOSS Tools\n\n\n\n\nHL7 FHIR\nRhapsody, Corepoint\nHAPI FHIR, LinuxForHealth\n\n\nHL7 v2.x\nRhapsody, Mirth\nMirth Connect (open source), HAPI\n\n\nCDA/C-CDA\nVarious EHR vendors\nMDHT, Reference CDA\n\n\n\nMirth Connect\nMirth Connect is widely used in public health for health information exchange:\n\nOpen source (NextGen Healthcare)\nHL7 v2, FHIR, CDA support\nVisual interface builder\nUsed by many state health departments\n\n\n\n\n12.1.3 Building Your Stack\n\n12.1.3.1 Small Public Health Program\n\n\n\nFunction\nRecommended Tool\nNotes\n\n\n\n\nProject Management\nTrello or Taiga\nFree tier sufficient\n\n\nDocumentation\nGitHub Wiki or BookStack\nVersion-controlled\n\n\nDiagramming\ndiagrams.net\nFree, export to any format\n\n\nData Collection\nREDCap\nStandard for research\n\n\nAnalysis\nR + RStudio\nFree, extensive packages\n\n\nVisualization\nR Shiny or Excel\nDepends on technical capacity\n\n\n\n\n\n12.1.3.2 Large State Health Department\n\n\n\nFunction\nRecommended Tool\nNotes\n\n\n\n\nProject Management\nAzure DevOps or OpenProject\nEnterprise scale\n\n\nDocumentation\nConfluence or BookStack\nTeam collaboration\n\n\nRequirements\nJira or GitHub\nIntegrated with development\n\n\nData Collection\nREDCap + DHIS2\nResearch + program monitoring\n\n\nData Platform\nPostgreSQL + Airflow\nScalable, HIPAA-capable\n\n\nAnalysis\nR + Python\nComprehensive capabilities\n\n\nVisualization\nR Shiny + Superset\nDashboards + self-service\n\n\nGIS\nQGIS + PostGIS\nFull spatial capabilities\n\n\nIntegration\nMirth Connect\nHL7/FHIR integration\n\n\n\n\n\n\n12.1.4 Considerations for Tool Selection\n\n12.1.4.1 Total Cost of Ownership\nFree software is not always cheaper:\n\n\n\nCost Factor\nCommercial\nOpen Source\n\n\n\n\nLicense fees\nYes\nNo\n\n\nImplementation\nVendor/partner\nInternal/consultant\n\n\nTraining\nOften included\nSelf-directed or purchased\n\n\nSupport\nIncluded in license\nCommunity or purchased\n\n\nCustomization\nLimited\nUnlimited but costly\n\n\nInfrastructure\nCloud included or on-prem\nYou manage\n\n\n\n\n\n12.1.4.2 Compliance and Security\n\n\n\n\n\n\n\n\nConsideration\nCommercial\nOpen Source\n\n\n\n\nHIPAA compliance\nOften certified\nYour responsibility to configure\n\n\nSOC 2 certification\nCommon\nRare; your responsibility\n\n\nSecurity updates\nVendor manages\nYou monitor and apply\n\n\nAudit trails\nBuilt-in\nMay require configuration\n\n\n\n\n\n12.1.4.3 Sustainability\nConsider long-term viability:\n\nCommercial: Vendor may be acquired, change pricing, sunset product\nOpen Source: Community may lose momentum; check activity levels\nHybrid: Consider tools with both commercial and open source options\n\n\n\n\n12.1.5 Summary\nThe choice between commercial and open source tools depends on your context: budget, technical capacity, data sensitivity, and compliance requirements. Public health has excellent open source options, particularly for data collection (REDCap), analysis (R), mapping (QGIS), and integration (Mirth Connect). Evaluate total cost of ownership, not just license fees.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tools Comparison</span>"
    ]
  },
  {
    "objectID": "chapters/11-implementation-science.html",
    "href": "chapters/11-implementation-science.html",
    "title": "13  Implementation Science",
    "section": "",
    "text": "13.1 CFIR and Implementation Frameworks\nWhy do evidence-based interventions fail to achieve expected outcomes when deployed in the real world? Implementation science provides the answer: the gap between efficacy (works under ideal conditions) and effectiveness (works in practice) is bridged by understanding implementation context. This chapter introduces key frameworks that help business analysts anticipate and address adoption barriers.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implementation Science</span>"
    ]
  },
  {
    "objectID": "chapters/11-implementation-science.html#cfir-and-implementation-frameworks",
    "href": "chapters/11-implementation-science.html#cfir-and-implementation-frameworks",
    "title": "13  Implementation Science",
    "section": "",
    "text": "13.1.1 Why Implementation Science Matters for BA\nTraditional requirements focus on what a system must do. Implementation science asks: Will people actually use it? This question is critical because:\n\n70% of change initiatives fail to achieve their objectives\nClinical guidelines take an average of 17 years to become standard practice\nTechnology adoption depends on factors beyond functionality\n\nFor the business analyst, implementation science provides:\n\nA structured way to assess organizational readiness\nLanguage for discussing non-technical barriers with stakeholders\nFrameworks for designing implementation strategies\nMetrics for measuring adoption, not just deployment\n\n\n\n13.1.2 The Consolidated Framework for Implementation Research (CFIR)\nCFIR is the most widely used implementation science framework. It organizes factors influencing implementation into five domains:\n\n\n\n\n\n\nflowchart TB\n    subgraph CFIR[\"CFIR Framework\"]\n        A[Intervention&lt;br/&gt;Characteristics]\n        B[Outer&lt;br/&gt;Setting]\n        C[Inner&lt;br/&gt;Setting]\n        D[Characteristics&lt;br/&gt;of Individuals]\n        E[Process]\n    end\n    \n    A --&gt; F[Implementation&lt;br/&gt;Outcome]\n    B --&gt; F\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n\n\n\n\nFigure 13.1: CFIR Domains\n\n\n\n\n\n\n13.1.2.1 Domain 1: Intervention Characteristics\nProperties of the intervention itself that influence adoption:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nIntervention Source\nPerception of whether intervention is externally vs internally developed\nInvolve users in design; customize for local context\n\n\nEvidence Strength\nStakeholders’ perception of evidence supporting the intervention\nDocument benefits; reference standards (CDC, NAACCR)\n\n\nRelative Advantage\nPerception that the intervention is better than current practice\nQuantify improvements; demonstrate in pilot\n\n\nAdaptability\nDegree to which intervention can be modified for local needs\nBuild configurability; separate core from periphery\n\n\nTrialability\nAbility to test on a small scale\nSupport pilot deployments; sandbox environments\n\n\nComplexity\nPerceived difficulty of implementation\nSimplify UI; phase rollout; provide training\n\n\nDesign Quality\nPerceived excellence in how intervention is presented\nInvest in UX; professional appearance\n\n\nCost\nCosts of implementation and ongoing operation\nDocument TCO; demonstrate ROI\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example: Intervention Characteristics\n\n\n\n\n\n\n\n\n\n\n\nConstruct\nAssessment\nDesign Response\n\n\n\n\nRelative Advantage\nHigh: modern UI, remote access, better analytics\nEmphasize in training; demonstrate side-by-side\n\n\nComplexity\nMedium: new workflows, new interface\nPhased training; role-based simplified views\n\n\nAdaptability\nMedium: some local customization needed\nConfigurable report templates; custom fields\n\n\nTrialability\nHigh: pilot sites planned\n8-week pilot with 3 hospitals\n\n\n\n\n\n\n\n13.1.2.2 Domain 2: Outer Setting\nExternal context influencing the implementing organization:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nPatient/Community Needs\nExtent to which needs are known and prioritized\nGather community input; equity analysis\n\n\nCosmopolitanism\nDegree of networking with external organizations\nPlan for interoperability; support data sharing\n\n\nPeer Pressure\nCompetitive pressure from peer organizations\nReference successful implementations elsewhere\n\n\nExternal Policies\nExternal mandates, regulations, guidelines\nDocument compliance requirements early\n\n\n\nFor public health IT projects, outer setting often includes:\n\nCDC reporting requirements\nHIPAA regulations\nState health information exchange policies\nGrant funder expectations\nNAACCR standards (for cancer registries)\n\n\n\n13.1.2.3 Domain 3: Inner Setting\nInternal organizational context:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nStructural Characteristics\nOrganization size, maturity, structure\nAssess readiness; tailor approach\n\n\nNetworks & Communications\nInformation flow within organization\nPlan communication strategy\n\n\nCulture\nNorms, values, assumptions\nAlign with organizational culture\n\n\nImplementation Climate\nReceptivity to change\nAssess readiness; address resistance\n\n\nReadiness for Implementation\nTangible indicators of commitment\nSecure resources, leadership support\n\n\n\nAssessing Implementation Climate:\n\nIs there leadership commitment?\nAre resources allocated?\nIs there a sense of urgency?\nAre staff held accountable for adoption?\nAre early adopters rewarded?\n\n\n\n13.1.2.4 Domain 4: Characteristics of Individuals\nAttributes of people involved in implementation:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nKnowledge & Beliefs\nAttitudes toward the intervention\nEducation, demonstration, testimonials\n\n\nSelf-Efficacy\nConfidence in ability to use intervention\nTraining, support resources, simplification\n\n\nIndividual Stage of Change\nReadiness to adopt\nTailored engagement by readiness level\n\n\nIndividual Identification\nRelationship with organization\nLeverage organizational loyalty\n\n\n\n\n\n13.1.2.5 Domain 5: Process\nThe implementation process itself:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nPlanning\nDegree to which implementation is planned\nDetailed implementation plan\n\n\nEngaging\nAttracting and involving appropriate people\nStakeholder engagement strategy\n\n\nExecuting\nCarrying out implementation as planned\nProject management, monitoring\n\n\nReflecting & Evaluating\nFeedback about progress\nPDSA cycles, metrics, retrospectives\n\n\n\nKey Roles in Process:\n\nChampions: Individuals who advocate for the intervention\nOpinion Leaders: Respected individuals who influence peers\nImplementation Leaders: Those formally responsible\nExternal Change Agents: Consultants, vendors supporting implementation\n\n\n\n\n13.1.3 Mapping NFRs to CFIR\nA practical application of CFIR is translating non-functional requirements into implementation characteristics:\n\n\n\n\n\n\n\n\nNFR Category\nCFIR Mapping\nRequirement Example\n\n\n\n\nPerformance\nComplexity, Design Quality\n“Response time &lt;3 seconds to maintain workflow efficiency”\n\n\nUsability\nComplexity, Self-Efficacy\n“Interface requires &lt;4 hours training for basic proficiency”\n\n\nReliability\nRelative Advantage\n“99.9% uptime to maintain user confidence”\n\n\nScalability\nAdaptability\n“Support 2x current case volume for outbreak surge”\n\n\nSecurity\nExternal Policies\n“HIPAA-compliant access controls”\n\n\nInteroperability\nCosmopolitanism\n“HL7 FHIR APIs for health information exchange”\n\n\nAccessibility\nSelf-Efficacy\n“WCAG 2.1 AA compliance; support for screen readers”\n\n\n\n\n\n13.1.4 RE-AIM Framework\nRE-AIM provides a complementary framework focused on public health impact1,2:\n\n\n\n\n\n\n\n\nDimension\nDefinition\nMetric Examples\n\n\n\n\nReach\nProportion of target population participating\n% registrars using system; % facilities connected\n\n\nEffectiveness\nImpact on outcomes\nData completeness; abstraction time\n\n\nAdoption\nProportion of settings/staff adopting\n% hospitals submitting electronically\n\n\nImplementation\nFidelity to protocol; consistency\nAdherence to data standards; training completion\n\n\nMaintenance\nSustainability over time\nContinued use at 12 months; staff turnover impact\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example: RE-AIM Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nIndicator\nTarget\nActual (12 mo)\n\n\n\n\nReach\n% registrars trained\n100%\n98%\n\n\nEffectiveness\nData completeness\n95%\n96%\n\n\nAdoption\n% hospitals on ELR\n90%\n87%\n\n\nImplementation\nTraining completion rate\n95%\n92%\n\n\nMaintenance\nActive users at 12 mo\n90%\n94%\n\n\n\n\n\n\n\n13.1.5 Applying Implementation Science in Practice\n\n13.1.5.1 During Planning\n\nConduct CFIR-based readiness assessment\nIdentify potential barriers across all domains\nDesign implementation strategies to address barriers\n\n\n\n13.1.5.2 During Design\n\nEnsure intervention characteristics support adoption\nBuild in adaptability for local context\nMinimize complexity; maximize relative advantage\n\n\n\n13.1.5.3 During Implementation\n\nEngage champions and opinion leaders\nMonitor adoption, not just deployment\nUse PDSA cycles to address emerging barriers\n\n\n\n13.1.5.4 During Evaluation\n\nAssess both implementation outcomes and intervention outcomes\nUse RE-AIM dimensions for comprehensive evaluation\nDocument lessons for future implementations\n\n\n\n\n13.1.6 Implementation Strategies\nWhen barriers are identified, select appropriate implementation strategies:\n\n\n\n\n\n\n\n\nBarrier\nStrategy Category\nExample Strategies\n\n\n\n\nLack of knowledge\nTraining & Education\nWorkshops, e-learning, job aids\n\n\nLow self-efficacy\nSupport\nHelp desk, super-users, mentoring\n\n\nResistance to change\nStakeholder Engagement\nChampions, leadership messaging\n\n\nWorkflow disruption\nPlanning\nPhased rollout, parallel operation\n\n\nResource constraints\nInfrastructure\nDedicated staff, protected time\n\n\nComplexity\nIntervention Modification\nSimplified views, guided workflows\n\n\n\n\n\n13.1.7 Summary\nImplementation science provides business analysts with frameworks to anticipate and address the human and organizational factors that determine whether a technically sound solution actually achieves its intended outcomes. By incorporating CFIR assessment into requirements gathering and using RE-AIM for evaluation, hybrid BA/PH projects can bridge the gap between deployment and adoption.\nKey takeaways:\n\nRequirements must address adoption, not just functionality\nCFIR provides a comprehensive lens for assessing implementation context\nNFRs should map to implementation characteristics\nRE-AIM offers a framework for evaluating public health impact\nImplementation strategies should target specific barriers\n\n\n\n\n\n\n\n1. RE-AIM – Home – Reach Effectiveness Adoption Implementation Maintenance [Internet]. [cited 2026 Jan 9]. Available from: https://re-aim.org/\n\n\n2. 1999 – RE-AIM [Internet]. [cited 2026 Jan 9]. Available from: https://re-aim.org/1999/",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implementation Science</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html",
    "href": "chapters/12-process-optimization.html",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "",
    "text": "14.1 Optimizing for Impact\nBusiness analysts and public health analysts share a fundamental responsibility: identifying inefficiencies and optimizing processes to maximize program impact. Whether measuring return on investment (BA) or population health outcomes (PH), both domains must demonstrate tangible value. In public health, that value must ultimately be apparent to the taxpayer funding these programs.\nThis chapter synthesizes strategies from psychology, sociology, project management, informatics, and organizational behavior to create a comprehensive approach to process optimization.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#optimizing-for-impact",
    "href": "chapters/12-process-optimization.html#optimizing-for-impact",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "",
    "text": "14.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nProcess Improvement\nProgram Optimization\n\n\nOperational Efficiency\nResource Stewardship\n\n\nROI Demonstration\nTaxpayer Value\n\n\nAutomation Strategy\nScalable Interventions\n\n\nChange Management\nImplementation Science\n\n\n\n\n\n14.1.2 The Optimization Hierarchy\nBefore optimizing a process, apply this decision framework:\n\n\n\n\n\n\nflowchart TD\n    A[Identify Manual Process] --&gt; B{Mission Critical?}\n    B --&gt;|No| C[Eliminate]\n    B --&gt;|Yes| D{Can It Be Automated?}\n    D --&gt;|Yes| E[Automate with Oversight]\n    D --&gt;|No| F[Standardize & Document]\n    E --&gt; G{Scalable?}\n    F --&gt; G\n    G --&gt;|Yes| H[Implement]\n    G --&gt;|No| I[Reassess Requirements]\n\n\n\n\nFigure 14.1: Process Optimization Decision Tree\n\n\n\n\n\n\n\n\n\n\n\nImportantCore Principle\n\n\n\nAutomation and scale should be prioritized above aesthetics. A functional, scalable system that processes 10,000 records reliably is more valuable than a beautifully designed system that handles 100. Invest in robustness first; polish later.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#foundations-of-organizational-efficiency",
    "href": "chapters/12-process-optimization.html#foundations-of-organizational-efficiency",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.2 Foundations of Organizational Efficiency",
    "text": "14.2 Foundations of Organizational Efficiency\nProcess optimization does not happen in a vacuum. Organizations are complex systems where psychological, social, and structural factors interact. Understanding these foundations helps analysts anticipate barriers and design interventions more likely to succeed.\nEach section below identifies common barriers alongside practical remedies. These tables are not exhaustive but represent patterns frequently encountered in both BA and public health contexts.\n\n14.2.1 Psychological Foundations\nUnderstanding human motivation and well-being is essential for sustainable process improvement. Optimizing systems without considering the people who operate them leads to burnout, resistance, and ultimately failure.\n\n14.2.1.1 Self-Determination Theory (SDT)\nDeci and Ryan’s Self-Determination Theory identifies three innate psychological needs that drive motivation:\n\n\n\n\n\n\n\n\nNeed\nDescription\nProcess Implication\n\n\n\n\nAutonomy\nControl over one’s work\nAllow flexibility in how tasks are completed\n\n\nCompetence\nMastery and effectiveness\nProvide training, feedback, and achievable challenges\n\n\nRelatedness\nConnection to others\nFoster team collaboration and shared purpose\n\n\n\nApplication to Process Design:\n\nAutomated systems should augment, not replace, human decision-making\nStaff should understand why processes exist, not just how to follow them\nBuild in opportunities for skill development as systems evolve\n\n\n\n14.2.1.2 Barriers and Remedies\nCommon human-factor barriers that undermine optimization and efficiency, with practical alternatives:\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nBullying or intimidation\nStaff silence concerns, errors go unreported, talent leaves\nEstablish psychological safety as a core value; enforce clear anti-bullying policies; provide anonymous reporting channels; train managers to recognize and address harmful behaviors\n\n\nPoor communication styles\nMisunderstandings, duplicated effort, delayed decisions\nAdopt structured formats (SBAR for updates, decision logs for choices); practice active listening; summarize and confirm understanding before acting\n\n\nDisruptive interpersonal patterns\nUnpredictable decisions, fear-based culture, high turnover\nAddress observable behaviors rather than speculating on motives; document conduct expectations; use HR processes and governance to protect team function\n\n\nLeadership lacking domain expertise\nMisprioritized work, unrealistic timelines, poor resource allocation\nPair leaders with subject matter experts; require decision review gates for technical choices; schedule regular domain briefings\n\n\nResistance to change\nSlow adoption, workarounds, shadow systems\nCo-design solutions with end users; run small pilots before full rollout; provide training and support; align incentives with adoption\n\n\n\n\n\n14.2.1.3 Psychological Safety\nAmy Edmondson’s research on psychological safety demonstrates that teams perform better when members feel safe to take risks, ask questions, and admit mistakes without fear of punishment.\n\n\n\n\n\n\n\nSafety Element\nProcess Design Implication\n\n\n\n\nSpeaking up\nCreate feedback channels for process improvements\n\n\nRisk-taking\nAllow experimentation with PDSA cycles\n\n\nMistake tolerance\nDesign systems with error recovery, not just prevention\n\n\nHelp-seeking\nDocument processes so staff can learn independently\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nWhen implementing CancerSurv, the state registry created a “no-blame” error reporting system. Registrars could flag data quality issues or workflow problems without fear of criticism. This resulted in:\n\n47% more process improvement suggestions in the first quarter\nIdentification of a critical duplicate detection gap that would have affected NPCR submission\nHigher staff satisfaction scores (3.2 → 4.1 on 5-point scale)\n\n\n\n\n\n14.2.1.4 Job Demands-Resources Model\nThe JD-R model explains that job stress results from imbalance between demands and resources:\n\n\n\nJob Demands\nJob Resources\n\n\n\n\nWorkload\nAutonomy\n\n\nTime pressure\nSocial support\n\n\nComplexity\nFeedback\n\n\nAmbiguity\nGrowth opportunities\n\n\n\nProcess optimization should reduce demands while maintaining or increasing resources. Automation that simply increases throughput expectations without adding support leads to burnout.\n\n\n\n\n\n\nTipHuman-Factor Guardrails\n\n\n\nDesign processes that protect well-being:\n\nSet realistic throughput targets aligned to available resources\nProvide recovery mechanisms when errors occur\nRotate high-stress tasks to avoid fatigue concentration\nInclude regular check-ins focused on workload and support needs\n\n\n\n\n\n\n14.2.2 Sociological Foundations\nOrganizations are social systems. Process changes must account for group dynamics, power structures, and cultural norms.\n\n14.2.2.1 Organizational Culture (Schein)\nEdgar Schein’s model describes three levels of organizational culture:\n\nArtifacts: Visible structures and processes (what we see)\nEspoused Values: Stated strategies and goals (what we say)\nBasic Assumptions: Unconscious beliefs (what we actually believe)\n\nProcess optimization often fails when it addresses only artifacts while conflicting with basic assumptions. A new data system requiring transparency may fail in an organization where the underlying assumption is “information is power.”\n\n\n14.2.2.2 Barriers and Remedies\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nUnhealthy competition\nInformation hoarding, territorial behavior, duplicated effort\nEstablish shared goals that require collaboration; use team-based (not individual) incentives; conduct cross-team reviews to surface dependencies\n\n\nPolitical factors\nDecisions driven by influence rather than evidence; resource allocation disconnected from priorities\nDocument decision criteria transparently; maintain decision logs accessible to stakeholders; require conflict-of-interest declarations for major choices\n\n\nMisalignment across teams\nConflicting priorities, duplicated work, wasted resources\nUse cascading OKRs to connect team goals to organizational objectives; align work to shared logic models; maintain visible roadmaps showing dependencies\n\n\nIsolation of opposing views\nBlind spots in planning, groupthink, low trust from excluded parties\nPractice structured dissent (assign devil’s advocate roles); use red teaming to stress-test plans; ensure facilitation includes diverse perspectives\n\n\nInformal leadership undermining formal roles\nFragmented decision-making, unclear accountability, team confusion\nDefine RACI for all key processes; formally appoint accountable owners; channel leadership energy into collaborative problem-solving rather than competing authority\n\n\n\n\n\n14.2.2.3 Communities of Practice\nWenger’s Communities of Practice (CoP) concept describes how learning and knowledge sharing occur through social participation:\n\n\n\nCoP Element\nApplication to Process Optimization\n\n\n\n\nDomain\nShared competence (e.g., data quality)\n\n\nCommunity\nMembers who interact and learn together\n\n\nPractice\nShared resources, tools, and approaches\n\n\n\nEstablish cross-functional communities to prevent silos and ensure process improvements are adopted across the organization.\n\n\n14.2.2.4 Deduplication of Effort\nOne of the most significant inefficiencies in organizations is duplicated work: multiple teams solving the same problem independently. This represents wasted resources and inconsistent outcomes.\n\n\n\n\n\n\n\n\nProblem\nSolution\nMechanism\n\n\n\n\nMultiple teams cleaning the same data\nCentralized data team\nShared Bronze/Silver data layers\n\n\nParallel development of similar tools\nInternal tool registry\nSearchable repository of existing solutions\n\n\nReinventing processes\nProcess documentation\nWiki/knowledge base with templates\n\n\nRedundant meetings\nMeeting audit\nRegular review of recurring meetings\n\n\n\n\n\n\n\n\n\nTipCentralization Principle\n\n\n\nCore functions should be centralized. If multiple groups rely on the same data, that data should be centrally stored, cleaned, and documented. This ensures consistency, reduces duplicate effort, and allows specialized expertise to develop.\nExamples of centralizable functions:\n\nData cleaning and standardization\nReport generation and dissemination\nTraining material development\nCompliance documentation\n\n\n\n\n\n\n\n\n\nWarningSilo Risks\n\n\n\nDeduplication fails when:\n\nGoals and definitions differ across teams\nShared repositories lack findability or curation\nIncentives reward local optimization over system impact\n\nRemedies: establish taxonomy and metadata standards, maintain an internal tool registry, and set a review cadence for shared assets.\n\n\n\n\n\n14.2.3 Project Management Frameworks\nEffective process optimization requires disciplined project management. Choose frameworks appropriate to your context.\n\n14.2.3.1 Framework Comparison\n\n\n\n\n\n\n\n\nFramework\nBest For\nKey Features\n\n\n\n\nAgile/Scrum\nEvolving requirements, software\nIterative, adaptive, team-based\n\n\nKanban\nContinuous flow, operations\nVisual, WIP limits, pull-based\n\n\nWaterfall\nFixed requirements, compliance\nSequential, documented, predictable\n\n\nHybrid\nPublic health programs\nGrant milestones + iterative delivery\n\n\n\n\n\n14.2.3.2 Project Management Tools\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nFull PM Suite\nJira, Azure DevOps, MS Project\nOpenProject, Taiga\n\n\nKanban Boards\nTrello (paid), Monday.com\nTrello (free tier), Wekan\n\n\nTask Management\nAsana, ClickUp\nNextcloud Tasks, GitHub Issues\n\n\nCommunication\nSlack, MS Teams\nMattermost, Zulip\n\n\n\n\n\n\n\n\n\nImportantTool Selection Criteria\n\n\n\nWhen selecting project management tools:\n\nVisibility: Everyone should know goals, objectives, roles, and responsibilities\nAccountability: Tasks should have clear owners and deadlines\nIntegration: Tools should connect to reduce manual status updates\nAccessibility: All team members should be able to access and use the system\nData sovereignty: Consider where project data is stored, especially for sensitive public health programs\n\n\n\n\n\n14.2.3.3 The Iterative Imperative\nAvoid spending months developing systems that do not meet critical needs. Systems should be iteratively developed with priority functionality assessed at each step.\n\n\n\n\n\n\n\nAnti-Pattern\nBetter Approach\n\n\n\n\n12-month development before user feedback\n2-week sprints with demos\n\n\nComplete feature set before release\nMVP with core functionality first\n\n\nComprehensive documentation before coding\nJust-in-time documentation\n\n\nPerfect architecture upfront\nEvolutionary architecture\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[Identify Priority&lt;br/&gt;Functionality] --&gt; B[Develop&lt;br/&gt;Increment]\n    B --&gt; C[Deploy &&lt;br/&gt;Gather Feedback]\n    C --&gt; D{Delivering&lt;br/&gt;Value?}\n    D --&gt;|Yes| E[Continue &&lt;br/&gt;Expand]\n    D --&gt;|No| F[Pivot or&lt;br/&gt;Discontinue]\n    E --&gt; A\n    F --&gt; G[Lessons&lt;br/&gt;Learned]\n\n\n\n\nFigure 14.2: Iterative Development with Value Assessment\n\n\n\n\n\n\n\n14.2.3.4 Barriers and Remedies\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nPoor leadership\nScope drift, low morale, unclear direction\nEstablish clear project charter with defined authority; provide leadership coaching; implement governance oversight with regular check-ins\n\n\nNo clear goals\nScattered effort, difficulty measuring progress, weak demonstrated value\nDefine measurable outcomes upfront; adopt OKRs connecting daily work to strategic objectives; align deliverables to logic model outcomes\n\n\nDisorganization\nMissed deadlines, rework, lost information\nImplement lightweight rituals (standups, retrospectives); use visual management (Kanban boards); establish regular planning and review cadence\n\n\nMisalignment across workstreams\nConflicting priorities, duplicated effort, integration failures\nConduct cross-functional prioritization sessions; maintain a single prioritized backlog; map and communicate dependencies explicitly\n\n\nOveremphasis on aesthetics\nDelayed value delivery, resources diverted from core function\nPrioritize scalability and reliability first; defer visual polish until core functionality proven; measure success by outcomes, not appearance\n\n\nUnrealistic expectations\nBurnout, quality shortcuts, failed delivery, eroded trust\nUse evidence-based estimation (historical velocity, throughput data); align commitments to actual capacity; set SLAs based on demonstrated capability\n\n\nShifting expectations (scope creep)\nConstant rework, team churn, inability to complete anything\nImplement formal change control process; refine backlog regularly with stakeholders; document scope decisions; re-baseline schedules with explicit stakeholder sign-off when scope changes\n\n\n\n\n\n\n14.2.4 Communication and Transparency\nClear communication is the foundation of organizational efficiency. Teams cannot avoid duplicating work if they do not know what others are doing.\n\n14.2.4.1 Communication Hierarchy\n\n\n\n\n\n\n\n\n\nLevel\nContent\nFrequency\nTool\n\n\n\n\nStrategic\nGoals, priorities, resource allocation\nQuarterly\nAll-hands, leadership memo\n\n\nTactical\nProject status, blockers, decisions\nWeekly\nTeam meetings, PM tool\n\n\nOperational\nTask updates, questions, collaboration\nDaily\nChat, task comments\n\n\nAd-hoc\nUrgent issues, clarifications\nAs needed\nDirect message, huddle\n\n\n\n\n\n14.2.4.2 RACI Matrix for Process Clarity\nDefine roles clearly to prevent gaps and overlaps:\n\n\n\nRole\nDefinition\n\n\n\n\nResponsible\nDoes the work\n\n\nAccountable\nUltimately answerable (one person only)\n\n\nConsulted\nProvides input\n\n\nInformed\nKept updated\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nRACI for Data Quality Process:\n\n\n\nActivity\nData Analyst\nData Manager\nEpidemiologist\nIT Support\n\n\n\n\nReceive hospital files\nI\nR\nI\nC\n\n\nValidate file format\nC\nR\nI\nA\n\n\nClean demographic data\nR\nC\nI\nI\n\n\nApply edits/business rules\nR\nC\nA\nI\n\n\nGenerate quality report\nR\nI\nA\nI\n\n\nResolve data issues\nR\nC\nA\nC\n\n\n\n\n\n\n\n14.2.4.3 Poor Styles and Productive Alternatives\n\n\n\n\n\n\n\n\nPoor Style\nTypical Outcome\nProductive Alternative\n\n\n\n\nVague updates (“Things are going fine”)\nConfusion about actual status, surprises, rework\nUse SBAR format: Situation (what’s happening), Background (context), Assessment (analysis), Recommendation (proposed action)\n\n\nOne-way broadcasting (announcements without dialogue)\nLow engagement, unaddressed concerns, passive resistance\nPractice active listening; explicitly solicit questions; summarize decisions and confirm understanding\n\n\nUnstructured meetings (no agenda, no outcomes)\nWasted time, repeated discussions, unclear decisions\nPublish agenda with specific objectives; timebox each topic; end with documented decisions and assigned next steps\n\n\nEmail-only coordination for complex work\nSlow responses, fragmented context, lost threads\nUse shared PM tools for task tracking; maintain threaded discussions tied to work items; keep decision logs for reference\n\n\nPublic criticism of individuals\nFear, defensiveness, hidden problems\nProvide feedback privately; focus on specific behaviors rather than character; reinforce expected norms constructively\n\n\n\n\n\n\n14.2.5 Educational Foundations\nProcess optimization requires ongoing learning and skill development.\n\n14.2.5.1 Adult Learning Principles (Andragogy)\nMalcolm Knowles identified key principles for adult learners:\n\n\n\n\n\n\n\nPrinciple\nApplication\n\n\n\n\nSelf-directed\nProvide resources for independent learning\n\n\nExperience-based\nConnect new processes to existing knowledge\n\n\nRelevance-oriented\nExplain why processes matter\n\n\nProblem-centered\nFrame training around real challenges\n\n\nInternally motivated\nAppeal to professional growth, not just compliance\n\n\n\n\n\n14.2.5.2 Building Technical Capacity\nDesktop data management tasks should be scripted. Manual data cleaning in spreadsheets is error-prone, non-reproducible, and does not scale.\n\n\n\nManual Approach\nScripted Approach\n\n\n\n\nCopy-paste in Excel\nR/Python script\n\n\nPoint-and-click transformations\nDocumented code\n\n\n“I remember how I did it”\nVersion-controlled workflow\n\n\nOne person can do it\nAnyone can run it\n\n\n\n\n\n\n\n\n\nTipLearning Path Recommendation\n\n\n\nEncourage all analysts to learn basic programming:\n\nStart with R or Python: Both are free, well-documented, and widely used\nFocus on data manipulation first: tidyverse (R) or pandas (Python)\nLearn version control: Git fundamentals for collaboration\nBuild incrementally: Automate one task at a time\nShare and document: Create institutional knowledge\n\nRecommended resources:\n\nR for Data Science (free online book)\nPython for Data Analysis (O’Reilly)\nThe Carpentries (workshops for researchers)\n\n\n\n\n\n14.2.5.3 Barriers and Remedies\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nLeadership lacks domain expertise\nMisguided priorities, unrealistic technical decisions\nPair leaders with SMEs for technical decisions; schedule regular domain briefings; require review gates before committing to technical approaches\n\n\nTraining deprioritized\nPersistent skill gaps, reliance on few experts, fragility\nInvest in microlearning (short, focused modules); build practice into regular work; establish mentorship programs pairing experienced and developing staff\n\n\nTool resistance (“I’ve always done it this way”)\nManual work persists despite better options, inconsistent outputs\nProvide low-stakes practice environments; pair resistant staff with supportive peers; share concrete success stories showing time saved\n\n\nKnowledge silos\nRework when experts unavailable, inconsistent methods across team\nDevelop shared curricula documenting standard approaches; host brown-bag sessions for knowledge sharing; consider internal certifications to validate and spread expertise",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#automation-strategy",
    "href": "chapters/12-process-optimization.html#automation-strategy",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.3 Automation Strategy",
    "text": "14.3 Automation Strategy\n\n14.3.1 The Automation Spectrum\nNot all automation is equal. Consider the level appropriate for each process:\n\n\n\n\n\n\n\n\n\nLevel\nDescription\nExample\nHuman Role\n\n\n\n\nManual\nHuman does all work\nAd-hoc data requests\nFull control\n\n\nAssisted\nTools support human work\nSpell-check, templates\nDecision-maker\n\n\nPartial\nSystem handles routine; human handles exceptions\nAuto-coding with review queue\nException handler\n\n\nConditional\nSystem does most; human monitors\nScheduled reports with alerts\nSupervisor\n\n\nFull\nSystem operates independently\nAutomated backups\nOversight only\n\n\n\n\n\n14.3.2 Automation with Accountability\nSystems that are automated sometimes miss critical requirements. Automated systems should be flexible enough to adjust to evolving demands.\n\n\n\nRequirement\nImplementation\n\n\n\n\nFlexibility\nConfigurable rules, not hard-coded logic\n\n\nAuditability\nLogging of all automated decisions\n\n\nOverride capability\nHuman can intervene when needed\n\n\nFeedback loops\nMechanism to report automation failures\n\n\nVersion control\nTrack changes to automation rules\n\n\n\n\n14.3.2.1 The AI Accountability Challenge\nArtificial intelligence presents unique challenges for process automation:\n\n\n\n\n\n\n\nAI Challenge\nMitigation Strategy\n\n\n\n\nLack of accountability\nAssign human owner for AI-assisted decisions\n\n\nInconsistent reliability\nImplement confidence thresholds and fallback processes\n\n\nVolume of output\nTrain more reviewers; focus review on high-risk items\n\n\nOpacity (“black box”)\nRequire explainable AI or human decision for critical paths\n\n\nDrift over time\nRegular performance monitoring and recalibration\n\n\n\n\n\n\n\n\n\nWarningAI Oversight Principle\n\n\n\nThere should always be someone who reviews critical operations. AI can accelerate work, but lacks the accountability and judgment required for consequential decisions.\nThe challenge: humans cannot review the vast amounts of AI-generated output. Solutions include:\n\nRisk-based review: Focus human attention on high-stakes decisions\nSampling: Statistically valid review of AI output subsets\nBuilding reviewer capacity: Train more staff to critically evaluate AI outputs\nAutomated validation: Use rule-based systems to catch obvious AI errors\n\n\n\n\n\n14.3.2.2 Barriers and Remedies\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nResistance to tool adoption\nParallel manual processes undermine efficiency gains, inconsistent outputs\nCo-design automation with end users from the start; pilot features with willing early adopters; identify champions and super-users to support peers\n\n\nRigid automation (hard-coded logic)\nSystem cannot adapt to legitimate exceptions, workarounds proliferate\nBuild configurable rules rather than fixed logic; ensure human override capability; plan for rapid iteration as requirements evolve\n\n\nOpaque AI decisions\nLow trust, reluctance to rely on system, manual double-checking negates efficiency\nRequire explainability for consequential decisions; implement confidence thresholds that trigger human review; maintain fallback to human judgment for edge cases\n\n\nOver-automation (removing humans entirely)\nBrittleness when exceptions occur, catastrophic failures without intervention\nKeep humans in the loop for exception handling; monitor automated systems actively; recalibrate regularly based on performance data\n\n\n\n\n\n\n14.3.3 Scripting Desktop Tasks\nTransform manual data management into reproducible workflows:\n\n\n\n\n\n\nflowchart LR\n    subgraph Manual[\"Manual Process\"]\n        M1[Open Excel] --&gt; M2[Copy data]\n        M2 --&gt; M3[Clean manually]\n        M3 --&gt; M4[Format output]\n        M4 --&gt; M5[Email results]\n    end\n    \n    subgraph Scripted[\"Scripted Process\"]\n        S1[Run script] --&gt; S2[Auto-clean]\n        S2 --&gt; S3[Generate report]\n        S3 --&gt; S4[Archive & notify]\n    end\n    \n    Manual --&gt;|Transform| Scripted\n\n\n\n\nFigure 14.3: From Manual to Scripted Data Workflow\n\n\n\n\n\nBenefits of scripted workflows:\n\n\n\nBenefit\nDescription\n\n\n\n\nReproducibility\nSame code produces same results\n\n\nAuditability\nCode documents exactly what was done\n\n\nScalability\nProcess 10 or 10,000 records identically\n\n\nError reduction\nEliminates copy-paste mistakes\n\n\nKnowledge transfer\nNew staff can run existing scripts",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#centralization-and-shared-services",
    "href": "chapters/12-process-optimization.html#centralization-and-shared-services",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.4 Centralization and Shared Services",
    "text": "14.4 Centralization and Shared Services\n\n14.4.1 The Medallion Architecture for Shared Data\nWhen multiple teams rely on the same data, implement a centralized data architecture:\n\n\n\n\n\n\nflowchart LR\n    subgraph Sources[\"Data Sources\"]\n        H[Hospitals]\n        L[Labs]\n        V[Vital Records]\n    end\n    \n    subgraph Central[\"Centralized Data Team\"]\n        B[(Bronze&lt;br/&gt;Raw Data)]\n        S[(Silver&lt;br/&gt;Cleaned Data)]\n        G[(Gold&lt;br/&gt;Analysis-Ready)]\n    end\n    \n    subgraph Consumers[\"Data Consumers\"]\n        E[Epidemiologists]\n        R[Registrars]\n        A[Analysts]\n        P[Program Staff]\n    end\n    \n    H --&gt; B\n    L --&gt; B\n    V --&gt; B\n    B --&gt; S\n    S --&gt; G\n    G --&gt; E\n    G --&gt; R\n    G --&gt; A\n    G --&gt; P\n\n\n\n\nFigure 14.4: Centralized Data Architecture\n\n\n\n\n\nCentralization benefits:\n\nConsistency: All consumers use the same cleaned data\nExpertise: Data team develops specialized cleaning skills\nEfficiency: Clean once, use many times\nQuality: Single point of accountability for data quality\n\n\n\n14.4.2 Shared Services Model\nExtend centralization beyond data to other core functions:\n\n\n\n\n\n\n\n\nFunction\nCentralized Model\nBenefits\n\n\n\n\nData cleaning\nCentral data team maintains Silver layer\nConsistent quality, reduced duplication\n\n\nReport generation\nShared reporting infrastructure\nStandard formats, automated distribution\n\n\nTraining development\nCentral learning team\nConsistent messaging, professional quality\n\n\nCompliance documentation\nCompliance office coordinates\nComplete coverage, expert interpretation\n\n\nTool administration\nIT manages shared tools\nSecurity, licensing, support",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#establishing-accountability-structures",
    "href": "chapters/12-process-optimization.html#establishing-accountability-structures",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.5 Establishing Accountability Structures",
    "text": "14.5 Establishing Accountability Structures\n\n14.5.1 Clear Governance\nClear structures should be in place to ensure responsibility and accountability. Without governance, process improvements drift and deteriorate.\n\n\n\n\n\n\n\n\nGovernance Element\nPurpose\nExample\n\n\n\n\nProcess owner\nSingle point of accountability\nData Quality Manager\n\n\nSteering committee\nStrategic decisions, resource allocation\nMonthly leadership review\n\n\nWorking groups\nOperational improvements\nData Quality Working Group\n\n\nSLAs/OLAs\nDocumented expectations\n99.9% uptime, 48-hour response\n\n\nEscalation paths\nClear routes for unresolved issues\nAnalyst → Manager → Director\n\n\n\n\n\n14.5.2 Metrics and Monitoring\nWhat gets measured gets managed. Establish metrics for process performance:\n\n\n\nMetric Category\nExamples\n\n\n\n\nEfficiency\nTime per task, throughput, backlog size\n\n\nQuality\nError rates, rework rates, audit findings\n\n\nAdoption\nUsage rates, training completion, feedback scores\n\n\nValue\nCost savings, time saved, outcomes improved\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nProcess Optimization Dashboard:\n\n\n\nMetric\nBaseline\nTarget\nCurrent\nStatus\n\n\n\n\nCase abstraction time\n15 min\n8 min\n9.2 min\n🟡\n\n\nData completeness\n89%\n95%\n94.3%\n🟡\n\n\nDuplicate detection rate\n73%\n95%\n96.1%\n🟢\n\n\nHospital submission lag\n14 days\n7 days\n5.2 days\n🟢\n\n\nManual interventions/week\n127\n50\n43\n🟢\n\n\nStaff satisfaction\n3.2/5\n4.0/5\n4.1/5\n🟢\n\n\n\nThe dashboard is reviewed weekly by the Data Quality Working Group and monthly by the program steering committee.\n\n\n\n14.5.2.1 Barriers and Remedies\n\n\n\n\n\n\n\n\nBarrier\nImpact\nProductive Alternative\n\n\n\n\nPolitical interference\nDecision volatility, resources redirected without justification, staff demoralization\nEstablish charter-based governance with documented authority; use transparent criteria for decisions; engage external audit for high-stakes or contentious choices\n\n\nAmbiguous accountability\nTasks dropped between roles, finger-pointing when problems arise\nAssign explicit process owners with documented responsibility; define SLAs/OLAs for handoffs; establish clear escalation paths when issues arise\n\n\nGoal drift\nOriginal value proposition lost, effort disconnected from outcomes\nConduct quarterly goal reviews against original objectives; maintain traceability from daily work to logic model outcomes and grant requirements\n\n\nFragmented oversight\nConflicting directives from multiple authorities, staff confusion\nConsolidate to single steering committee with clear authority; integrate calendars to prevent conflicting demands; establish unified reporting cadence",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#bringing-it-together-a-comprehensive-framework",
    "href": "chapters/12-process-optimization.html#bringing-it-together-a-comprehensive-framework",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.6 Bringing It Together: A Comprehensive Framework",
    "text": "14.6 Bringing It Together: A Comprehensive Framework\n\n14.6.1 The Process Optimization Lifecycle\n\n\n\n\n\n\nflowchart TD\n    A[Identify Process] --&gt; B[Assess Value]\n    B --&gt; C{Mission&lt;br/&gt;Critical?}\n    C --&gt;|No| D[Eliminate or&lt;br/&gt;Reduce]\n    C --&gt;|Yes| E[Map Current State]\n    E --&gt; F[Identify Inefficiencies]\n    F --&gt; G[Design Improvements]\n    G --&gt; H[Pilot & Test]\n    H --&gt; I{Effective?}\n    I --&gt;|No| G\n    I --&gt;|Yes| J[Implement]\n    J --&gt; K[Monitor & Measure]\n    K --&gt; L{Meeting&lt;br/&gt;Targets?}\n    L --&gt;|No| F\n    L --&gt;|Yes| M[Standardize & Document]\n    M --&gt; N[Continuous Monitoring]\n    N --&gt; F\n\n\n\n\nFigure 14.5: Process Optimization Lifecycle\n\n\n\n\n\n\n\n14.6.2 Integration Checklist\nWhen optimizing processes, ensure you address all dimensions:\n\n\n\n\n\n\n\nDimension\nQuestions to Ask\n\n\n\n\nPsychology\nDoes this support autonomy, competence, and relatedness? Is the environment psychologically safe?\n\n\nSociology\nDoes this align with organizational culture? Are communities of practice engaged?\n\n\nProject Management\nIs there a clear plan with milestones? Are tools appropriate?\n\n\nCommunication\nDo all stakeholders know the goals, roles, and status?\n\n\nEducation\nDo staff have the skills needed? Is training available?\n\n\nCentralization\nAre core functions appropriately centralized? Is duplication minimized?\n\n\nAutomation\nIs the right level of automation applied? Is there human oversight?\n\n\nAccountability\nAre governance structures clear? Are metrics tracked?\n\n\n\n\n\n14.6.3 The Public Health Value Proposition\nUltimately, process optimization in public health must demonstrate value to the taxpayer. Every efficiency gain should connect to improved health outcomes:\n\n\n\nEfficiency Gain\nTaxpayer Value\n\n\n\n\nFaster data processing\nEarlier outbreak detection\n\n\nReduced manual errors\nMore accurate surveillance\n\n\nAutomated reporting\nMore resources for intervention\n\n\nStreamlined workflows\nLower cost per case processed\n\n\nBetter data quality\nMore reliable public health decisions\n\n\n\n\n\n\n\n\n\nImportantThe Accountability Test\n\n\n\nFor every process, ask: “Could I explain to a taxpayer why this is necessary and how it contributes to public health?”\nIf the answer is no, the process should be eliminated, automated, or fundamentally redesigned.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/12-process-optimization.html#summary",
    "href": "chapters/12-process-optimization.html#summary",
    "title": "14  Process Optimization & Organizational Efficiency",
    "section": "14.7 Summary",
    "text": "14.7 Summary\nProcess optimization requires a multidisciplinary approach that addresses both technical systems and human factors:\n\nUnderstand human factors: Motivation, psychological safety, and well-being are prerequisites for sustainable improvement. Processes that ignore people will fail.\nAddress organizational dynamics: Culture, power structures, and communication patterns can enable or obstruct change. Work with these forces, not against them.\nApply disciplined management: Iterative development, clear accountability, and appropriate tools prevent wasted effort. Set realistic expectations and manage scope actively.\nAutomate thoughtfully: Prioritize scalability and flexibility over perfection. Maintain human oversight, especially for AI-assisted processes that lack inherent accountability.\nCentralize core functions: Reduce duplication by sharing data, tools, and expertise. Build specialized capability where it matters most.\nDemonstrate value: Connect every optimization to measurable outcomes. In public health, that means taxpayer value and improved health for the populations we serve.\n\nThe goal is not efficiency for its own sake, but rather maximizing public health impact with the resources entrusted to us.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Process Optimization & Organizational Efficiency</span>"
    ]
  },
  {
    "objectID": "chapters/A-templates.html",
    "href": "chapters/A-templates.html",
    "title": "15  Templates & Checklists",
    "section": "",
    "text": "15.1 Templates and Checklists\nThis appendix provides ready-to-use templates for hybrid BA/PH projects. Each template is presented in both BA and PH framings where appropriate.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Templates & Checklists</span>"
    ]
  },
  {
    "objectID": "chapters/A-templates.html#templates-and-checklists",
    "href": "chapters/A-templates.html#templates-and-checklists",
    "title": "15  Templates & Checklists",
    "section": "",
    "text": "15.1.1 Stakeholder / Community Partner Matrix\nUse this template to identify and analyze project participants:\n\n\n\n\n\n\n\n\n\n\n\nName/Group\nBA Role\nPH Role\nInterest Level\nInfluence Level\nEngagement Strategy\n\n\n\n\nRegistry Director\nExecutive Sponsor\nState Epidemiologist\nHigh\nHigh\nMonthly briefings, decision authority\n\n\nCancer Registrars\nEnd Users\nFrontline Data Collectors\nHigh\nMedium\nTraining, feedback sessions, super-users\n\n\nHospital IT\nTechnical SMEs\nData Partners\nMedium\nMedium\nIntegration meetings, testing\n\n\nCDC/NPCR\nGovernance\nFunder/Standards Body\nHigh\nHigh\nFormal reporting, compliance reviews\n\n\nCancer Survivors\nIndirect Stakeholders\nRights Holders/Beneficiaries\nLow\nLow\nAdvisory input, outcome focus\n\n\n\n\n15.1.1.1 Analysis Questions\n\nWho might be missing from this list?\nAre marginalized voices represented?\nWho has formal authority vs. informal influence?\nWhat are the potential conflicts of interest?\n\n\n\n\n15.1.2 Logic Model Template\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              LOGIC MODEL                                     │\n│  Program: _______________________________________________                    │\n│  Date: __________________________________________________                    │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│  INPUTS              ACTIVITIES           OUTPUTS            OUTCOMES        │\n│  (Resources)         (What we do)         (Products)         (Changes)       │\n│                                                                              │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐       ┌──────────┐     │\n│  │          │       │          │        │          │       │ Short-   │     │\n│  │ Funding  │──────▶│ Develop  │───────▶│ Working  │──────▶│ term     │     │\n│  │          │       │ system   │        │ software │       │          │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                   │          │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐              ▼          │\n│  │          │       │          │        │          │       ┌──────────┐     │\n│  │ Staff    │──────▶│ Train    │───────▶│ Trained  │──────▶│ Medium-  │     │\n│  │          │       │ users    │        │ users    │       │ term     │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                   │          │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐              ▼          │\n│  │          │       │          │        │          │       ┌──────────┐     │\n│  │ Data     │──────▶│ Process  │───────▶│ Quality  │──────▶│ Long-    │     │\n│  │ feeds    │       │ data     │        │ reports  │       │ term     │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  ASSUMPTIONS:                          EXTERNAL FACTORS:                     │\n│  _________________________________     _________________________________     │\n│  _________________________________     _________________________________     │\n│  _________________________________     _________________________________     │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n15.1.2.1 CancerSurv Logic Model Example\n\n\n\n\n\n\n\n\n\n\nInputs\nActivities\nOutputs\nShort-term Outcomes\nLong-term Impact\n\n\n\n\nNPCR grant funding\nSystem development\nOperational platform\nImproved workflow efficiency\nAccurate survival statistics\n\n\nRegistry staff\nUser training\nTrained registrars\nReduced abstraction time\nTargeted prevention\n\n\nHospital data feeds\nData integration\nElectronic data capture\nIncreased data completeness\nHealth disparity identification\n\n\nNAACCR standards\nQuality assurance\nValidated records\nTimely NPCR submission\nEvidence-based cancer policy\n\n\n\n\n\n\n15.1.3 Requirements Specification Template\n\n15.1.3.1 Functional Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nFR-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nDescription\nThe system shall [action] [object] [condition/constraint]\n\n\nRationale\n[Why this requirement exists; link to business need/program goal]\n\n\nSource\n[Stakeholder, document, or session where identified]\n\n\nPriority\nMust / Should / Could / Won’t (this release)\n\n\nAcceptance Criteria\nGiven [context], when [action], then [expected result]\n\n\nDependencies\n[Other requirements this depends on or enables]\n\n\n\n\n\n15.1.3.2 Non-Functional Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nNFR-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nCategory\nPerformance / Security / Usability / Reliability / Scalability / Compliance\n\n\nDescription\nThe system shall [quality attribute] [measurable target]\n\n\nRationale\n[Why this matters; CFIR domain if applicable]\n\n\nMeasurement\n[How compliance will be verified]\n\n\nPriority\nMust / Should / Could\n\n\n\n\n\n15.1.3.3 Example: CancerSurv Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nFR-105\n\n\nTitle\nDuplicate Case Detection\n\n\nDescription\nThe system shall identify potential duplicate case records based on patient identifiers (name, DOB, SSN) and tumor characteristics\n\n\nRationale\nPrevents double-counting of cancer incidence; required for accurate epidemiological analysis\n\n\nSource\nRegistrar interviews; NAACCR standards\n\n\nPriority\nMust\n\n\nAcceptance Criteria\nGiven a new case entry, when patient identifiers match an existing case with confidence &gt;80%, then the system displays potential duplicates for review before saving\n\n\nDependencies\nFR-101 (Patient search)\n\n\n\n\n\n\n15.1.4 CFIR Readiness Assessment Checklist\nUse this checklist to assess implementation readiness:\n\n15.1.4.1 Intervention Characteristics\n\nEvidence of effectiveness documented and communicated\nRelative advantage over current practice clearly articulated\nCore vs. adaptable components identified\nComplexity minimized; training plan addresses remaining complexity\nPilot/trialability planned\n\n\n\n15.1.4.2 Outer Setting\n\nRegulatory requirements identified (HIPAA, CDC, state)\nExternal stakeholder needs understood\nInteroperability requirements documented\nPeer organization experiences reviewed\n\n\n\n15.1.4.3 Inner Setting\n\nLeadership commitment secured\nResources allocated (budget, staff, time)\nImplementation team identified\nCommunication plan developed\nOrganizational culture alignment assessed\n\n\n\n15.1.4.4 Characteristics of Individuals\n\nUser attitudes toward change assessed\nTraining needs identified\nChampions identified and engaged\nSelf-efficacy barriers addressed\n\n\n\n15.1.4.5 Process\n\nDetailed implementation plan developed\nKey milestones and decision points defined\nMonitoring and feedback mechanisms established\nContingency plans for common risks\n\n\n\n\n15.1.5 User Story Templates\n\n15.1.5.1 Standard Format\n\nAs a [role], I want [feature/capability], so that [benefit/value].\n\n\n\n15.1.5.2 GPS Format (Clinical Contexts)\n\nGiven [clinical context/trigger], the [health worker role] should [specific action] to [health outcome].\n\n\n\n15.1.5.3 Service-User Scenario Format\n\n[Name], a [demographic description], [presents with situation]. The system must [support their need] while [addressing constraints]. Success means [outcome].\n\n\n\n15.1.5.4 Example Set: CancerSurv\nStandard: &gt; As a cancer registrar, I want to search existing cases before creating a new record, so that I avoid creating duplicate entries.\nGPS: &gt; Given a new pathology report, the registrar should search for existing patient records using at least two identifiers, to prevent duplicate case creation and ensure accurate incidence counts.\nService-User Scenario: &gt; Maria is a senior registrar abstracting cases from a high-volume hospital. She receives 50 pathology reports daily and needs to quickly determine if each represents a new case or an existing patient. The system must support rapid searching with fuzzy matching while flagging potential duplicates. Success means Maria can process her daily queue in under 4 hours with less than 1% duplicate creation rate.\n\n\n\n15.1.6 Test Case Template\n\n\n\nField\nContent\n\n\n\n\nID\nTC-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nRequirement(s)\n[FR-XXX, NFR-XXX]\n\n\nPreconditions\n[System state before test]\n\n\nTest Data\n[Specific data needed]\n\n\nSteps\n1. [Action] 2. [Action] 3. [Action]\n\n\nExpected Result\n[What should happen]\n\n\nActual Result\n[Fill in during testing]\n\n\nPass/Fail\n[ ]\n\n\nNotes\n[Observations, defect IDs if failed]\n\n\n\n\n\n15.1.7 Project Charter Template (Dual-Frame)\n\n\n\n\n\n\n\n\nSection\nBA Content\nPH Content\n\n\n\n\nProject Name\n[System name]\n[Program name]\n\n\nBusiness Need\n[Operational driver]\nPublic Health Challenge: [Health need]\n\n\nObjectives\n[Business objectives]\nProgram Goals: [Health objectives]\n\n\nScope\n[In/out of scope]\nIntervention Boundaries: [Target population, geography]\n\n\nStakeholders\n[Stakeholder list]\nCommunity Partners: [Partner list]\n\n\nSuccess Metrics\n[KPIs]\nHealth Indicators: [Outcome measures]\n\n\nTimeline\n[Project milestones]\nGrant Milestones: [Funder deadlines]\n\n\nBudget\n[Project budget]\nFunding Source: [Grant, appropriation]\n\n\nRisks\n[Project risks]\nImplementation Barriers: [CFIR-informed risks]\n\n\nGovernance\n[Decision structure]\nOversight: [Funder, advisory board]\n\n\n\n\n\n15.1.8 Evaluation Plan Template\n\n\n\nComponent\nContent\n\n\n\n\nEvaluation Questions\nWhat do we want to know?\n\n\nType\nFormative / Summative / Process / Outcome\n\n\nIndicators\nWhat will we measure?\n\n\nData Sources\nWhere will data come from?\n\n\nMethods\nHow will we collect and analyze?\n\n\nTimeline\nWhen will we measure?\n\n\nResponsibilities\nWho will conduct evaluation?\n\n\nUse of Findings\nHow will results inform decisions?\n\n\n\n\n15.1.8.1 RE-AIM Evaluation Matrix\n\n\n\nDimension\nIndicator\nData Source\nTarget\nActual\n\n\n\n\nReach\n\n\n\n\n\n\nEffectiveness\n\n\n\n\n\n\nAdoption\n\n\n\n\n\n\nImplementation\n\n\n\n\n\n\nMaintenance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Templates & Checklists</span>"
    ]
  },
  {
    "objectID": "chapters/B-development-tools.html",
    "href": "chapters/B-development-tools.html",
    "title": "16  Development Tools",
    "section": "",
    "text": "16.1 Development Tools for Toolkit Contributors\nThis appendix documents the tools used to build and maintain the Bridgeframe Toolkit. This information is for contributors who want to modify or extend the toolkit, not for practitioners using the toolkit content.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Development Tools</span>"
    ]
  },
  {
    "objectID": "chapters/B-development-tools.html#development-tools-for-toolkit-contributors",
    "href": "chapters/B-development-tools.html#development-tools-for-toolkit-contributors",
    "title": "16  Development Tools",
    "section": "",
    "text": "16.1.1 Technology Stack\nThe Bridgeframe Toolkit is built with:\n\n\n\nTool\nPurpose\nVersion\n\n\n\n\nQuarto\nDocument publishing system\n1.4+\n\n\nVS Code\nCode editor\nCurrent\n\n\nGitHub Copilot\nAI-assisted development\nCurrent\n\n\nGit/GitHub\nVersion control and hosting\nN/A\n\n\nGitHub Actions\nAutomated publishing\nN/A\n\n\nMermaid\nDiagram generation\nBuilt into Quarto\n\n\n\n\n\n16.1.2 AI-Assisted Development\nGitHub Copilot is used throughout the development of this toolkit for:\n\nRefining ideas: Brainstorming content structure and terminology mappings\nAnalysis: Reviewing and improving BA-PH framework alignments\nCode development: Generating Mermaid diagrams, R scripts, and configuration files\n\n\n16.1.2.1 LLM Models Used\nGitHub Copilot provides access to multiple large language models:\n\n\n\n\n\n\n\n\nProvider\nModels\nPrimary Use\n\n\n\n\nAnthropic\nClaude (Sonnet, Opus)\nComplex analysis, long-form content\n\n\nGoogle\nGemini\nResearch, fact-checking\n\n\nOpenAI\nGPT-4, GPT-4o\nCode generation, quick edits\n\n\n\n\n\n16.1.2.2 Copilot Features\n\nChat: Interactive conversation for complex tasks\nInline suggestions: Real-time code and content completion\nAgent mode: Multi-step tasks with file creation and terminal access\n\n\n\n16.1.2.3 Deep Research Tools\nResearch and analysis is additionally supported by deep research features in:\n\nGoogle Gemini: Extended research capabilities for comprehensive literature review and framework analysis\nOpenAI ChatGPT: Deep research mode for exploring complex BA-PH terminology mappings and implementation science concepts\n\nThese tools complement GitHub Copilot by providing broader research context beyond the immediate codebase.\n\n\n16.1.2.4 Project Context\nThe .github/copilot-instructions.md file provides Copilot with project-specific context including:\n\nRepository structure and conventions\nBA-PH terminology mappings\nCancerSurv case study details\nFormatting standards and style guidelines\n\n\n\n\n16.1.3 Local Development Setup\n\n16.1.3.1 Prerequisites\n\nInstall Quarto: Download from quarto.org\nInstall VS Code: Download from code.visualstudio.com\nInstall Git: Download from git-scm.com\nInstall Quarto VS Code Extension: Search “Quarto” in VS Code extensions\n\n\n\n16.1.3.2 Clone the Repository\ngit clone https://github.com/andre-inter-collab-llc/Bridgeframe-Toolkit.git\ncd Bridgeframe-Toolkit\n\n\n16.1.3.3 Preview Locally\nquarto preview\nThis starts a local server and opens the book in your browser. Changes to .qmd files automatically refresh.\n\n\n16.1.3.4 Render the Book\nquarto render\nOutput is generated in the _book/ directory.\n\n\n\n16.1.4 Repository Structure\nBridgeframe-Toolkit/\n├── _quarto.yml           # Book configuration\n├── _brand.yml            # Branding (colors, fonts, logo)\n├── index.qmd             # Landing page\n├── preface.qmd           # Author preface\n├── references.qmd        # Bibliography\n├── chapters/             # Book chapters\n│   ├── 01-introduction.qmd\n│   ├── ...\n│   └── C-glossary.qmd\n├── assets/\n│   ├── branding/         # Logos, icons, templates\n│   ├── references/       # Bibliography files\n│   └── styles/           # Custom SCSS\n├── .github/\n│   ├── copilot-instructions.md  # AI assistant context\n│   └── workflows/\n│       └── publish.yml   # GitHub Actions workflow\n└── _book/                # Generated output (gitignored)\n\n\n16.1.5 Configuration Files\n\n16.1.5.1 _quarto.yml\nThe main configuration file controls:\n\nBook metadata (title, author, date)\nChapter organization\nOutput formats (HTML, DOCX)\nTheme and styling\nBibliography settings\n\nKey sections:\nproject:\n  type: book\n  output-dir: _book\n\nbook:\n  title: \"Bridgeframe\"\n  chapters:\n    - index.qmd\n    - part: \"Foundations\"\n      chapters:\n        - chapters/01-introduction.qmd\n        # ...\n\nformat:\n  html:\n    theme:\n      - brand\n      - assets/styles/custom.scss\n  docx:\n    reference-doc: assets/branding/templates/IntersectCollab-reference-doc.docx\n\n\n16.1.5.2 _brand.yml\nControls visual branding:\ncolor:\n  palette:\n    blue: \"#2494f7\"\n    teal: \"#00a4bb\"\n    # ...\n  primary: blue\n  secondary: teal\n\ntypography:\n  base: \"Inter\"\n  headings:\n    family: \"Inter\"\n    weight: 800\n  monospace: \"Fira Code\"\n\nlogo:\n  medium: assets/branding/logos/intersect-logo.png\n\n\n\n16.1.6 Writing Content\n\n16.1.6.1 Quarto Markdown Basics\nQuarto uses extended Markdown. Key features:\nHeadings:\n# Chapter Title (H1)\n## Section (H2)\n### Subsection (H3)\nCallout Boxes:\n::: {.callout-note title=\"CancerSurv Example\"}\nContent here...\n:::\n\n::: {.callout-tip}\nTip content...\n:::\n\n::: {.callout-warning}\nWarning content...\n:::\nTables:\n| Column 1 | Column 2 |\n|:---------|:---------|\n| Data 1   | Data 2   |\nCitations:\nAccording to the BABOK [@babok2015], requirements should be...\n\n\n16.1.6.2 Mermaid Diagrams\nDiagrams are created with Mermaid syntax in code blocks:\n```{mermaid}\n%%| label: fig-example\n%%| fig-cap: \"Example Diagram\"\nflowchart LR\n    A[Start] --&gt; B[Process]\n    B --&gt; C[End]\n```\nCommon diagram types:\n\nflowchart: Process flows, architecture\nsequenceDiagram: Interactions over time\nerDiagram: Data models\ngantt: Project timelines\n\n\n\n16.1.6.3 Cross-References\nReference figures, tables, and sections:\nSee @fig-example for the diagram.\nAs discussed in @sec-planning...\n\n\n\n16.1.7 Publishing\n\n16.1.7.1 GitHub Pages Deployment\nThe book automatically publishes to GitHub Pages when changes are pushed to main. The workflow (.github/workflows/publish.yml) handles rendering and deployment.\n\n\n16.1.7.2 Manual Publishing\nTo publish manually:\nquarto publish gh-pages\nThis creates/updates the gh-pages branch with rendered content.\n\n\n16.1.7.3 Initial Setup\nFor first-time setup:\n\nCreate .nojekyll file in repository root (empty file)\nRun quarto publish gh-pages once locally\nCommit the generated _publish.yml file\nEnable GitHub Pages in repository settings (source: gh-pages branch)\n\n\n\n\n16.1.8 Contributing\n\n16.1.8.1 Branch Strategy\n\nmain: Production-ready content\nFeature branches: For new content or significant changes\nPull requests: For review before merging to main\n\n\n\n16.1.8.2 Content Guidelines\n\nFollow the BA-PH dual framing throughout\nInclude CancerSurv examples in callout boxes\nUse tables for terminology mapping\nAvoid em dashes and en dashes; rewrite sentences instead\nEvery .qmd file needs YAML frontmatter with at least a title\n\n\n\n16.1.8.3 Style Guidelines\n\nFirst-person voice only in Preface and personal sections\nProfessional/instructional tone for toolkit content\nAcademic yet accessible language\nGround claims in evidence where possible\n\n\n\n\n16.1.9 Troubleshooting\n\n16.1.9.1 Common Issues\nQuarto preview not updating: - Check for syntax errors in YAML frontmatter - Restart preview with quarto preview\nMermaid diagram not rendering: - Verify syntax at mermaid.live - Check for unsupported features\nGitHub Pages not updating: - Check Actions tab for workflow failures - Verify gh-pages branch exists - Check repository Pages settings\nBibliography not working: - Verify references.bib exists at specified path - Check citation key matches bib entry - Ensure bibliography field in _quarto.yml is correct\n\n\n\n16.1.10 Resources\n\nQuarto Documentation\nQuarto Books Guide\nMermaid Documentation\nGitHub Pages Documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Development Tools</span>"
    ]
  },
  {
    "objectID": "chapters/D-workforce-development.html",
    "href": "chapters/D-workforce-development.html",
    "title": "17  Workforce Development",
    "section": "",
    "text": "17.1 Building Hybrid Professionals and Teams\nThe translation gap between IT business analysis and public health is not just a process problem or a terminology problem. It is fundamentally a workforce problem. Both government agencies and private industry need professionals who can operate fluently in both worlds, yet most training programs still operate in silos.\nThis appendix explores the challenge, highlights emerging resources, and suggests strategies for building hybrid capacity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Workforce Development</span>"
    ]
  },
  {
    "objectID": "chapters/D-workforce-development.html#building-hybrid-professionals-and-teams",
    "href": "chapters/D-workforce-development.html#building-hybrid-professionals-and-teams",
    "title": "17  Workforce Development",
    "section": "",
    "text": "17.1.1 The Training Silo Problem\nConsider the typical career paths:\nPublic Health Path:\n\nMPH or equivalent graduate training\nCoursework in epidemiology, biostatistics, health policy\nRarely covers: Agile methodology, BABOK, software development lifecycle, requirements engineering\n\nBusiness Analysis Path:\n\nBusiness or IT undergraduate degree\nCBAP, PMI-PBA, or similar certification\nRarely covers: Epidemiology, CDC frameworks, public health ethics, community engagement methods\n\nWhen these professionals meet on a health IT project, each brings deep expertise in their domain but limited fluency in the other’s language, frameworks, and assumptions. The result is the translation gap this book addresses.\n\n\n\n\n\n\nNoteInsight from the Field\n\n\n\nThe lack of skilled hybrid workforce and limited resources to hire such professionals are two significant limiting factors in bridging this gap. Organizations often recognize the need but struggle to find candidates or justify positions that span traditional departmental boundaries.\n\n\n\n\n17.1.2 Emerging Training Programs\nSeveral initiatives are working to close this gap:\n\n17.1.2.1 CDC Public Health Informatics Fellowship Program (PHIFP)\nThe Public Health Informatics Fellowship Program trains professionals specifically for this intersection. Fellows work on real public health informatics projects while developing competencies in both public health practice and information systems.\nThe program recruits from diverse backgrounds and provides structured mentorship bridging technical and programmatic domains. Since its establishment, PHIFP has produced many of the hybrid professionals now leading health IT initiatives across state and local health departments.\n\n\n17.1.2.2 PHIT Workforce Development Program\nThe Public Health Informatics & Technology (PHIT) Workforce Development Program, funded by the Office of the National Coordinator for Health IT (ONC), focuses on training diverse professionals in health informatics. The program emphasizes recruiting from underrepresented communities, recognizing that the hybrid workforce should reflect the populations public health serves.\n\n\n17.1.2.3 AMIA Public Health Informatics Working Group\nThe American Medical Informatics Association (AMIA) hosts an active Public Health Informatics Working Group with members from state health departments, academic institutions, nonprofits, and consulting firms. The working group:\n\nDevelops competency frameworks for public health informatics\nShares best practices across organizations\nAdvocates for workforce development resources\nConnects professionals navigating similar challenges\n\nFor those seeking community and professional development in this space, AMIA membership provides access to peers tackling the same translation challenges.\n\n\n17.1.2.4 Other Resources\n\n\n\nResource\nFocus\nAccess\n\n\n\n\nCDC TRAIN\nFree public health courses, including informatics fundamentals\ntrain.org\n\n\nAMIA 10x10\nIntensive health informatics courses\namia.org\n\n\nCAHIMS / CPHIMS\nHealth IT certifications from HIMSS\nhimss.org\n\n\nCoursera / edX\nPublic health informatics specializations (Johns Hopkins, UCSF)\nOnline\n\n\n\n\n\n\n17.1.3 Organizational Strategies for Building Hybrid Capacity\nBeyond individual training, organizations can take structural steps to build translation capability:\n\n17.1.3.1 1. Create Explicit Bridge Roles\nRather than expecting traditional BAs or PH staff to develop hybrid skills on their own, create positions specifically designed for translation:\n\nHealth Informatics Liaison: Embedded in IT teams but reporting to public health leadership\nTechnical Program Analyst: Embedded in public health programs but with explicit BA responsibilities\nImplementation Specialist: Focused on adoption and change management across both domains\n\nThese roles should have:\n\nDual reporting or matrix accountability\nPerformance metrics that span technical and programmatic outcomes\nProfessional development budgets for cross-training\n\n\n\n17.1.3.2 2. Invest in Onboarding Crosswalks\nWhen new team members join hybrid projects, provide structured onboarding that covers both domains:\n\nTerminology glossaries (like Chapter 3 of this book)\nFramework overviews (BABOK basics for PH staff; CDC evaluation basics for BA staff)\nProject-specific crosswalks mapping deliverables to both languages\n\nThis upfront investment reduces the clarification loops that consume project time later.\n\n\n17.1.3.3 3. Include Translation Capacity in Grant Budgets\nAs discussed in Chapter 1, the business case stage is the ideal time to secure resources for cross-domain facilitation. When writing grant applications or project proposals:\n\nInclude line items for terminology alignment workshops\nBudget for cross-training or professional development\nAllocate facilitation time for mixed-team meetings\nConsider consultants or contractors with hybrid backgrounds\n\n\n\n17.1.3.4 4. Build Communities of Practice\nInternal communities of practice can connect professionals across organizational silos:\n\nRegular brown-bag sessions where IT and PH staff present to each other\nShared Slack/Teams channels for quick translation questions\nJoint retrospectives that surface communication challenges\nRotating assignments that expose staff to the other domain\n\n\n\n\n17.1.4 Competencies for the Hybrid Professional\nWhat should a hybrid BA/PH professional be able to do? Based on established competency frameworks and practical experience, key capabilities include:\n\n17.1.4.1 Core Translation Competencies\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nDual-framework thinking\nMap Agile artifacts to Logic Model outcomes and vice versa\n\n\nTerminology fluency\nSpeak both languages without constant translation pauses\n\n\nAudience adaptation\nAdjust communication style for technical vs. programmatic audiences\n\n\nFramework selection\nKnow when to use BA tools vs. PH tools vs. hybrid approaches\n\n\n\n\n\n17.1.4.2 Technical Competencies (BA Side)\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nRequirements engineering\nElicit, analyze, specify, and validate requirements\n\n\nProcess modeling\nCreate BPMN, UML, or similar diagrams\n\n\nAgile practices\nParticipate effectively in sprints, stand-ups, retrospectives\n\n\nData modeling\nUnderstand ERDs, data dictionaries, database concepts\n\n\n\n\n\n17.1.4.3 Technical Competencies (PH Side)\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nEpidemiological thinking\nUnderstand surveillance, case definitions, health indicators\n\n\nProgram evaluation\nApply CDC evaluation framework, logic models, PDSA\n\n\nRegulatory knowledge\nNavigate HIPAA, IRB, grant compliance requirements\n\n\nHealth equity lens\nIdentify disparities, ensure inclusive engagement\n\n\n\n\n\n17.1.4.4 Implementation Science Competencies\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nCFIR application\nAssess implementation context using CFIR domains\n\n\nChange management\nPlan for adoption, not just deployment\n\n\nStakeholder engagement\nBuild relationships across organizational boundaries\n\n\nSustainability planning\nDesign for long-term operation, not just project completion\n\n\n\n\n\n\n17.1.5 Career Pathways\nFor professionals seeking to develop hybrid capabilities, several pathways exist:\n\n17.1.5.1 From Public Health to Hybrid\n\nSeek projects involving health IT systems (surveillance, registries, EHRs)\nRequest assignment to vendor-managed implementations\nPursue CAHIMS or similar health IT certification\nTake online courses in Agile, requirements engineering, or project management\nJoin AMIA or similar professional communities\n\n\n\n17.1.5.2 From Business Analysis to Hybrid\n\nSeek health sector clients or employers\nLearn public health fundamentals (CDC TRAIN offers free courses)\nStudy regulatory context (HIPAA, CDC reporting requirements)\nUnderstand grant-driven funding cycles and constraints\nDevelop cultural competency for community engagement\n\n\n\n17.1.5.3 Academic Pathways\n\nDual degrees: MPH + MBA, MPH + MIS\nHealth informatics programs: Specifically designed for this intersection\nGraduate certificates: Add informatics credentials to existing degrees\n\n\n\n\n17.1.6 The Long View\nBuilding hybrid workforce capacity is a long-term investment. Individual professionals take years to develop fluency in both domains. Organizational cultures take even longer to change.\nBut the need is urgent. Public health IT projects will continue to grow in scope and complexity. The professionals and organizations that build translation capacity now will be better positioned to deliver systems that actually improve health outcomes.\nBridgeframe is one contribution to this effort: a crosswalk that teams can use while the workforce develops. But crosswalks are not substitutes for people. The field needs more hybrid professionals, more training programs, and more organizational commitment to building this capacity.\n\n\n\n\n\n\nTipGet Involved\n\n\n\nIf you are working on workforce development for health informatics, or have experience building hybrid teams, consider contributing to this conversation. The challenges described in this appendix are shared across many organizations, and solutions developed in one context often transfer to others.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Workforce Development</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html",
    "href": "chapters/E-career-navigation.html",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "",
    "text": "18.1 Finding and Landing Roles at the BA/PH Intersection\nThe integration of business analysis and public health is not merely a theoretical exercise in framework alignment; it is a burgeoning labor market reality. However, a significant translation gap exists within the workforce ecosystem itself. While the operational need for professionals who can navigate both epidemiological principles and information technology (IT) requirements is acute, the job market often lacks the taxonomy to describe them.\nHuman Resources departments, government civil service commissions, and automated Applicant Tracking Systems (ATS) predominantly operate within siloed definitions: a candidate is either a “Health Scientist” or an “IT Specialist,” rarely both. This appendix addresses the “structural invisibility” of the hybrid professional, providing a comprehensive analysis of the employment landscape and practical strategies for navigating it.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#finding-and-landing-roles-at-the-baph-intersection",
    "href": "chapters/E-career-navigation.html#finding-and-landing-roles-at-the-baph-intersection",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "",
    "text": "TipThe Bridgeframe Workforce Philosophy\n\n\n\nContext is the most valuable currency in health IT. Technical skills (SQL, Python, Jira) are commodities that can be learned or contracted out. Domain context, understanding why a disease case definition changes during an outbreak or how a rural clinic workflow differs from an urban hospital, is the scarce resource.\nThe career path for the Bridgeframe professional is rarely linear. It does not always move from “Junior Analyst” to “Senior Analyst.” Instead, it often moves laterally across sectors, accumulating context in one domain to leverage it in another.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#the-landscape-of-opportunity",
    "href": "chapters/E-career-navigation.html#the-landscape-of-opportunity",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.2 The Landscape of Opportunity",
    "text": "18.2 The Landscape of Opportunity\nResearch into the public health workforce reveals a distinct ecosystem of opportunity. The demand for hybrid skills is universal, but the capacity to hire and the language used to describe these roles varies significantly by sector. We can map these sectors onto a “Maturity Model” of hybrid employment.\n\n18.2.1 The Incubator: Local and Rural Health Departments\nEarly-career professionals should consider prioritizing local health organizations (LHOs) in communities with limited access to highly trained workers. These environments, including rural county health departments, tribal health organizations, and small community nonprofits, offer a unique value proposition: necessity-driven hybridization.\n\n18.2.1.1 The Operational Reality\nIn a large, well-funded state health department, roles are siloed. A “Database Administrator” manages the servers; an “Epidemiologist” analyzes the data; a “Health Educator” creates the charts.\nIn a rural county health department serving 20,000 residents, a single employee often performs all three functions. They may troubleshoot the clinic’s label printer in the morning (IT Support), query the state immunization registry at lunch (Data Analysis), and present vaccination rates to the County Board of Health in the evening (Data Visualization).\n\n\n18.2.1.2 Strategic Advantages\n\n\n\n\n\n\n\nAdvantage\nDescription\n\n\n\n\nFull-stack experience\nExposure to the entire data lifecycle, from point of collection to point of reporting. This end-to-end visibility provides deep understanding of data provenance and quality issues at the source.\n\n\nRapid decision cycles\nUnlike federal agencies where changes take years, local departments often need immediate solutions. An analyst can propose, build, and implement a new tracking tool in a single week.\n\n\nHigh impact\nThe hybrid professional in this setting is often the only person with advanced data skills, giving them disproportionate influence on agency operations and strategy.\n\n\n\n\n\n18.2.1.3 Navigating the Challenges\n\nTitle ambiguity: These roles almost never carry titles like “Informatics Specialist” or “Business Analyst.” Instead, search for titles such as Health Educator, Emergency Preparedness Coordinator, Disease Intervention Specialist, or Program Coordinator.\nResource constraints: Salaries are typically lower than in the private sector. Consider this compensation gap as an investment in real-world experience.\nLegacy infrastructure: Be prepared to work with outdated hardware, slow internet connections, and paper-based processes. Learning to design systems within these constraints is a critical skill for any high-level architect.\n\n\n\n\n18.2.2 The Intermediary: State Health Departments and Large Universities\nState-level agencies and large academic institutions act as the “middle layer” of the public health stack, translating federal funding and standards into local implementation.\n\n18.2.2.1 State Health Departments\nState health departments manage the centralized systems of record: the Electronic Disease Surveillance System (EDSS), the Immunization Information System (IIS), and Vital Records. Roles here focus heavily on interoperability and compliance. The analyst ensures that data flowing from hospitals (via HL7 messages) meets the validation rules required by the CDC.\nTypical titles: Epidemiologist (Informatics), Surveillance System Manager, Data Modernization Lead, Interoperability Coordinator.\n\n\n18.2.2.2 Universities and Academic Centers\nUniversities are often the engines of research and workforce development. They hire staff to manage large research datasets, coordinate multi-site clinical trials, or evaluate public health programs. Positions often bridge the gap between Principal Investigators (academics) and Data Management Centers (IT).\nTypical titles: Research Data Manager, Center Administrator, Project Coordinator.\n\n\n\n18.2.3 The Engine: Federal Contractors and Consultancies\nA significant portion of the public health IT workforce is employed not by the government directly, but by the private ecosystem that supports it. Since government agencies often lack the flexibility to hire rapid-response IT teams, they contract this work out to consulting firms.\n\n18.2.3.1 Large Systems Integrators\nFirms like Deloitte, Accenture, Booz Allen Hamilton, Leidos, GDIT (General Dynamics), and ICF execute the massive modernization contracts (e.g., CDC’s Data Modernization Initiative).\n\nThe culture: These firms operate with corporate speed. They value “billable” skills: specific experience with platforms (Salesforce, ServiceNow, Azure) combined with the ability to obtain security clearances (Public Trust).\nThe hybrid value: These firms need Subject Matter Experts (SMEs) who can sit in a room with CDC scientists, understand the nuance of “hepatitis serology,” and translate it into user stories for software developers.\n\n\n\n18.2.3.2 Niche Public Health Consultancies\nSpecialized boutique firms focus exclusively on the public health domain, often offering a culture more aligned with public health values.\nKey players: J Michael Consulting, Berry Technology Solutions, Lantana Consulting Group (standards focus), Public Health Informatics Institute (PHII) (a nonprofit/consultancy hybrid).\nThese firms often handle the “high-touch” aspects of informatics: training, technical assistance to states, and standards development (HL7/FHIR implementation guides).\n\n\n\n18.2.4 The Infrastructure: EHR Vendors and Retail Health\nPrivate sector entities that generate clinical data are increasingly hiring public health experts to ensure their products remain relevant and compliant.\n\n18.2.4.1 Electronic Health Record Vendors\nCompanies like Epic, Oracle Health (Cerner), MEDITECH, and athenahealth need professionals who understand the regulatory landscape. Federal regulations (like “Promoting Interoperability”) mandate that EHRs must be able to send data to public health agencies.\nTypical titles: Implementation Consultant, Regulatory Affairs Analyst, Public Health Liaison.\n\n\n18.2.4.2 National Pharmacy Chains\nRetail health giants like CVS Health, and Walgreens have transformed into primary care and public health hubs. Since the COVID-19 pandemic, pharmacies generate massive volumes of immunization and testing data. These companies recruit informatics professionals to manage complex data flows between thousands of retail locations and fifty distinct state immunization registries.\nFocus areas: Population health analytics, medication adherence tracking, interoperability management.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#the-rfp-search-strategy",
    "href": "chapters/E-career-navigation.html#the-rfp-search-strategy",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.3 The RFP Search Strategy",
    "text": "18.3 The RFP Search Strategy\nStandard job searching is reactive: a candidate waits for a job description to be posted and then applies. The Bridgeframe strategy is proactive: it analyzes the business drivers of hiring to identify opportunities before they are advertised.\nIn the public sector, jobs are downstream of projects, and projects are downstream of funding. Before a job description can be posted, an agency must secure a budget and usually hire a vendor to execute the work. This procurement process leaves a public paper trail that savvy job seekers can follow.\nThe central document in this trail is the Request for Proposal (RFP).\n\n18.3.1 The Logic of Procurement-Based Job Searching\nWhen a government agency needs to modernize a system (like the CancerSurv registry), they rarely build it in-house. Instead, they release an RFP to hire a private vendor. This document contains three critical pieces of intelligence:\n\n\n\n\n\n\n\nComponent\nWhat It Tells You\n\n\n\n\nProblem Statement\nDescribes exactly what is broken (e.g., “Legacy mainframe cannot accept HL7 2.5.1 messages”)\n\n\nSolution Requirements\nDescribes exactly what skills are needed (e.g., “Vendor must provide data migration, cloud architecture, and training for 500 registrars”)\n\n\nTimeline\nDescribes when the work (and the hiring) will begin\n\n\n\nThe strategy:\n\nFinding the RFP = Finding the Demand: If you find an RFP for “Surveillance System Modernization,” you know that specific high-value skills (Cloud, SQL, Training) are about to be in demand in that geographic area.\nFinding the Award Notice = Finding the Employer: Eventually, the agency announces which vendor won the contract. That vendor has just received a multi-million dollar contract and is legally obligated to staff the project immediately.\n\n\n\n18.3.2 Google Boolean Search Operators\nMost RFPs are buried on obscure government procurement portals. However, they are almost always indexed by Google. By using Boolean search operators, you can locate the source documents directly.\n\n18.3.2.1 Limiting Results by Date\nTo find only recent postings, use Google’s date filtering tools:\n\nUsing Google Search Tools: After running your search, click “Tools” below the search bar, then “Any time” → select “Past week,” “Past month,” or “Custom range.”\nUsing URL Parameters: Append &tbs=qdr:w (past week), &tbs=qdr:m (past month), or &tbs=qdr:y (past year) to your Google search URL.\nUsing the after: Operator: Add after:2026-01-01 to your search string to find only documents indexed after a specific date.\n\nFor job searches, limiting to the past week or month ensures you are seeing active opportunities rather than archived postings. For RFP searches, consider a broader timeframe (past 3 to 6 months) since procurement cycles move slowly.\n\n\n18.3.2.2 Automating Your Search with Google Alerts\nRather than running manual searches repeatedly, use Google Alerts to have new results delivered to your inbox automatically.\nSetting up an RFP Alert:\n\nGo to google.com/alerts\nEnter your search query (use the Boolean strings from this chapter)\nClick “Show options” to configure:\n\nHow often: “As-it-happens” or “Once a day”\nSources: Select “Automatic” or narrow to specific types\nLanguage: English (or your preferred language)\nRegion: United States (or your target region)\nHow many: “All results” to ensure nothing is missed\nDeliver to: Your email address or RSS feed\n\nClick “Create Alert”\n\nExample Alert Queries for Hybrid Professionals:\n\n\"Request for Proposal\" \"public health\" \"informatics\" site:.gov\n\"Contract Award\" \"CDC\" \"data modernization\"\n\"public health\" \"business analyst\" job\n\nGoogle Alerts will email you whenever new content matching your query is indexed. This passive monitoring ensures you never miss an opportunity because you forgot to run a search.\n\n\n18.3.2.3 The Master Search String\n(RFP OR \"Request for Proposal\" OR \"Request for Applications\" OR \n\"Notice of Funding Opportunity\") AND (\"Public Health\" OR \"Epidemiology\" \nOR \"Surveillance\" OR \"Informatics\" OR \"Vital Records\") AND (\"System\" \nOR \"Modernization\" OR \"Implementation\" OR \"Software\" OR \"Data\") \nfiletype:pdf site:.gov\nDeconstructing the operators:\n\n\n\n\n\n\n\nOperator Group\nPurpose\n\n\n\n\nDocument type\n(RFP OR \"Request for Proposal\"...) casts a wide net for solicitation names\n\n\nDomain\nAND (\"Public Health\"...) ensures content is relevant to the field\n\n\nActivity\nAND (\"System\"...) ensures the project has a technical/IT component\n\n\nfiletype:pdf\nOfficial government RFPs are almost universally PDF documents\n\n\nsite:.gov\nRestricts results to US government domains (federal, state, and local)\n\n\n\n\n\n\n18.3.3 Customizing Your Search\n\n18.3.3.1 Strategy A: Geographic Targeting\nIf you wish to work in a specific state or region, add location keywords:\n...AND (\"Texas\" OR \"TX\" OR \"Austin\")...\n\n\n18.3.3.2 Strategy B: The Winning Vendor Hunt\nFinding an old RFP is valuable because it leads to the Award Notice. If an RFP was due 3 months ago, the contract has likely just been awarded.\n(\"Notice of Award\" OR \"Contract Award\" OR \"Bid Tabulation\" OR \n\"Intent to Award\") AND (\"Department of Health\" OR \"CDC\") AND \n(\"IT\" OR \"Informatics\" OR \"Data System\") 2024..2026\nThe 2024..2026 operator (two dots) tells Google to search for numbers within that range, helping identify recent fiscal year awards.\n\n\n18.3.3.3 Strategy C: Skill-Specific Targeting\nIf you have a niche skill (e.g., HL7 FHIR), search for RFPs that specifically require it:\n(RFP OR \"Scope of Work\") AND \"Public Health\" AND (\"FHIR\" OR \"HL7\" \nOR \"Interoperability\") filetype:pdf\n\n\n\n\n\n\nNoteCancerSurv Example: The RFP Strategy in Action\n\n\n\nThe Scenario: The State Health Department releases an RFP for the “CancerSurv Modernization Project” (RFP #2026-CS-001). The RFP describes a need to “replace a legacy mainframe system with a cloud-based registry compliant with NAACCR standards.”\nThe Job Seeker: Alex is a Business Analyst with SQL skills who wants to transition into public health. Alex runs the Google Boolean search:\n(\"Request for Proposal\") AND \"Cancer Registry\" AND \"Modernization\" filetype:pdf site:.gov\nThe Insight: Alex finds the RFP and reads the Scope of Work. Section 4.2 states: “Vendor must provide training and change management for 150 hospital registrars.” Alex realizes this is a green flag, as Alex has extensive experience in corporate training.\nThe Move: Alex searches for the award notice: (\"Contract Award\" OR \"Bid Tabulation\") AND \"CancerSurv\". The search reveals that TechHealth Solutions won the bid last month.\nAlex goes to TechHealth’s careers page. There is no “Cancer Registry Trainer” job listed yet. Alex sends a proactive application to the TechHealth hiring manager: “I see TechHealth won the CancerSurv contract. The RFP requires extensive training for 150 registrars. My background in large-scale change management training would allow you to meet that deliverable immediately.”\nThe Result: TechHealth was indeed worrying about how to staff the training component. Alex is interviewed immediately, bypassing the competitive stack of generic resumes.\n\n\n\n\n\n18.3.4 The Blind Application\nOnce you identify a winning vendor, the next step is the proactive application. Even if the vendor has not yet posted job openings, they are likely in the “ramp-up” phase.\nSample outreach script:\n\n“I noticed that [Company Name] was recently awarded the Disease Surveillance Modernization contract. I have reviewed the RFP requirements and see a need for [specific skill]. I have [X] years of experience executing exactly this type of migration for public health agencies and would like to discuss how I can support the new contract.”\n\nThis approach positions you not as a job seeker asking for a favor, but as a solution provider solving an immediate problem for the vendor.\n\n\n18.3.5 The Power of Collaboration\nYou do not have to navigate the job market, or the RFP process, alone. Collaboration multiplies your reach and fills gaps in your expertise.\n\n18.3.5.1 For RFP Responses\nIf you identify an RFP opportunity but lack certain skills (whether technical, domain-specific, or the hybrid professional perspective itself), consider partnering with others. Teaming arrangements are common in government contracting; a small firm with deep domain expertise can partner with a larger firm that has the administrative capacity to manage federal contracts.\nPotential collaborators in the Bridgeframe space:\n\nIntersect Collaborations LLC: Specializes in bridging business analysis and public health\nChaptico Hundred: Purpose-driven consultancy with transformative expertise\nMagpie Public Health LLC: Specialists in mixed-methods research and qualitative study design\n\n\n\n18.3.5.2 For Job Seekers\nEven if you already have a position, you may benefit from coaching or mentorship to prepare for a hybrid role. Reaching out to consultancies or professionals in the Bridgeframe space can help you:\n\nTranslate your existing experience into the language of the other domain\nIdentify skill gaps and training resources\nPractice “bridge” interview techniques\nConnect with networks that understand hybrid roles\n\nThe hybrid professional community is still small enough that collaboration is often more valuable than competition. A referral from someone who understands your unique skill set can open doors that cold applications cannot.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#specialized-job-boards-and-platforms",
    "href": "chapters/E-career-navigation.html#specialized-job-boards-and-platforms",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.4 Specialized Job Boards and Platforms",
    "text": "18.4 Specialized Job Boards and Platforms\nWhile the RFP strategy unearths hidden opportunities, traditional platforms remain a necessary component of a comprehensive search. The hybrid professional must know where to look.\n\n18.4.1 Platform Directory\n\n\n\nPlatform\nTarget Audience\nSearch Strategy\n\n\n\n\nGovernmentJobs.com\nLocal/State Government\nSearch for Program Specialist, Health Analyst, Epidemiologist\n\n\nUSAJobs.gov\nFederal Agencies (CDC, CMS, IHS)\nFilter for Job Series 0601 (General Health Science), 0685 (Public Health Program Specialist), 2210 (IT Management)\n\n\nPublicHealthJobs.org\nAcademia/Nonprofit (ASPPH)\nExcellent for roles at universities and NGOs\n\n\nAMIA Career Center\nInformatics Professionals\nHigh-quality listings; often requires advanced degrees\n\n\nHIMSS JobMine\nHealth IT/Vendors\nHeavily focused on the private sector and hospital IT\n\n\n3RNET\nRural/Underserved\nNational Rural Recruitment and Retention Network; essential for “incubator” roles\n\n\nORISE (Zintellect)\nFellowships/Internships\nPrimary portal for CDC fellowships, including PHIFP\n\n\n\n\n\n18.4.2 Using Google Jobs for Passive Monitoring\nGoogle Jobs aggregates listings from multiple job boards into a single interface. More importantly, it offers a Following feature that sends you email alerts when new jobs matching your criteria are posted.\nSetting up Google Jobs Alerts:\n\nGo to google.com and search for jobs using keywords like public health informatics jobs or health data analyst jobs\nGoogle will display a job search panel. Refine your search using:\n\nLocation: Enter your target city, state, or “Remote”\nDate posted: Filter to “Past week” or “Past 3 days”\nType: Full-time, Part-time, Contractor\nCompany type: Filter by employer category if available\n\nOnce your search is configured, look for the “Turn on” or “Follow” button (often shown as a bell icon or “Create alert”)\nEnable notifications to receive email alerts when new jobs matching your search are posted\n\nRecommended Searches to Follow:\n\npublic health informatics (your location or Remote)\nhealth data analyst CDC (Remote or Washington, DC)\nepidemiologist data systems (your state)\nhealth IT business analyst (Remote)\n\nBy following multiple searches, you build a passive monitoring system that brings opportunities to you rather than requiring daily manual searches. Combine this with Google Alerts for RFP monitoring to create a comprehensive opportunity radar.\n\n\n18.4.3 Target Employer Watch List\nA proactive search involves monitoring specific companies known to hire hybrid talent.\n\n18.4.3.1 Federal Contractors\nThese firms are the primary engine of federal health IT employment:\n\nLeidos, GDIT, Booz Allen Hamilton\nDeloitte, Accenture Federal Services\nICF, Maximus\n\nStrategy: Monitor their “Health” or “Civilian” career pages. Look for keywords like “CDC,” “CMS,” or “Data Modernization.”\n\n\n18.4.3.2 Niche Public Health Firms\nThese smaller firms often offer a more mission-driven culture:\n\nJ Michael Consulting, Karna, Goldbelt\nChickasaw Nation Industries, DLH Corp\nAbt Associates, JSI (John Snow, Inc.)\n\nStrategy: These firms often recruit directly from public health conferences (CSTE, NACCHO). Follow their LinkedIn pages for contract wins.\n\n\n18.4.3.3 Digital Health and Global Health Tech\nThese organizations build technology platforms used by health programs worldwide:\n\nDimagi (creators of CommCare, a mobile data collection platform)\nDHIS2 (the world’s largest health management information system)\n\nStrategy: These organizations value professionals who understand both the technology and the public health context in which it operates.\n\n\n18.4.3.4 NGOs and Nonprofits\n\nTask Force for Global Health (home of PHII)\nAPHL (Association of Public Health Labs)\nASTHO, NACCHO\n\nStrategy: These organizations often manage cooperative agreements with the CDC, functioning similarly to contractors but with a nonprofit structure.\n\n\n\n\n\n\nImportantDon’t Limit Your Search\n\n\n\nThe employers listed above represent a starting point, not a comprehensive directory. Any health department, at any level, could benefit from hybrid professionals. In fact, smaller health departments may lack the expertise to even identify the need for a hybrid professional in the first place. They may not know to advertise for an “Informatics Specialist” because they have never had one.\nThis creates a unique opportunity: you can approach local health departments proactively, demonstrate how your skills address their data challenges, and help them define a role that did not previously exist. The “hidden” job market is especially fertile in under-resourced settings where the need is greatest but the vocabulary to describe it is absent.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#building-experience-through-volunteering",
    "href": "chapters/E-career-navigation.html#building-experience-through-volunteering",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.5 Building Experience Through Volunteering",
    "text": "18.5 Building Experience Through Volunteering\nEven a few hours each week of volunteer work can solidify your experience and help you stand out as a candidate who goes above and beyond. For professionals transitioning into hybrid roles, or those seeking to demonstrate data and analysis skills in a public health context, volunteering offers a low-barrier path to meaningful experience.\n\n18.5.1 Volunteer Platforms for Data and Analysis Skills\nThese platforms connect skilled volunteers with organizations that need data analysis, technology, and public health expertise:\n\n\n\nPlatform\nFocus Area\nBest For\n\n\n\n\nVolunteerMatch\nGeneral nonprofit matching\nFinding local health organizations needing data help\n\n\nCatchafire\nSkills-based volunteering\nProject-based work with defined scope and timeline\n\n\nTechfleet\nTech volunteers for nonprofits\nSoftware development and data infrastructure projects\n\n\nDataKind\nData science for social good\nHigh-impact data projects with mentorship opportunities\n\n\nStatistics Without Borders\nStatistical consulting\nEpidemiological and public health research support\n\n\nUnited Nations Volunteers\nGlobal development\nInternational health programs and humanitarian response\n\n\nCode for America\nCivic technology\nGovernment and public sector technology projects\n\n\n\n\n\n18.5.2 The Value of Volunteer Experience\nWhile volunteer work is unpaid, it delivers three critical benefits:\n\nPurpose while you wait: Job searches can be demoralizing. Volunteer work provides structure, meaning, and forward momentum during gaps in employment.\nSkill maintenance and growth: Skills atrophy without use. Volunteering keeps your technical abilities sharp and exposes you to new tools, datasets, and problem types.\nReal experience for your resume: A completed volunteer project analyzing immunization coverage for a community health center is indistinguishable on a resume from paid consulting work. It demonstrates capability and initiative.\n\n\n\n18.5.3 Strategic Approaches to Volunteering\nFor experienced professionals, a more intentional approach to volunteering can maximize both impact and career benefit.\n\n18.5.3.1 Treat Volunteer Work as Discounted Professional Work\nSome organizations struggle to fully value volunteer contributions when the cost is invisible. One practical approach is to frame volunteer work as professional services provided at a reduced rate. For example, you can send an invoice that reflects the true market value of the work alongside the discounted rate, even if the final amount due is $0.\nThis approach:\n\nMakes your contribution tangible to the organization\nReinforces the expertise being provided\nCreates documentation of professional-level work for your portfolio\nHelps the organization understand the true value of what they received\n\n\n\n18.5.3.2 Start Your Own Initiative\nAn alternative to traditional volunteering is to start your own business, nonprofit, or independent project to carry forward work you believe is important, even if it is initially unpaid. This approach offers several advantages:\n\nOwnership: You retain rights to frameworks, tools, and products you develop\nPortfolio building: Your work demonstrates skills across multiple clients or use cases\nFlexibility: You control the scope, timeline, and direction of the work\nFoundation for future consulting: Pro bono projects can evolve into paid engagements\n\n\n\n\n\n\n\nTipFrom Volunteer to Consultant\n\n\n\nMany successful public health consultancies began as volunteer efforts. A professional who builds a reporting dashboard for one health department can offer that same framework to others. The first project builds the product; subsequent projects generate revenue. This “productized service” model is especially effective for hybrid professionals whose specialized skills are in high demand but undersupplied.\n\n\nThis strategic approach tends to be most effective for experienced professionals who bring specialized expertise and want to be intentional about how their time and skills are invested. For early-career professionals, traditional volunteering through established platforms may offer more structure and mentorship.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#decoding-job-descriptions",
    "href": "chapters/E-career-navigation.html#decoding-job-descriptions",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.6 Decoding Job Descriptions",
    "text": "18.6 Decoding Job Descriptions\nOne of the most significant barriers to entry is the inconsistency of job titles. A “Business Analyst” in a corporate setting might be called a “Public Health Program Specialist II” in a state agency. The hybrid professional must learn to read job descriptions for skills and activities rather than titles.\n\n18.6.1 Title-to-Function Mapping\n\n\n\n\n\n\n\n\nIf You Want To Do…\nSearch For These Titles…\nLook For These Keywords…\n\n\n\n\nBusiness Analysis\nManagement Analyst, Program Analyst, Operations Coordinator, Product Owner, Systems Analyst\n“Requirements gathering,” “Workflow analysis,” “Process improvement,” “Stakeholder liaison,” “User stories,” “SOP development”\n\n\nInformatics/Data\nEpidemiologist (Data Systems), Health Scientist, Informatics Specialist, Surveillance Coordinator, Data Manager\n“HL7,” “ELR” (Electronic Lab Reporting), “Syndromic Surveillance,” “Registry,” “Interoperability,” “FHIR,” “Cleaning data”\n\n\nProject Management\nPublic Health Advisor, Program Manager, Implementation Specialist, Grant Manager\n“Agile,” “Scrum,” “Scope management,” “Timeline,” “Vendor management,” “Grant reporting,” “Deliverables”\n\n\nTraining/Rollout\nTechnical Assistance (TA) Lead, Onboarding Specialist, Implementation Specialist\n“User adoption,” “Go-live,” “Manuals,” “Capacity building,” “Workforce development”\n\n\n\n\n\n18.6.2 The “Unicorn” Job Description\nOften, hiring managers in public health know they need “someone technical” but do not know exactly what that entails. This results in the unicorn job description: a posting that asks for a PhD in Epidemiology, a PMP certification, 10 years of Python coding experience, and a nursing license.\n\nThe interpretation: This usually signals a lack of role clarity. The agency has a problem (data) and is listing every possible qualification.\nThe strategy: Apply if you meet 50 to 60 percent of the criteria.\nThe interview pivot: Use the interview to perform business analysis on the role itself. Ask: “I see you asked for both Python coding and clinical nursing. Is the primary goal of this role to write the code (Python) or to talk to the nurses who use the system? I excel at the translation between the two.” By helping them define the role, you demonstrate your value as a Business Analyst before you are hired.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#resume-and-interview-strategy",
    "href": "chapters/E-career-navigation.html#resume-and-interview-strategy",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.7 Resume and Interview Strategy",
    "text": "18.7 Resume and Interview Strategy\nTo land the job, you must translate your experience into the language of the employer. This requires a “dual-frame” approach.\n\n18.7.1 The Hybrid Resume\nA standard resume is often chronological and mono-lingual (either all tech or all health). A hybrid resume must explicitly bridge the gap.\n\n18.7.1.1 Strategy: The Translation Bullet Point\nRewrite experience to highlight implications for the other domain.\nPublic Health professional applying for a Tech/BA role:\n\n❌ Original: “Managed the Gonorrhea Surveillance Program.”\n✅ Translated: “Managed a disease surveillance system with 5,000+ annual records; acted as Product Owner to define reporting requirements for the IT vendor and led User Acceptance Testing (UAT) for system upgrades.”\n\nTech/BA professional applying for a Public Health role:\n\n❌ Original: “Gathered requirements for SQL database migration.”\n✅ Translated: “Facilitated workshops with clinical stakeholders to define data standards for a patient registry; ensured system design complied with HIPAA privacy regulations and supported epidemiological reporting needs.”\n\n\n\n18.7.1.2 Strategy: The Dual-Competency Skills Section\nCreate a skills section that visually separates but presents both domains:\n\nTechnical & Analysis: SQL, Tableau, Jira, Visio (BPMN), User Stories, Agile/Scrum\nPublic Health & Regulatory: Epidemiology, Surveillance Systems, HIPAA, HL7/FHIR Standards, CDC Grant Reporting\n\n\n\n\n18.7.2 The Behavioral Bridge Interview\nIn an interview, you will often face a panel with mixed backgrounds (e.g., a Program Director and an IT Manager). Your answers must satisfy both.\n\n18.7.2.1 The Bridge Answer Structure\nWhen answering behavioral questions (“Tell me about a time…”), the narrative arc should always be about translation.\nQuestion: “Tell me about a time you faced a challenge.”\nBridge Answer:\n\nContext: “We had a disconnect where the developers built a feature that did not match the clinical workflow.”\nAction: “I scheduled a joint session where I mapped the clinical process on a whiteboard while the developers watched. I translated clinical terms into technical specifications in real time.”\nResult: “The developers understood the ‘why,’ the nurses got a tool that fit their process, and we reduced data entry errors by 50%.”\n\n\n\n\n18.7.3 Questions to Ask the Employer\nThe questions you ask reveal your sophistication regarding the BA/PH gap.\n\nTo the IT Manager: “How does the development team currently receive input from the epidemiologists? Do you use a specific framework to map public health goals to technical backlogs?”\nTo the Program Manager: “How is the success of this IT project being measured? Are we tracking just ‘system uptime,’ or are we tracking public health metrics like ‘time to intervention’?”\nThe Role-Definition Question: “Who currently owns the ‘translation’ between the clinical subject matter experts and the technical vendors? Is that a gap this role is intended to fill?”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#a-note-for-hiring-managers",
    "href": "chapters/E-career-navigation.html#a-note-for-hiring-managers",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.8 A Note for Hiring Managers",
    "text": "18.8 A Note for Hiring Managers\nThis appendix is primarily for job seekers, but a note for those hiring is essential. The “talent shortage” in public health informatics is often a “visibility shortage.” The candidates exist, but they are filtered out by rigid requirements.\n\n18.8.1 Re-Examine Degree Requirements\nDo not filter exclusively for MPH or Computer Science degrees. A History major who has spent five years managing a complex nonprofit database often makes an excellent Business Analyst due to their research and synthesis skills.\n\n\n18.8.2 Hunt for “Super-Users”\nThe best future analysts are often currently working inside your organization as nurses, registrars, or administrative assistants. They are the ones who become the “go-to” person for their team’s technology problems. They possess the deep domain context that is hardest to teach; technical skills (SQL, Visio) can be taught more easily.\n\n\n18.8.3 Use Your Contracts as Signals\nIf you are a vendor, use your RFP wins as recruitment tools. Advertise that you are “staffing for a newly awarded CDC modernization contract.” This signals stability and high-impact work to prospective candidates.\n\n\n18.8.4 Make Your Postings Discoverable via Google for Jobs\nGoogle for Jobs aggregates job postings from across the web and surfaces them directly in Google Search results. Millions of job seekers search Google every day, and properly formatted job postings can reach candidates who might never visit your agency’s careers page or specialized job boards.\nWhy This Matters for Public Health Employers:\n\nBroader reach: Candidates searching for “epidemiologist jobs” or “public health data analyst” will see your posting in their Google search results, not just those who know to check your specific portal.\nEquity in hiring: Many qualified candidates from non-traditional backgrounds may not know to search specialized platforms like USAJobs.gov or PublicHealthJobs.org. Google surfaces your posting to anyone searching relevant terms.\nNo cost: Google for Jobs is free; you simply need to ensure your postings meet their technical requirements.\n\nHow to Get Your Job Postings on Google:\n\nIf you use a major job board or ATS: Platforms like LinkedIn, Indeed, ZipRecruiter, Lever, Greenhouse, and Workday already integrate with Google for Jobs. Postings on these platforms are automatically indexed.\nIf you post directly on your website: Add structured data markup (JSON-LD) to your job posting pages following Google’s JobPosting schema. Key fields include:\n\nJob title, description, and location\nSalary range (increasingly expected by candidates)\nDate posted and application deadline\nEmployment type (full-time, part-time, contract)\nRemote work eligibility\n\nTest your implementation: Use Google’s Rich Results Test to verify your structured data is correctly formatted before publishing.\nKeep postings current: Google penalizes stale or expired job postings. Remove listings promptly when positions are filled, and ensure “date posted” fields are accurate.\n\n\n\n\n\n\n\nTipFor Small Health Departments\n\n\n\nIf your agency lacks IT resources to implement structured data, post your positions on a platform that already integrates with Google for Jobs (such as Indeed or LinkedIn). This ensures discoverability without requiring technical implementation on your end.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/E-career-navigation.html#quick-reference-search-string-cheat-sheet",
    "href": "chapters/E-career-navigation.html#quick-reference-search-string-cheat-sheet",
    "title": "18  Career Navigation for Hybrid Professionals",
    "section": "18.9 Quick Reference: Search String Cheat Sheet",
    "text": "18.9 Quick Reference: Search String Cheat Sheet\nUse these strings in Google to identify opportunities. Remember to use Google’s date filtering: click Tools → Any time → Past week (for jobs) or Past month (for RFPs) to see only recent postings. Alternatively, add after:2026-01-01 to your search string.\n\n18.9.1 1. The Generalist Hybrid Search\n(\"Business Analyst\" OR \"Systems Analyst\" OR \"Product Owner\") AND \n(\"Public Health\" OR \"Epidemiology\" OR \"Infectious Disease\") AND \n(\"Remote\" OR \"Hybrid\") after:2026-01-01\n\n\n18.9.2 2. The Informatics Specialist Search\n(\"Informatics\" OR \"Informatician\") AND (\"HL7\" OR \"FHIR\" OR \n\"Interoperability\" OR \"ELR\" OR \"eCR\") AND (\"CDC\" OR \"DOH\" OR \n\"Department of Health\") after:2026-01-01\n\n\n18.9.3 3. The RFP Hunter (Contract Finding)\n(RFP OR \"Request for Proposal\" OR \"Notice of Funding Opportunity\") \nAND (\"Surveillance System\" OR \"Registry\" OR \"Data Modernization\") \nAND (\"2025\" OR \"2026\") filetype:pdf\n\n\n18.9.4 4. The Winning Vendor Search\n(\"Contract Award\" OR \"Notice of Award\") AND (\"Department of Health\" \nOR \"CDC\") AND (\"IT\" OR \"System\") 2024..2026\n\n\n18.9.5 5. The ATS Platform Search\nsite:lever.co OR site:greenhouse.io OR site:workday.com \n(\"Public Health\" AND \"Analyst\")\nThis searches inside common HR platforms to find jobs that might not be indexed on aggregators.\n\n\n\n\n\n\nTipApplying the Bridgeframe Mindset\n\n\n\nThe job search for a hybrid professional is, in itself, an act of business analysis. It requires analyzing the market (Current State), identifying the gaps where hybrid skills add value (Future State), and designing a strategy (The Application) to bridge that gap.\nWhether the journey begins in a rural health department, building foundational knowledge of public health reality, or leverages advanced search operators to uncover a major federal contract, the opportunities are vast. The field of public health is undergoing a massive digital transformation. It does not just need code; it needs context. It needs the Bridgeframe professional.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Career Navigation for Hybrid Professionals</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html",
    "href": "chapters/C-glossary.html",
    "title": "19  Glossary",
    "section": "",
    "text": "19.1 A\nThis glossary provides comprehensive terminology mapping between Business Analysis (BA) and Public Health (PH) domains. Terms are organized alphabetically with cross-references.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#a",
    "href": "chapters/C-glossary.html#a",
    "title": "19  Glossary",
    "section": "",
    "text": "Acceptance Criteria (BA)\n\nConditions that a solution must meet to be accepted by stakeholders. PH equivalent: Evaluation protocol measures, success criteria.\n\nAcceptance Testing (BA)\n\nFormal testing to verify a solution meets acceptance criteria. PH equivalent: Pilot evaluation, field testing.\n\nActivity (PH)\n\nAn action taken as part of a program or intervention. BA equivalent: Functional requirement, use case step.\n\nAdaptability (Implementation Science)\n\nDegree to which an intervention can be modified for local context. BA equivalent: Configurability, customization capability.\n\nAfter-Action Review (PH)\n\nStructured review of what happened, why, and how to improve. BA equivalent: Lessons learned, retrospective.\n\nAgile (BA)\n\nIterative development methodology emphasizing flexibility and stakeholder collaboration. PH parallel: Adaptive management, PDSA cycles.\n\nAccountability Structures (Organizational)\n\nGovernance mechanisms ensuring responsibility and ownership for processes and outcomes. Includes process owners, steering committees, escalation paths, and service-level agreements (SLAs). Application: Both BA and PH require clear accountability to prevent process drift.\n\nAndragogy (Education)\n\nTheory of adult learning emphasizing self-direction, experience-based learning, and relevance. Key principles include connecting training to real problems and appealing to internal motivation rather than compliance.\n\nApplicant Tracking System (ATS) (Workforce)\n\nAutomated software used by employers to filter job applications by keywords and qualifications. Hybrid professionals must tailor resumes to pass ATS filters in both technical and public health domains.\n\nAward Notice (Government Procurement)\n\nPublic announcement of which vendor won a government contract. Job seekers can use award notices to identify employers who are actively staffing new projects.\n\nAutomation Spectrum (Process Optimization)\n\nRange of automation levels from manual through assisted, partial, conditional, to full automation. Each level defines different human-machine collaboration and oversight requirements.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#b",
    "href": "chapters/C-glossary.html#b",
    "title": "19  Glossary",
    "section": "19.2 B",
    "text": "19.2 B\n\nBacklog (BA/Agile)\n\nPrioritized list of work items. PH equivalent: Workplan, action item list.\n\nBaseline (Both)\n\nStarting point measurement for comparison. In BA, current state metrics. In PH, epidemiological baseline data.\n\nBeneficiary (PH)\n\nPerson or group intended to benefit from a program. BA equivalent: End user, customer.\n\nBlind Application (Workforce)\n\nProactive job application sent to a vendor before they post open positions, typically after identifying a contract award. Positions the applicant as a solution provider rather than a job seeker.\n\nBridge Role (Workforce)\n\nPosition explicitly designed for translation between IT and public health domains, such as Health Informatics Liaison or Technical Program Analyst. These roles typically have dual reporting and cross-domain performance metrics.\n\nBronze Layer (Data Architecture)\n\nThe first layer in medallion architecture containing raw, unprocessed data as received from source systems. Data is preserved in its original format for auditability and reprocessing. PH equivalent: Ingestion layer, source data repository. Key PH roles: Data managers, data stewards, IT operations.\n\nBPMN (Business Process Model and Notation) (BA)\n\nStandard for process diagrams. PH equivalent: Intervention flowchart, workflow diagram.\n\nBug (BA)\n\nDefect in software. PH equivalent: Adverse event, variance from protocol.\n\nBullying (Organizational) (Organizational)\n\nHarmful behaviors that intimidate, belittle, or undermine staff, leading to silence, unreported errors, and talent departure. Response: Establish psychological safety as a core organizational value; enforce clear anti-bullying policies with consequences; provide anonymous reporting channels; train managers to recognize and address harmful behaviors early.\n\nBusiness Case (BA)\n\nJustification for a project based on expected benefits. PH equivalent: Needs assessment, funding proposal.\n\nBusiness Need (BA)\n\nProblem or opportunity driving a project. PH equivalent: Public health challenge, health need.\n\nBusiness Rule (BA)\n\nConstraint governing system behavior. PH equivalent: Clinical guideline, protocol rule.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#c",
    "href": "chapters/C-glossary.html#c",
    "title": "19  Glossary",
    "section": "19.3 C",
    "text": "19.3 C\n\nCapacity Building (PH)\n\nDeveloping skills and resources in individuals and organizations. BA equivalent: Training, organizational readiness.\n\nCase Definition (PH)\n\nCriteria for identifying disease cases. BA equivalent: Data validation rules, entity definition.\n\nCFIR (Consolidated Framework for Implementation Research) (PH)\n\nFramework for understanding implementation context. BA application: Assessing organizational readiness, NFR refinement.\n\nChampion (Both)\n\nPerson who advocates for and promotes an initiative. Usage similar in both domains.\n\nChange Management (BA)\n\nApproach for transitioning organizations to new systems/processes. PH equivalent: Implementation strategy, adoption support.\n\nChange Control (Project Management)\n\nFormal process for evaluating, approving, and documenting changes to scope, schedule, or requirements. Prevents scope creep by requiring explicit stakeholder sign-off before modifying baselines. Components: Change request form, impact assessment, approval authority, decision log.\n\nCentralization (Organizational)\n\nStrategy of consolidating core functions (e.g., data cleaning, report generation) into shared services to reduce duplication, ensure consistency, and build specialized expertise. Application: When multiple teams rely on the same data, that data should be centrally stored and cleaned.\n\nChange Resistance (Organizational)\n\nReluctance to adopt new tools, processes, or approaches due to fear, uncertainty, perceived loss of control, or comfort with existing methods. Manifests as slow adoption, workarounds, or shadow systems. Mitigation: Co-design solutions with end users; run small pilots before full rollout; provide training and support; align incentives with adoption; demonstrate concrete benefits through success stories.\n\nCAHIMS / CPHIMS (Workforce)\n\nCertified Associate/Professional in Healthcare Information and Management Systems. HIMSS certifications demonstrating health IT competency; valuable credentials for BA professionals entering the health sector.\n\nCommunity Health Assessment (PH)\n\nSystematic examination of health status and needs. BA equivalent: Current state analysis, needs assessment.\n\nCommunity of Practice (Workforce)\n\nInternal group connecting professionals across organizational silos to share knowledge. In hybrid contexts, CoPs facilitate BA/PH translation through brown-bag sessions, shared channels, and joint retrospectives.\n\nCommunity Partner (PH)\n\nExternal organization collaborating on health initiatives. BA equivalent: Stakeholder, vendor, integration partner.\n\nComplexity (Implementation Science)\n\nPerceived difficulty of implementing an intervention. BA equivalent: Usability concerns, training requirements.\n\nCompliance (Both)\n\nAdherence to regulations, standards, or requirements. In PH, often HIPAA, CDC standards. In BA, often regulatory NFRs.\n\nConstraint (BA)\n\nLimitation on solution options. PH equivalent: Policy constraint, resource limitation.\n\nCurrent State (BA)\n\nExisting situation before change. PH equivalent: Baseline, pre-intervention status.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#d",
    "href": "chapters/C-glossary.html#d",
    "title": "19  Glossary",
    "section": "19.4 D",
    "text": "19.4 D\n\nData Dictionary (Both)\n\nDocumentation of data elements, definitions, and formats. Used similarly in both domains.\n\nData Lake (Data Architecture)\n\nCentralized repository for storing raw data in native format. Supports schema-on-read, allowing flexible analysis without predefined structure. PH application: Repository for diverse health data sources (EHRs, labs, vital records) before standardization.\n\nData Lakehouse (Data Architecture)\n\nArchitecture combining data lake flexibility with data warehouse reliability. Supports both raw data storage and structured analytics. Enables medallion architecture patterns.\n\nData Lineage (Data Architecture)\n\nDocumentation of data’s origin and transformations from source to final output. PH equivalent: Chain of custody for data, audit trail.\n\nData Manager (PH)\n\nProfessional responsible for receiving, organizing, and maintaining data from source systems. Works primarily with Bronze layer data, ensuring completeness and proper storage of incoming files. BA equivalent: Data steward, data operations specialist.\n\nData Quality (Both)\n\nAccuracy, completeness, and reliability of data. Critical in both domains.\n\nDefect (BA)\n\nFlaw in a deliverable. PH equivalent: Protocol deviation, adverse event.\n\nDeliverable (BA)\n\nOutput of project work. PH equivalent: Program output, product.\n\nDemo (BA/Agile)\n\nPresentation of completed work. PH equivalent: Progress presentation, milestone review.\n\nDisruptive Interpersonal Patterns (Organizational)\n\nObservable behaviors that undermine team function, such as unpredictable decision-making, intimidation, or creating fear-based dynamics. Approach: Address behaviors rather than speculating on motives or diagnosing; document conduct expectations clearly; use HR and governance processes to protect team function and individual well-being. See also: Bullying (Organizational), Psychological Safety.\n\nDeduplication of Effort (Organizational)\n\nIdentifying and eliminating redundant work across teams. Mechanisms include shared data layers, internal tool registries, and knowledge bases with reusable templates.\n\nDual-Framework Thinking (Workforce)\n\nCore competency of hybrid professionals: the ability to map Agile artifacts (user stories, sprints) to Logic Model outcomes (activities, outputs) and vice versa.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#e",
    "href": "chapters/C-glossary.html#e",
    "title": "19  Glossary",
    "section": "19.5 E",
    "text": "19.5 E\n\nEffectiveness (PH)\n\nDegree to which an intervention achieves intended outcomes. BA equivalent: Solution value, ROI.\n\nElicitation (BA)\n\nTechniques for gathering requirements from stakeholders. PH equivalent: Community engagement, data collection.\n\nEpic (BA/Agile)\n\nLarge user story spanning multiple sprints. PH equivalent: Grant objective, program goal.\n\nEpidemiological Baseline (PH)\n\nPre-intervention disease/health status data. BA equivalent: Current state metrics.\n\nEvaluation (Both)\n\nAssessment of value, outcomes, or quality. In BA, solution evaluation. In PH, program evaluation.\n\nExpectation Management (Organizational/PM)\n\nPractice of aligning goals and delivery commitments with capacity, constraints, and evidence. Techniques: Evidence-based estimation, transparent SLAs/OLAs, and frequent re-baselining when scope or resources change.\n\nEvidence-Based Estimation (Project Management)\n\nUsing historical data (velocity, throughput, cycle time) rather than guesswork to forecast delivery timelines and capacity. Reduces unrealistic expectations by grounding commitments in demonstrated capability. Application: Track actual completion rates over time; use running averages to predict future sprints.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#f",
    "href": "chapters/C-glossary.html#f",
    "title": "19  Glossary",
    "section": "19.6 F",
    "text": "19.6 F\n\nFeasibility (Both)\n\nAssessment of whether something can be done. Technical, economic, operational (BA) or evidence-based, resource, political (PH).\n\nFidelity (PH)\n\nDegree to which an intervention is delivered as designed. BA equivalent: Conformance to specifications.\n\nFocus Group (Both)\n\nGroup discussion for gathering perspectives. Used similarly in both domains.\n\nFunctional Requirement (BA)\n\nWhat a system must do. PH equivalent: Program activity, intervention component.\n\nFuture State (BA)\n\nDesired situation after change. PH equivalent: Program goals, intended outcomes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#g",
    "href": "chapters/C-glossary.html#g",
    "title": "19  Glossary",
    "section": "19.7 G",
    "text": "19.7 G\n\nGemba (Lean/Both)\n\nGoing to the actual place where work happens. Applicable in both BA observation and PH site visits.\n\nGo-Live (BA)\n\nSystem deployment to production. PH equivalent: Program launch, intervention rollout.\n\nGold Layer (Data Architecture)\n\nThe final layer in medallion architecture containing curated, analytics-ready data. Optimized for reporting, dashboards, and decision support. Examples include line lists for contact tracing, summary reports, and regulatory submissions. PH equivalent: Reporting layer, analytics datasets, CDC submission files.\n\nGovernance (Both)\n\nDecision-making structure and authority. Similar usage in both domains.\n\nGPS Format (PH-adapted)\n\n“Given [context], Person [role] Should [action]” user story format for clinical contexts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#h",
    "href": "chapters/C-glossary.html#h",
    "title": "19  Glossary",
    "section": "19.8 H",
    "text": "19.8 H\n\nHealth Indicator (PH)\n\nMeasurable characteristic of population health. BA equivalent: KPI, metric.\n\nHealth Information Exchange (HIE) (PH)\n\nElectronic sharing of health data. BA equivalent: System integration, data exchange.\n\nHybrid Professional (Workforce)\n\nProfessional who operates fluently in both IT/BA and public health domains. May also be called Public Health Business Analyst (PH-BA) or Health Informatician. See also: Bridge Role.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#i",
    "href": "chapters/C-glossary.html#i",
    "title": "19  Glossary",
    "section": "19.9 I",
    "text": "19.9 I\n\nImpact (PH)\n\nLong-term effects of an intervention. BA equivalent: Business value, strategic outcomes.\n\nImplementation (Both)\n\nPutting a solution or intervention into practice. Similar usage.\n\nImplementation Climate (CFIR)\n\nOrganizational receptivity to change. BA equivalent: Organizational readiness.\n\nImplementation Science (PH)\n\nStudy of methods to promote adoption of evidence-based practices. BA application: Change management, adoption strategy.\n\nIndicator (PH)\n\nMeasurable variable reflecting status or change. BA equivalent: Metric, KPI.\n\nInner Setting (CFIR)\n\nInternal organizational context. BA equivalent: Organizational environment, culture.\n\nInput (PH Logic Model)\n\nResources invested in a program. BA equivalent: Resources, constraints.\n\nIntegration (BA)\n\nConnecting systems to work together. PH equivalent: Interoperability, HIE.\n\nInterest Holder (PH)\n\nPerson or group with interest in a program. BA equivalent: Stakeholder.\n\nIntervention (PH)\n\nAction taken to improve health. BA equivalent: Solution, system, process change.\n\nIteration (BA/Agile)\n\nFixed time period for development work. PH equivalent: PDSA cycle, program phase.\n\nIterative Development (Process Optimization)\n\nApproach to building systems incrementally, assessing priority functionality at each step rather than developing complete solutions before user feedback. Prevents months of work on systems that do not meet critical needs.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#k",
    "href": "chapters/C-glossary.html#k",
    "title": "19  Glossary",
    "section": "19.10 K",
    "text": "19.10 K\n\nKanban (Project Management)\n\nVisual workflow management method emphasizing continuous flow, work-in-progress (WIP) limits, and pull-based task assignment. Best for: Operations, continuous improvement, teams with unpredictable incoming work. Key practices: Visualize work, limit WIP, manage flow, make policies explicit.\n\nKey Informant (PH)\n\nPerson with specialized knowledge consulted for input. BA equivalent: Subject matter expert (SME).\n\nKPI (Key Performance Indicator) (BA)\n\nMetric measuring success. PH equivalent: Health indicator, outcome measure.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#j",
    "href": "chapters/C-glossary.html#j",
    "title": "19  Glossary",
    "section": "19.11 J",
    "text": "19.11 J\n\nJob Series (Federal Employment)\n\nNumerical classification system for federal positions. Key series for hybrid professionals include 0601 (General Health Science), 0685 (Public Health Program Specialist), and 2210 (IT Management).\n\nJob Demands-Resources Model (JD-R) (Psychology)\n\nModel explaining that job stress results from imbalance between demands (workload, time pressure, complexity) and resources (autonomy, support, feedback). Process optimization should reduce demands while maintaining or increasing resources.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#l",
    "href": "chapters/C-glossary.html#l",
    "title": "19  Glossary",
    "section": "19.12 L",
    "text": "19.12 L\n\nLeadership Competence (Organizational)\n\nAdequate domain understanding by leaders to guide priorities and decisions. Remedy: Pair with SMEs, establish decision review gates, and continuous domain briefings.\n\nLessons Learned (BA)\n\nKnowledge gained from experience. PH equivalent: After-action review findings.\n\nLine List (PH)\n\nTabular record of individual cases used for outbreak investigation, contact tracing, and epidemiological analysis. Contains one row per case with key variables (demographics, dates, outcomes). A Gold layer artifact because it serves operational purposes and derives from cleansed Silver layer data. BA equivalent: Operational report, case management export.\n\nLogic Model (PH)\n\nVisual representation of program theory (inputs → activities → outputs → outcomes). BA equivalent: Requirements traceability, value chain.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#m",
    "href": "chapters/C-glossary.html#m",
    "title": "19  Glossary",
    "section": "19.13 M",
    "text": "19.13 M\n\nMaintenance (RE-AIM)\n\nSustainability of an intervention over time. BA equivalent: Operational sustainability.\n\nMedallion Architecture (Data Architecture)\n\nData design pattern organizing data into three progressively refined layers: Bronze (raw), Silver (cleansed), and Gold (curated). Originated from Databricks circa 2020, building on traditional data warehousing concepts. PH application: Maps to surveillance data flow from ingestion through standardization to reporting.\n\nMilestone (Both)\n\nSignificant point in project timeline. In PH, often aligned with grant reporting.\n\nMVP (Minimum Viable Product) (BA/Agile)\n\nSmallest useful version of a solution. PH equivalent: Pilot intervention, proof of concept.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#n",
    "href": "chapters/C-glossary.html#n",
    "title": "19  Glossary",
    "section": "19.14 N",
    "text": "19.14 N\n\nNeeds Assessment (PH)\n\nSystematic identification of needs and gaps. BA equivalent: Business analysis, current state assessment.\n\nNFR (Non-Functional Requirement) (BA)\n\nQuality attribute or constraint. PH equivalent: Implementation characteristic (CFIR).\n\nNonviolent Communication (NVC) (Communication)\n\nCommunication approach focusing on observations, feelings, needs, and requests. Use: Reduces conflict and supports psychological safety.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#o",
    "href": "chapters/C-glossary.html#o",
    "title": "19  Glossary",
    "section": "19.15 O",
    "text": "19.15 O\n\nOutcome (PH)\n\nChange resulting from an intervention. BA equivalent: Benefit, value delivered.\n\nOnboarding Crosswalk (Workforce)\n\nStructured orientation materials that cover both BA and PH domains for new team members joining hybrid projects. Includes terminology glossaries, framework overviews, and project-specific mappings.\n\nOuter Setting (CFIR)\n\nExternal context (regulations, networks, peer pressure). BA equivalent: External environment, market forces.\n\nOutput (PH Logic Model)\n\nDirect products of program activities. BA equivalent: Deliverables, features.\n\nOrganizational Misalignment (Organizational)\n\nDisconnect between goals, priorities, or definitions across teams leading to duplication and conflict. Remedy: Cascading OKRs, shared roadmaps, single prioritized backlog.\n\nOrganizational Culture (Schein Model) (Organizational)\n\nEdgar Schein’s framework describing three levels of culture: (1) Artifacts (visible structures and processes), (2) Espoused Values (stated strategies and goals), and (3) Basic Assumptions (unconscious beliefs). Process optimization often fails when it addresses only artifacts while conflicting with basic assumptions.\n\nOKR (Objectives and Key Results) (Project Management)\n\nGoal-setting framework connecting high-level objectives to measurable key results. Structure: Objective (qualitative, inspirational) + 3-5 Key Results (quantitative, measurable). Use: Cascading OKRs align team goals to organizational strategy; helps prevent misalignment and ensures work connects to outcomes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#p",
    "href": "chapters/C-glossary.html#p",
    "title": "19  Glossary",
    "section": "19.16 P",
    "text": "19.16 P\n\nPDSA (Plan-Do-Study-Act) (PH)\n\nQuality improvement cycle. BA equivalent: Sprint/iteration, continuous improvement cycle.\n\nPHIFP (Public Health Informatics Fellowship Program) (Workforce)\n\nCDC fellowship program training professionals at the intersection of public health practice and information systems. Produces hybrid professionals who lead health IT initiatives across state and local health departments.\n\nPHIT (Public Health Informatics & Technology) Program (Workforce)\n\nONC-funded workforce development program training diverse professionals in health informatics. Emphasizes recruiting from underrepresented communities.\n\nPilot (Both)\n\nSmall-scale test of a solution or intervention. Similar usage.\n\nPersonality Pathology (Organizational Risk) (Organizational)\n\nConcerns about maladaptive interpersonal patterns impacting decisions and team function. Best practice: Avoid diagnosing or speculating on motives in workplace contexts; instead focus on observable behaviors, document conduct expectations, and use HR processes and governance mechanisms to protect staff and operations. See also: Disruptive Interpersonal Patterns.\n\nPolitical Factors / Political Interference (Organizational)\n\nFormal and informal power dynamics that influence decisions beyond evidence or merit. Can cause decision volatility, misdirected resources, and staff demoralization. Guardrails: Charter-based governance with documented authority, transparent decision criteria, decision logs accessible to stakeholders, conflict-of-interest declarations, and external review for high-stakes choices.\n\nProcess Evaluation (PH)\n\nAssessment of how an intervention was implemented. BA equivalent: Implementation review.\n\nProgram (PH)\n\nOrganized set of activities to achieve health objectives. BA equivalent: Solution, system, project.\n\nProtocol (PH)\n\nStandardized procedure or guideline. BA equivalent: Business rule, procedure specification.\n\nProcess Optimization (Both)\n\nSystematic approach to improving efficiency and effectiveness by eliminating unnecessary processes, automating mission-critical ones, and ensuring demonstrable value. Goal is maximizing impact with available resources.\n\nProcess Owner (Organizational)\n\nSingle point of accountability for a process. Responsible for performance, improvement, and governance of that process.\n\nPrototype (BA)\n\nEarly model for testing concepts. PH equivalent: Pilot, formative testing.\n\nPublic Trust (Federal Employment)\n\nSecurity clearance level commonly required for federal contractor positions in health IT. Lower than Secret/Top Secret but still requires background investigation.\n\nPsychological Safety (Organizational)\n\nEnvironment where team members feel safe to take risks, ask questions, and admit mistakes without fear of punishment. Essential for sustainable process improvement and innovation. Research by Amy Edmondson demonstrates that psychologically safe teams perform better.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#q",
    "href": "chapters/C-glossary.html#q",
    "title": "19  Glossary",
    "section": "19.17 Q",
    "text": "19.17 Q\n\nQuality Assurance (QA) (BA)\n\nSystematic quality activities. PH equivalent: Quality Improvement (QI).\n\nQuality Improvement (QI) (PH)\n\nContinuous improvement of processes and outcomes. BA equivalent: Continuous improvement, QA.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#r",
    "href": "chapters/C-glossary.html#r",
    "title": "19  Glossary",
    "section": "19.18 R",
    "text": "19.18 R\n\nRE-AIM (PH)\n\nFramework for evaluating public health impact (Reach, Effectiveness, Adoption, Implementation, Maintenance).\n\nRACI Matrix (Project Management)\n\nFramework defining roles for each activity: Responsible (does the work), Accountable (ultimately answerable, one person only), Consulted (provides input), Informed (kept updated). Prevents gaps and overlaps in responsibility.\n\nReach (RE-AIM)\n\nProportion of target population participating. BA equivalent: Adoption rate, market penetration.\n\nReadiness (Both)\n\nPreparedness for change. Implementation readiness (PH) or organizational readiness (BA).\n\nRed Teaming (Organizational)\n\nStructured practice of challenging assumptions by assigning a team to critique plans and surface risks. Use: Improves decisions by integrating opposing perspectives.\n\nRelative Advantage (CFIR)\n\nPerception that intervention is better than current practice. BA equivalent: Value proposition.\n\nRequest for Proposal (RFP) (Government Procurement)\n\nFormal solicitation document describing a government agency’s needs and inviting vendors to bid. For job seekers, RFPs reveal upcoming demand for specific skills before positions are posted. See also: Award Notice.\n\nRequirements (BA)\n\nConditions a solution must satisfy. PH equivalent: Program specifications, protocols.\n\nRetrospective (BA/Agile)\n\nMeeting to reflect on past work. PH equivalent: After-action review.\n\nRights Holder (PH)\n\nPerson with inherent claims (alternative to “stakeholder”). BA equivalent: Stakeholder (with different connotation).\n\nRisk (Both)\n\nPotential for adverse outcomes. Similar usage; PH may emphasize community risk.\n\nROI (Return on Investment) (BA)\n\nFinancial value relative to cost. PH equivalent: Cost-effectiveness, cost-benefit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#s",
    "href": "chapters/C-glossary.html#s",
    "title": "19  Glossary",
    "section": "19.19 S",
    "text": "19.19 S\n\nScope (Both)\n\nBoundaries of what is included. Similar usage in both domains.\n\nScope Creep (BA)\n\nUncontrolled expansion or shifting of scope that leads to churn and rework. PH equivalent: Program drift. Controls: Change control process, backlog refinement, decision logs, and re-baselining.\n\nService-User Scenario (PH-adapted)\n\nNarrative description of user journey. BA equivalent: User story, use case narrative.\n\nSBAR (Communication) (Both)\n\nStructured communication framework: Situation, Background, Assessment, Recommendation. Improves clarity and reduces misunderstandings in technical and clinical contexts.\n\nSelf-Determination Theory (SDT) (Psychology)\n\nTheory identifying three innate psychological needs that drive motivation: autonomy (control over one’s work), competence (mastery and effectiveness), and relatedness (connection to others). Process design should support all three.\n\nShared Services (Organizational)\n\nModel where core functions (data cleaning, reporting, training development, compliance) are centralized to serve multiple teams. Reduces duplication and builds specialized expertise.\n\nSLA / OLA (Service Level Agreement / Operating Level Agreement) (Organizational)\n\nDocumented commitments defining expected performance levels. SLA: Agreement with external customers or stakeholders (e.g., 99.9% uptime, 48-hour response). OLA: Internal agreement between teams supporting the SLA (e.g., IT commits to 4-hour escalation response to data team). Establishes accountability and enables governance.\n\nSilver Layer (Data Architecture)\n\nThe middle layer in medallion architecture containing cleansed, validated, and standardized data. Transformations include deduplication, format normalization, and quality checks. PH equivalent: Harmonization layer, FHIR/OMOP standardized datasets, de-identified data.\n\nSME (Subject Matter Expert) (BA)\n\nPerson with domain expertise. PH equivalent: Key informant, clinical expert.\n\nSolution (BA)\n\nSystem or process addressing a business need. PH equivalent: Intervention, program.\n\nSprint (BA/Agile)\n\nFixed-length iteration. PH equivalent: PDSA cycle, work period.\n\nStakeholder (BA)\n\nPerson with interest in a project. PH alternatives: Interest holder, community partner, rights holder.\n\nStructured Dissent (Organizational)\n\nFormal inclusion of opposing views in decision-making through methods like devil’s advocate roles, red teaming, and dissent summaries.\n\nSummative Evaluation (PH)\n\nAssessment of overall outcomes after implementation. BA equivalent: Post-implementation review.\n\nSustainability (PH)\n\nAbility to maintain intervention over time. BA equivalent: Operational viability.\n\nSuper-User (Workforce)\n\nStaff member who becomes the informal technology expert for their team. Often the best candidates for formal hybrid roles because they possess deep domain context that is difficult to teach.\n\nSystems Integrator (Federal Contracting)\n\nLarge consulting firm that executes major government IT contracts (e.g., Deloitte, Accenture, Booz Allen Hamilton, Leidos). Primary employers for hybrid professionals working on federal health IT modernization.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#t",
    "href": "chapters/C-glossary.html#t",
    "title": "19  Glossary",
    "section": "19.20 T",
    "text": "19.20 T\n\nTest Case (BA)\n\nSpecification for verifying a requirement. PH equivalent: Evaluation measure, data collection protocol.\n\nTraceability (BA)\n\nLinking requirements to sources and tests. PH equivalent: Theory of change alignment.\n\nTerminology Fluency (Workforce)\n\nAbility to speak both BA and PH languages without constant translation pauses. A core competency for hybrid professionals.\n\nTraining (Both)\n\nBuilding user/staff capability. Similar usage; PH may use “capacity building.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#u",
    "href": "chapters/C-glossary.html#u",
    "title": "19  Glossary",
    "section": "19.21 U",
    "text": "19.21 U\n\nUAT (User Acceptance Testing) (BA)\n\nStakeholder verification of solution. PH equivalent: Pilot evaluation, field testing.\n\nUse Case (BA)\n\nDescription of system-user interaction. PH equivalent: Clinical scenario, patient journey.\n\nUnicorn Job Description (Workforce)\n\nJob posting with unrealistic requirements spanning multiple specialties (e.g., PhD, PMP, 10 years Python, and nursing license). Usually signals role clarity issues; candidates meeting 50 to 60 percent of criteria should still apply.\n\nUser Story (BA/Agile)\n\nBrief requirement from user perspective. PH equivalents: Service-user scenario, GPS format.\n\nUnhealthy Competition (Organizational)\n\nInternal rivalry that undermines collaboration, causes information hoarding, and leads to duplicated effort. Remedy: Establish shared goals that require collaboration; use team-based (not individual) incentives; conduct cross-team reviews to surface dependencies and promote transparency.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#v",
    "href": "chapters/C-glossary.html#v",
    "title": "19  Glossary",
    "section": "19.22 V",
    "text": "19.22 V\n\nValidation (Both)\n\nConfirming the right thing is built. Similar usage.\n\nVariance (PH)\n\nDeviation from expected outcome. BA equivalent: Defect, exception.\n\nVerification (Both)\n\nConfirming the thing is built right. Similar usage.\n\nVisual Management (Lean/PM)\n\nUsing visible displays (Kanban boards, dashboards, status indicators) to make work status, blockers, and progress immediately apparent to all team members. Reduces need for status meetings and supports self-organization.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#w",
    "href": "chapters/C-glossary.html#w",
    "title": "19  Glossary",
    "section": "19.23 W",
    "text": "19.23 W\n\nWorkflow (Both)\n\nSequence of tasks to accomplish work. Similar usage.\n\nWorkplan (PH)\n\nDetailed plan of activities. BA equivalent: Project plan, backlog.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#quick-reference-most-common-translations",
    "href": "chapters/C-glossary.html#quick-reference-most-common-translations",
    "title": "19  Glossary",
    "section": "19.24 Quick Reference: Most Common Translations",
    "text": "19.24 Quick Reference: Most Common Translations\n\n\n\n\n\n\n\n\nWhen You Hear…\nBA Meaning\nPH Meaning\n\n\n\n\n“Requirements”\nSystem specifications\nProgram protocols\n\n\n“Stakeholder”\nAnyone with interest\nCommunity partner, rights holder\n\n\n“Sprint”\n2-week development cycle\nPDSA cycle\n\n\n“User story”\nFeature description\nService-user scenario\n\n\n“KPI”\nBusiness metric\nHealth indicator\n\n\n“MVP”\nMinimal product\nPilot intervention\n\n\n“Go-live”\nSystem deployment\nProgram launch\n\n\n“Bug”\nSoftware defect\nProtocol variance\n\n\n“Bronze layer”\nRaw data ingestion\nSource data, ingestion\n\n\n“Silver layer”\nCleansed/validated data\nStandardized, harmonized data\n\n\n“Gold layer”\nAnalytics-ready data\nReporting datasets, CDC submissions\n\n\n“Data lakehouse”\nUnified data platform\nIntegrated surveillance data repository\n\n\n“OKR”\nObjectives and Key Results\nLogic model alignment, grant objectives\n\n\n“Kanban”\nVisual workflow board\nTask tracking, workplan visualization\n\n\n“SLA”\nService Level Agreement\nPerformance commitment, response time target\n\n\n“Scope creep”\nUncontrolled scope expansion\nProgram drift",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "20  References",
    "section": "",
    "text": "References\n\n\n1. Iiba. Babok: A Guide\nto the Business Analysis Body of\nKnowledge. International Institute of Business Analysis;\n2015. \n\n\n2. CDC.\nCDC Program Evaluation\nFramework [Internet]. CDC Approach to Program Evaluation.\n2025 [cited 2026 Jan 9]. Available from: https://www.cdc.gov/evaluation/php/evaluation-framework/index.html\n\n\n3. Kidder DP. CDC\nProgram Evaluation Framework,\n2024. MMWR Recommendations and Reports [Internet]. 2024 [cited 2026 Jan\n9];73. Available from: https://www.cdc.gov/mmwr/volumes/73/rr/rr7306a1.htm\n\n\n4. 1999\n– RE-AIM [Internet]. [cited 2026 Jan 9].\nAvailable from: https://re-aim.org/1999/\n\n\n5. RE-AIM –\nHome – Reach Effectiveness\nAdoption Implementation\nMaintenance [Internet]. [cited 2026 Jan 9]. Available from:\nhttps://re-aim.org/",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>References</span>"
    ]
  }
]