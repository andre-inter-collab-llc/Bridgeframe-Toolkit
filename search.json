[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bridgeframe",
    "section": "",
    "text": "Welcome\nBridgeframe is a practical toolkit for professionals who work at the intersection of information technology and public health. Whether you are a business analyst stepping into your first health department project, or a public health professional learning to collaborate with software teams, this book provides the translation layer you need.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#the-challenge",
    "href": "index.html#the-challenge",
    "title": "Bridgeframe",
    "section": "0.1 The Challenge",
    "text": "0.1 The Challenge\nTwo disciplines. Two languages. One shared goal: building systems that improve health outcomes.\nBusiness analysts speak of user stories, sprints, and requirements traceability. Public health professionals speak of logic models, PDSA cycles, and program evaluation. Both are trying to define problems, design solutions, and measure success, yet their terminology creates friction rather than collaboration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#the-solution",
    "href": "index.html#the-solution",
    "title": "Bridgeframe",
    "section": "0.2 The Solution",
    "text": "0.2 The Solution\nLogic Model Components Mapped to Requirements Bridgeframe provides:\n\nA terminology dictionary mapping IT/Agile concepts to their public health equivalents\nPhase-by-phase guidance aligning the BABOK lifecycle with CDC frameworks\nA running case study (CancerSurv) demonstrating concepts in practice\nTemplates and tools for hybrid teams",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Bridgeframe",
    "section": "0.3 Who This Book Is For",
    "text": "0.3 Who This Book Is For\n\nIT Business Analysts entering the public health sector\nPublic Health Informaticians collaborating with software vendors\nProject Managers overseeing health IT implementations\nData Scientists working with epidemiological systems\nStudents in health informatics or public health programs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Bridgeframe",
    "section": "0.4 How to Use This Book",
    "text": "0.4 How to Use This Book\nThis book is organized into three parts:\n\nFoundations: Core concepts, terminology mapping, and the CancerSurv case study\nThe Analysis Process: Phase-by-phase guidance from planning through evaluation\nPutting It Into Practice: Tools comparison and implementation science frameworks\n\nEach chapter includes CancerSurv examples in callout boxes, making abstract concepts concrete.\n\n\n\n\n\n\nTipGetting Started\n\n\n\nIf you are new to this intersection, start with Chapter 1: Introduction and the Terminology Dictionary. If you are already working on a project, jump to the relevant phase chapter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Bridgeframe",
    "section": "0.5 About the Author",
    "text": "0.5 About the Author\nAndré van Zyl, MPH is an epidemiologist and data science professional with close to two decades of experience spanning public health, health informatics, and technical system development. His career has taken him across local, state, federal, tribal, and international health systems, from helping establish global surveillance systems at the CDC to implementing health interventions in resource-constrained settings.\nAs a Health Scientist at CDC’s National Center for Emerging and Zoonotic Infectious Diseases, André led data acquisition and reporting systems for global antimicrobial resistance surveillance spanning multiple continents. He pioneered the integration of artificial intelligence into public health workflows and modernized data processes using R, Python, Azure Databricks, and platforms like REDCap and DHIS2. A consistent theme throughout his work has been technical translation: bridging communication gaps between laboratory scientists, clinical teams, and technical developers.\nAndré holds a Master of Public Health from the University of Pretoria and a BA Honors in Psychology from Nelson Mandela University. This combination of behavioral science and technical expertise enables him to understand both user needs and system requirements when implementing solutions across diverse communities.\nHe is the founder of Intersect Collaborations LLC, a consultancy helping public health organizations transform data systems and analytics capabilities for improved decision-making and community health outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bridgeframe</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "2.1 The Translation Challenge\nThroughout my career, I’ve watched brilliant teams stumble, not because they lacked expertise, but because they were speaking entirely different professional languages.\nMy formal training is in social sciences and public health, where business analysis concepts weren’t part of the curriculum. But my career path led me through various technical roles (working for technology companies and alongside them) where I absorbed the terminology, tools, software, and processes that business analysts and project managers use daily. This dual exposure showed me both the value and the difficulty of blending these perspectives.\nWhen I joined projects bridging IT and public health, I saw the same friction points repeatedly:\nThe IT world optimizes for efficiency and profit. Public health optimizes for equity and outcomes. Both are rigorous disciplines with systematic frameworks. Yet when they collaborate, the translation gap creates delays, misaligned expectations, and sometimes outright project failure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#the-translation-challenge",
    "href": "preface.html#the-translation-challenge",
    "title": "2  Preface",
    "section": "",
    "text": "Business analysts asking for “user stories” while epidemiologists stared blankly\nDevelopers requesting “non-functional requirements” while program managers wondered if that related to grant compliance\nAgile sprints clashing with PDSA cycles or action research, even though both are iterative improvement frameworks\nStakeholder maps that missed community partners because they weren’t “decision-makers” in the traditional IT sense",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#lessons-from-both-sides",
    "href": "preface.html#lessons-from-both-sides",
    "title": "2  Preface",
    "section": "2.2 Lessons from Both Sides",
    "text": "2.2 Lessons from Both Sides\nI’ve experienced this challenge from multiple angles:\nIntroducing IT tools to public health teams. I’ve worked with groups who had strong public health backgrounds and wanted to streamline collaboration, tracking, and communication. I introduced project management software (including Azure DevOps, because it was the best tool available to us). But I ran into constant friction translating the software’s terminology (sprints, agile, user stories, backlogs) into concepts that made sense for public health professionals and their workflows.\nWatching tech-dominated engagements produce suboptimal products. I’ve also seen the reverse: public health organizations relying heavily on technology vendors where the frameworks, project management styles, and terminology of business analysts dominated the engagement. The public health perspective got lost, and the resulting products didn’t fit how health departments actually operate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#finding-common-ground",
    "href": "preface.html#finding-common-ground",
    "title": "2  Preface",
    "section": "2.3 Finding Common Ground",
    "text": "2.3 Finding Common Ground\nBridgeframe exists because I needed it. Every time I translated a clinical workflow into a requirements document, or explained why a “user story” doesn’t capture a patient journey, I wished for a reference guide that mapped these concepts clearly.\nThis toolkit distills what I’ve learned from CDC surveillance systems, COVID-19 contact tracing implementations, tuberculosis intervention trials, and countless cross-functional team meetings. It’s designed for the health informatician sitting between two worlds: ensuring the IT team’s technical specifications align with the health department’s programmatic goals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#a-work-in-progress",
    "href": "preface.html#a-work-in-progress",
    "title": "2  Preface",
    "section": "2.4 A Work in Progress",
    "text": "2.4 A Work in Progress\nI want to be clear: I am not positioning myself as an expert on business analysis, and Bridgeframe is not intended as an authoritative guide. This is a draft that I will continue to develop and refine.\nThe examples throughout this book (including the CancerSurv case study) are illustrative and fictitious. They are designed to be relatable, to help professionals in both domains recognize common patterns and challenges. The intent is educational and informative.\nMy hope is that Bridgeframe stimulates discussion and points readers toward the established frameworks and resources that offer deeper expertise: BABOK for business analysis, CDC evaluation frameworks for public health, CFIR for implementation science, and the many other rigorous methodologies developed by true experts in these fields.\nIf this toolkit helps even a few teams find common ground, or sparks conversations that improve how we build health information systems, it will have served its purpose. I welcome feedback, corrections, and contributions as this resource evolves.\nAndré van Zyl\nIntersect Collaborations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 Why Bridgeframe Exists\nPublic health and information technology are increasingly intertwined. Disease surveillance systems, immunization registries, electronic lab reporting, and health analytics platforms all require collaboration between two groups that often struggle to understand each other: business analysts and public health professionals.\nThis chapter explains the problem, introduces the Bridgeframe approach, and sets the stage for the detailed guidance that follows.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-bridgeframe-exists",
    "href": "chapters/01-introduction.html#why-bridgeframe-exists",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1.1 The Silo Problem\nConsider a typical scenario: A state health department receives funding to modernize its disease surveillance system. They contract with a software vendor whose project team includes experienced business analysts, developers, and testers. The health department brings epidemiologists, program managers, and data analysts. Both sides are competent in their respective domains. Yet within weeks, the project is mired in confusion.\nThe vendor’s BA asks for “user stories.” The epidemiologist provides a detailed case definition document. The BA politely explains that a case definition is not a user story. The epidemiologist wonders why the BA keeps asking about “acceptance criteria” when the CDC already publishes validation rules.\nThis is not a failure of intelligence or good faith. It is a failure of translation.\n\n\n3.1.2 Two Parallel Worlds\nBusiness analysis, as codified in the BABOK (Business Analysis Body of Knowledge), provides a rigorous framework for understanding needs, defining requirements, and ensuring solutions deliver value1. Public health analysis, guided by CDC frameworks and epidemiological methods, provides an equally rigorous approach to assessing community needs, designing interventions, and evaluating outcomes2,3.\nThese frameworks are parallel, not incompatible:\n\n\n\nBusiness Analysis\nPublic Health\n\n\n\n\nStakeholder Analysis\nCommunity Partner Mapping\n\n\nCurrent State Analysis\nEpidemiological Baseline\n\n\nRequirements Specification\nProgram Protocol\n\n\nSolution Design\nIntervention Design\n\n\nImplementation\nProgram Rollout\n\n\nEvaluation\nProgram Evaluation\n\n\n\nThe concepts align; the terminology diverges. Bridgeframe provides the translation layer.\n\n\n3.1.3 The Cost of Miscommunication\nWhen BA and PH professionals cannot communicate effectively, projects suffer:\n**Inability to prove impact**: Systems that cannot demonstrate public health value\nMore importantly, health outcomes suffer. A delayed surveillance system means delayed outbreak response. A poorly designed immunization registry means children missing vaccines. And increasingly, programs that cannot demonstrate measurable public health impact risk losing funding.\nBoth technical and public health teams share responsibility for proving the value of their work. Technology investments must translate to demonstrable improvements in population health, not just features delivered on time and on budget.\n\n\n3.1.4 The Bridgeframe Solution\nBridgeframe offers a structured approach to bridging these domains:\n\nCommon Vocabulary: Chapter 3 provides a comprehensive dictionary mapping BA terms to PH equivalents\nPhase Alignment: Chapters 4-9 walk through each lifecycle phase, showing how BA activities map to PH frameworks\nPractical Examples: The CancerSurv case study (Chapter 2) demonstrates concepts in action\nImplementation Science: Chapter 11 introduces CFIR and other frameworks for addressing adoption barriers\n\n\n\n3.1.5 Start with the Client’s Framework\nBefore diving into requirements gathering or sprint planning, a critical first question must be answered: What frameworks does the client already use?\nPublic health organizations operate within established methodological traditions. They may follow CDC evaluation frameworks, use logic models for program planning, apply CFIR for implementation readiness, or adhere to agency-specific protocols. These are not obstacles to be worked around; they are assets to be leveraged.\nWhen engaging with a public health client:\n\nAsk early: In the first meetings, explicitly ask what frameworks, methodologies, or standards guide their work\nPrioritize existing frameworks: If the client has established approaches, adapt your deliverables to align with them rather than imposing unfamiliar structures\nTranslate, do not replace: When BA frameworks offer value the client lacks, introduce them by mapping to familiar concepts. Present a “logic model with software requirements” rather than demanding they learn BABOK terminology\nDocument the bridge: Create explicit crosswalks showing how your deliverables map to their frameworks\n\n\n\n\n\n\n\nTipPractical Guidance\n\n\n\nIf you are the business analyst or developer, your role is translation, not conversion. A requirements document that aligns with the client’s existing CDC program evaluation framework will gain faster buy-in than one that requires the client to learn Agile terminology. Meet them where they are.\n\n\nThis principle works both ways. Public health professionals engaging with IT vendors should communicate their frameworks early, providing documentation and context so the technical team can adapt their approach accordingly.\n\n\n3.1.6 When to Introduce the Translation\nA common pattern across public health IT projects: translation challenges emerge during software requirements gathering, by which point significant momentum (and sometimes conflict) has already built up. The teams discover they are speaking different languages only after the project is well underway.\nThe earlier translation happens, the better the outcome.\nIdeally, translation should begin during the business case development that justifies funding. When writing grant applications or funding proposals, explicitly address:\n\nHow IT terminology maps to programmatic outcomes\nWhat resources (training, facilitation, documentation) will be needed for cross-domain communication\nWho will serve as translators or bridges between teams\n\nThis early attention accomplishes two things: it surfaces potential friction points before they become costly, and it ensures that resource discovery includes the human and process elements needed for successful collaboration, not just technical requirements.\n\n\n\n\n\n\nTipPractical Guidance\n\n\n\nIf you are writing a grant proposal or business case for a health IT project, include a line item for “cross-domain facilitation” or “terminology alignment workshops.” This signals awareness of the challenge and secures resources to address it.\n\n\nWhen translation training happens before or alongside business case development, teams enter requirements gathering with shared vocabulary and mutual understanding. When it happens only during software requirements definition, teams must untangle miscommunication while simultaneously trying to make progress.\n\n\n3.1.7 Who Should Use This Book\nThis book serves multiple audiences:\nFor IT Business Analysts entering public health:\n\nLearn the regulatory context (HIPAA, CDC reporting requirements)\nUnderstand grant-driven funding and its implications\nAdapt Agile practices for public health workflows\n\nFor Public Health Professionals working with IT teams:\n\nTranslate your needs into requirements language\nParticipate effectively in sprint planning and reviews\nEvaluate vendor proposals with confidence\n\nFor Project Managers and Leaders:\n\nBuild teams with shared vocabulary\nAnticipate common friction points\nStructure projects for hybrid success\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nThroughout this book, we follow the CancerSurv project: a state health department modernizing its cancer registry system. In the Introduction, the challenge is framed as both a Business Need (replace aging mainframe with modern cloud platform) and a Public Health Challenge (ensure timely, complete cancer data for prevention planning and disparity identification).\n\n\n\n\n3.1.8 What Bridgeframe Is Not\nThis book does not replace domain-specific training. Business analysts should still study the BABOK and pursue relevant certifications. Public health professionals should still learn epidemiology, biostatistics, and program evaluation. Bridgeframe provides the bridge between these bodies of knowledge, not a substitute for them.\nSimilarly, this book does not cover software development practices, database design, or clinical medicine in depth. It focuses specifically on the analysis phase: understanding needs, defining requirements, and evaluating solutions.\n\n\n3.1.9 Moving Forward\nThe following chapter introduces CancerSurv in detail, providing the context you will need to engage with examples throughout the book. Chapter 3 then provides the terminology dictionary, a reference you will return to often. From there, we proceed phase by phase through the analysis lifecycle.\nWelcome to Bridgeframe. Let us build this bridge together.\n\n\n\n\n\n\n1. Iiba. Babok: A Guide to the Business Analysis Body of Knowledge. International Institute of Business Analysis; 2015. \n\n\n2. CDC. CDC Program Evaluation Framework [Internet]. CDC Approach to Program Evaluation. 2025 [cited 2026 Jan 9]. Available from: https://www.cdc.gov/evaluation/php/evaluation-framework/index.html\n\n\n3. Kidder DP. CDC Program Evaluation Framework, 2024. MMWR Recommendations and Reports [Internet]. 2024 [cited 2026 Jan 9];73. Available from: https://www.cdc.gov/mmwr/volumes/73/rr/rr7306a1.htm",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-case-study.html",
    "href": "chapters/02-case-study.html",
    "title": "4  Meet CancerSurv",
    "section": "",
    "text": "4.1 The CancerSurv Case Study\nThroughout this book, we use a single comprehensive case study to illustrate concepts: CancerSurv, a cloud-based cancer surveillance and registry system. This chapter introduces the project context, stakeholders, and objectives that will appear in examples across all subsequent chapters.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meet CancerSurv</span>"
    ]
  },
  {
    "objectID": "chapters/02-case-study.html#the-cancersurv-case-study",
    "href": "chapters/02-case-study.html#the-cancersurv-case-study",
    "title": "4  Meet CancerSurv",
    "section": "",
    "text": "4.1.1 Project Overview\nA state public health department partners with TechHealth Solutions, a health IT company, to develop CancerSurv: a modern, cloud-based platform for cancer surveillance and registry operations.\n\n4.1.1.1 The Business Context\n\n\n\n\n\n\n\nAttribute\nDetail\n\n\n\n\nPublic Health Partner\nState Cancer Registry (Department of Health)\n\n\nTechnology Partner\nTechHealth Solutions (cloud software vendor)\n\n\nFunding Source\nCDC National Program of Cancer Registries (NPCR) grant\n\n\nTimeline\n18-month phased implementation\n\n\nScope\nReplace legacy mainframe system with modern cloud solution\n\n\n\n\n\n4.1.1.2 The Public Health Context\nCancer registries serve a critical public health function. They collect, process, and analyze data on cancer incidence, treatment, and outcomes. This data informs:\n\nPrevention program targeting\nEarly detection initiatives\nHealth disparity identification\nResearch and clinical trials\nHealthcare resource planning\n\nThe state’s current system, built on 1990s mainframe technology, cannot meet modern demands for interoperability, real-time analytics, and remote access.\n\n\n\n4.1.2 Stakeholder Landscape\nUnderstanding who participates in this project requires seeing stakeholders through both BA and PH lenses:\n\n\n\n\n\n\n\n\n\nRole\nBA Term\nPH Term\nKey Concerns\n\n\n\n\nProject Sponsor\nExecutive Sponsor\nRegistry Director / State Epidemiologist\nBudget, timeline, CDC compliance\n\n\nEnd Users\nSystem Users\nCancer Registrars, Epidemiologists, Data Analysts\nUsability, efficiency, data quality\n\n\nSubject Matter Experts\nBusiness SMEs\nOncologists, Pathologists, Tumor Board Members\nClinical accuracy, coding standards\n\n\nExternal Partners\nVendors / Integrators\nHospitals, Laboratories, Vital Records\nData submission, interoperability\n\n\nOversight Bodies\nGovernance Board\nNPCR Program, NAACCR Standards Committee\nStandards compliance, data quality metrics\n\n\n\n\n\n\n\n\n\nImportantTerminology Note\n\n\n\nWhile we use “stakeholder” here for its familiarity in BA contexts, public health practitioners often prefer alternatives: community partners, interest holders, rights holders, or beneficiaries. The choice of term signals values: “stakeholder” implies an interest or stake, while “rights holder” acknowledges inherent claims and dignity. See Chapter 3 for detailed discussion.\n\n\n\n\n4.1.3 System Functions\nCancerSurv must support five core functional areas:\n\n4.1.3.1 Case Abstraction\nCancer registrars enter and code cancer cases from medical records. This involves:\n\nExtracting relevant data from pathology reports, discharge summaries, and treatment records\nCoding diagnoses using ICD-O-3 (International Classification of Diseases for Oncology)\nStaging tumors using TNM and SEER summary staging\nLinking cases to patients across multiple treatment facilities\n\n\n\n4.1.3.2 Data Quality\nAutomated processes ensure data completeness and accuracy:\n\nEdit checks flagging inconsistent or missing data\nDuplicate detection identifying potential duplicate case records\nLinkage to vital records for death clearance\nInter-rater reliability monitoring\n\n\n\n4.1.3.3 Reporting\nThe registry must meet external reporting requirements:\n\nAnnual submissions to NPCR (National Program of Cancer Registries)\nData exchange with SEER (Surveillance, Epidemiology, and End Results Program)\nAd-hoc queries for researchers (with appropriate IRB approval)\nPublic health reports for state legislature and media\n\n\n\n4.1.3.4 Analytics Dashboard\nModern surveillance requires real-time analytics:\n\nCancer incidence trends by site, stage, and demographics\nGeographic mapping of cancer clusters\nSurvival analysis and outcomes tracking\nHealth disparity indicators\n\n\n\n4.1.3.5 Interoperability\nCancerSurv must integrate with external systems:\n\nHL7 FHIR APIs for hospital EHR integration\nElectronic pathology reporting from laboratories\nVital records linkage for death data\nNational data exchange protocols\n\n\n\n\n4.1.4 Project Phases\nThe 18-month implementation follows a phased approach, which we will revisit throughout the book:\n\n\n\n\n\n\ngantt\n    title CancerSurv Implementation Timeline\n    dateFormat  YYYY-MM\n    section Planning\n    Needs Assessment       :2025-01, 2M\n    Requirements Gathering :2025-02, 3M\n    section Development\n    Core Platform          :2025-04, 4M\n    Interoperability       :2025-07, 3M\n    Analytics              :2025-09, 2M\n    section Deployment\n    Pilot Testing          :2025-10, 2M\n    Training & Rollout     :2025-11, 3M\n    section Evaluation\n    Post-Implementation    :2026-01, 3M\n\n\n\n\nFigure 4.1: CancerSurv Implementation Phases\n\n\n\n\n\n\n\n4.1.5 How CancerSurv Appears in This Book\nEach chapter includes CancerSurv examples demonstrating concepts in practice:\n\nPlanning (Chapter 4): Needs assessment comparing cancer data gaps with CDC reporting requirements\nElicitation (Chapter 5): User stories from registrars; clinical guidelines from oncologists\nRequirements (Chapter 6): Functional specifications for case entry; NFRs for HIPAA compliance\nDesign (Chapter 7): System architecture; CFIR implementation readiness assessment\nImplementation (Chapter 8): Agile sprints mapped to grant milestones; PDSA cycles for workflow adoption\nEvaluation (Chapter 9): KPIs (data completeness ≥95%) mapped to health outcomes\n\n\n\n\n\n\n\nTipUsing the Case Study\n\n\n\nWhen reading subsequent chapters, refer back to this overview to ground abstract concepts in the CancerSurv context. The case study makes the BA-PH bridge tangible.\n\n\n\n\n4.1.6 The Dual Mandate\nCancerSurv illustrates the fundamental tension bridged by this book. The project must simultaneously satisfy:\nTechnology Requirements:\n\nModern cloud architecture\nAPI-first design\nAgile delivery methodology\nVendor best practices\n\nPublic Health Requirements:\n\nCDC/NPCR compliance\nNAACCR data standards\nHIPAA security\nHealth equity focus\n\nSuccess requires translation between these worldviews. That is what Bridgeframe provides.\n\n\n4.1.7 Shared Responsibility: Proving Public Health Value\nBoth teams in the CancerSurv project (TechHealth Solutions’ business analysts and the State Cancer Registry’s public health professionals) share a critical responsibility: proving the public health value and impact of the solution they develop.\nThis is not merely about meeting technical specifications. Both teams must prove the public health value of their work through measurable outcomes and return on investment. In an era of constrained budgets, demonstrating tangible health impact has become essential for program survival.\nChapter 4 explores how to establish these success metrics during the Planning phase, ensuring that both technical deliverables and programmatic outcomes are measured from the start.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Meet CancerSurv</span>"
    ]
  },
  {
    "objectID": "chapters/03-terminology.html",
    "href": "chapters/03-terminology.html",
    "title": "5  Terminology Dictionary",
    "section": "",
    "text": "5.1 The BA-PH Translation Guide\nThis chapter provides a comprehensive mapping between Business Analysis (BA) terminology and Public Health (PH) equivalents. Use this as a reference throughout your work on hybrid projects.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Terminology Dictionary</span>"
    ]
  },
  {
    "objectID": "chapters/03-terminology.html#the-ba-ph-translation-guide",
    "href": "chapters/03-terminology.html#the-ba-ph-translation-guide",
    "title": "5  Terminology Dictionary",
    "section": "",
    "text": "5.1.1 Core Process Mapping\nThe BA lifecycle aligns with public health program phases:\n\n\n\n\n\n\n\n\nBA / BABOK Phase\nPublic Health Equivalent\nKey Activities\n\n\n\n\nStrategy Analysis\nCommunity Health Assessment\nDefine the problem using epidemiological data vs business metrics\n\n\nRequirements Analysis\nData Analysis & Logic Models\nModel processes, define indicators\n\n\nSolution Evaluation\nProgram Evaluation (CDC Framework)\nMeasure outcomes against targets\n\n\nChange Management\nImplementation Science\nCFIR, RE-AIM for adoption barriers\n\n\n\n\n\n5.1.2 The Complete BA-PH Process Workflow\nThe following diagram illustrates the six-phase analysis process covered in Part II of this book, showing how each BA phase maps to its Public Health equivalent. Note the iterative feedback loop from evaluation back to planning, reflecting the continuous improvement philosophy shared by both domains.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#2494f7', 'primaryTextColor': '#ffffff', 'primaryBorderColor': '#01272f', 'lineColor': '#777777', 'secondaryColor': '#00a4bb', 'tertiaryColor': '#01272f', 'edgeLabelBackground': '#ffffff'}}}%%\nflowchart TB\n    P1[\"&lt;b&gt;Ch 6: Planning & Strategy (BA) | Needs Assessment (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: business need, current state, future state, stakeholder identification&lt;br/&gt;PH Analog: public health challenge, epidemiological baseline, program goals&lt;br/&gt;Frameworks: SMART, RACI, SIPOC, 5 Whys, Fishbone&lt;br/&gt;Artifacts: business case, needs assessment, stakeholder map, current-state analysis\"]\n\n    P2[\"&lt;b&gt;Ch 7: Elicitation (BA) | Stakeholder Engagement (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: interviews, workshops, document analysis, observation, prototyping&lt;br/&gt;PH Analog: key informant interviews, focus groups, community engagement&lt;br/&gt;Frameworks: facilitation techniques, participatory research methods&lt;br/&gt;Artifacts: interview notes, workshop outputs, elicitation findings\"]\n\n    P3[\"&lt;b&gt;Ch 8: Requirements Analysis (BA) | Data Analysis & Logic Models (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: functional requirements, NFRs, data requirements, business rules&lt;br/&gt;PH Analog: program activities, implementation characteristics, case definitions&lt;br/&gt;Frameworks: MoSCoW, INVEST, UML, ERD; Logic Model, Theory of Change&lt;br/&gt;Artifacts: requirements specification, data dictionary, logic model, RTM\"]\n\n    P4[\"&lt;b&gt;Ch 9: Design & Solution Definition (BA) | Intervention Design (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: solution architecture, interface design, integration design&lt;br/&gt;PH Analog: intervention framework, program design, service delivery model&lt;br/&gt;Frameworks: UML, CFIR implementation planning&lt;br/&gt;Artifacts: architecture diagrams, prototypes, implementation strategy\"]\n\n    P5[\"&lt;b&gt;Ch 10: Implementation (BA) | Program Execution (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: sprint, release management, UAT, go-live, defect management&lt;br/&gt;PH Analog: PDSA cycle, phased rollout, pilot evaluation, program launch&lt;br/&gt;Frameworks: Agile/Scrum, Kanban; PDSA, implementation science&lt;br/&gt;Artifacts: release notes, test results, training materials, change log\"]\n\n    P6[\"&lt;b&gt;Ch 11: Evaluation (BA) | Program Evaluation & QI (PH)&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;BA Terms: solution evaluation, KPI tracking, ROI, lessons learned&lt;br/&gt;PH Analog: program evaluation, health indicators, cost-effectiveness, QI&lt;br/&gt;Frameworks: CDC Evaluation Framework, RE-AIM, PDCA/PDSA&lt;br/&gt;Artifacts: KPI dashboard, evaluation report, after-action review\"]\n\n    P1 --&gt; P2\n    P2 --&gt; P3\n    P3 --&gt; P4\n    P4 --&gt; P5\n    P5 --&gt; P6\n    P6 -.-&gt;|\"Iterate and Improve\"| P1\n\n    linkStyle 5 stroke:#333333\n\n\n\n\nFigure 5.1: Business Analyst Process Mapped to Public Health Analyst Workflow\n\n\n\n\n\nEach phase produces artifacts and applies frameworks that translate across domains. The following sections detail the terminology mappings for each phase.\n\n\n5.1.3 Terminology Translation Table\n\n5.1.3.1 Planning & Strategy Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nBusiness Need / Business Case\nPublic Health Challenge / Health Need\nIn PH, the driver for change is health equity or social determinants, not profit\n\n\nCurrent State Analysis\nEpidemiological Baseline\nDocument existing conditions: disease incidence, service gaps, health disparities\n\n\nFuture State / Vision\nProgram Goals & Intended Outcomes\nDefine success via improved health indicators, not market share\n\n\nConstraints / Assumptions\nSocial Determinants / Policy Constraints\nInclude funding limitations, regulatory requirements (HIPAA), cultural barriers, equity considerations\n\n\n\n\n\n\n5.1.4 Stakeholder & Communication Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nStakeholder\nInterest Holder / Community Partner / Rights Holder\nSee detailed note below on terminology considerations\n\n\nStakeholder Analysis\nCommunity Partner Mapping\nIdentify power dynamics, health equity implications\n\n\nRequirements Workshop\nCommunity Engagement Session\nPH emphasizes participatory approaches and cultural competency\n\n\n\n\n\n\n\n\n\nWarningA Note on ‘Stakeholder’ Terminology\n\n\n\nWhile “stakeholder” is standard in BA/Agile contexts (and used throughout this book for clarity), it carries colonial connotations in public health settings. The term evokes land claims and power imbalances, potentially disempowering Indigenous peoples and marginalized communities.\nPreferred alternatives in public health contexts:\n\nGeneral Use: Interest Holders, Parties/Affected Parties, Beneficiaries, Actants\nAction-Oriented: Constituents, Key Informants, Knowledge Users, Rights Holders\n\nWhen writing for mixed audiences, acknowledge both terms. When writing for public health audiences exclusively, favor the alternatives.\n\n\n\n5.1.4.1 Requirements & Analysis Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nUser Story\nService-User Scenario / GPS Format\nGPS = “Given [context], the Person [role] Should [action]” for clinical settings\n\n\nEpic\nGrant Objective / Program Goal\nHigh-level outcome (e.g., “Reduce TB incidence by 10%”)\n\n\nRequirements\nProgram Protocols / Clinical Guidelines\nBusiness rules are often legally or clinically mandated in PH\n\n\nNFRs (Non-Functional Requirements)\nImplementation Characteristics (CFIR)\nScalability = Outbreak Resilience; Security = HIPAA/Trust\n\n\nProcess Model (BPMN)\nIntervention Flowchart / Logic Model\nVisualize Inputs → Activities → Outputs → Outcomes\n\n\nData Model / Schema\nCase Definition / Data Dictionary\nER diagrams vs epidemiological case criteria\n\n\nUse Case Diagram\nPatient Journey Map\nUML diagrams vs mapping patient experience through care continuum\n\n\n\n\n\n5.1.4.2 Agile & Iteration Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nSprint\nPDSA Cycle / Adaptive Management\nPlan-Do-Study-Act: cyclic improvement framework\n\n\nBacklog\nWorkplan / Action Items\nPrioritized list of features vs outreach tasks\n\n\nSprint Review / Demo\nProgress Reporting\nOften aligned with grant reporting periods\n\n\nRetrospective\nAfter-Action Review\nSystematic reflection on what worked and what needs improvement\n\n\n\n\n\n5.1.4.3 Quality & Evaluation Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nKPI (Key Performance Indicator)\nHealth Indicator\n“15% improvement in customer satisfaction” vs “10% reduction in infection rate”\n\n\nAcceptance Test Plan\nEvaluation Protocol\nTest cases vs data collection and analysis plan\n\n\nQuality Assurance\nQuality Improvement (QI)\nSystematic checks, continuous improvement cycles\n\n\nBug / Defect\nAdverse Event / Variance\nSystem error vs deviation from expected health outcome\n\n\nLessons Learned\nAfter-Action Review\nRetrospective analysis, sharing successes and gaps across organization\n\n\n\n\n\n5.1.4.4 Design & Implementation Terms\n\n\n\n\n\n\n\n\nBA / Agile Term\nPublic Health Equivalent\nContext / Nuance\n\n\n\n\nPrototype / Mockup\nPilot Study / Field Test\nSoftware wireframe vs PH intervention pilot in limited population\n\n\nRisk Analysis\nCommunity Risk Assessment\nStandard risk assessment vs PH frameworks (CFIR, RE-AIM)\n\n\nGo-Live\nProgram Launch / Rollout\nSystem deployment vs intervention implementation\n\n\nTraining Plan\nCapacity Building\nEnd-user training vs workforce development\n\n\n\n\n\n\n5.1.5 Alternative User Story Formats for Public Health\nThe standard “As a [user], I want [feature], so that [benefit]” format often fails in clinical contexts. Use these alternatives:\n\n5.1.5.1 GPS Format (Given-Person-Should)\n\n“Given [clinical context], the [health worker role] should [specific action] to [health outcome].”\n\nExample:\n\n“Given a positive TB test result, the contact tracer should initiate household investigation within 48 hours to prevent secondary transmission.”\n\n\n\n5.1.5.2 Service-User Scenario\nA narrative vignette describing a patient’s journey through the system:\n\n“Maria, a 45-year-old farmworker, visits a mobile clinic for diabetes screening. She speaks primarily Spanish and has no regular primary care provider. The system must support her preferred language, connect her to follow-up care, and track her screening results for population health reporting.”\n\n\n\n5.1.5.3 Situational Protocol\nContext-specific workflow tied to clinical guidelines:\n\n“When a laboratory reports a confirmed measles case, the system shall generate a contact list and notify the assigned epidemiologist within 4 hours.”\n\n\n\n\n5.1.6 The Logic Model as Requirements Framework\nIn public health, the Logic Model serves a similar purpose to a requirements specification. Understanding this mapping helps BA professionals communicate with PH colleagues:\n\n\n\n\n\n\nflowchart LR\n    A[Inputs&lt;br/&gt;Resources] --&gt; B[Activities&lt;br/&gt;What we do]\n    B --&gt; C[Outputs&lt;br/&gt;Direct products]\n    C --&gt; D[Outcomes&lt;br/&gt;Short-term]\n    D --&gt; E[Impact&lt;br/&gt;Long-term]\n\n\n\n\nFigure 5.2: Logic Model Structure\n\n\n\n\n\n\n\n\nLogic Model Component\nBA Equivalent\n\n\n\n\nInputs\nResources, Constraints, Assumptions\n\n\nActivities\nFunctional Requirements, Use Cases\n\n\nOutputs\nDeliverables, System Features\n\n\nOutcomes\nSuccess Metrics, KPIs\n\n\nImpact\nBusiness Value, Strategic Objectives\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nIn CancerSurv, the Logic Model framing helped translate between teams:\n\nInput: Grant funding, registrar staff, hospital data feeds\nActivity: Case abstraction, data quality checks, interoperability\nOutput: Complete case records, quality reports, data submissions\nOutcome: 95% data completeness, timely NPCR submissions\nImpact: Accurate survival statistics, targeted prevention resources\n\n\n\n\n\n5.1.7 Data Standards as Requirements\nIn public health IT, data standards are non-negotiable requirements, not optional “technical details”:\n\n\n\n\n\n\n\n\nStandard\nPurpose\nBA Implication\n\n\n\n\nHL7\nMessaging between systems\nDefine trigger events (e.g., “ADT A01, Admit Patient”)\n\n\nFHIR\nModern API-based exchange\nSpecify FHIR Resources (Patient, Observation)\n\n\nUSCDI\nFederal data interoperability\nRequired for ONC certification\n\n\nICD-10 / ICD-O-3\nDiagnosis coding\nValidation rules in requirements\n\n\nSNOMED CT\nClinical terminology\nConcept mapping specifications\n\n\nLOINC\nLab test coding\nInterface specifications\n\n\n\n\n\n5.1.8 Quick Reference Card\nFor easy access during meetings, here are the most commonly needed translations:\n\n\n\n\n\n\n\nWhen they say…\nThey might mean…\n\n\n\n\n“What’s the user story?”\n“What is the service-user scenario or clinical workflow?”\n\n\n“Let’s do a sprint”\n“Let’s run a PDSA cycle”\n\n\n“What are the NFRs?”\n“What are the implementation characteristics?”\n\n\n“Stakeholder meeting”\n“Community partner engagement session”\n\n\n“Business case”\n“Public health challenge / needs assessment”\n\n\n“Acceptance criteria”\n“Evaluation protocol measures”\n\n\n“Technical debt”\n“System sustainability issues”\n\n\n“MVP (Minimum Viable Product)”\n“Pilot intervention”",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Terminology Dictionary</span>"
    ]
  },
  {
    "objectID": "chapters/04-planning.html",
    "href": "chapters/04-planning.html",
    "title": "6  Planning & Needs Assessment",
    "section": "",
    "text": "6.1 Planning & Strategy / Needs Assessment\nEvery successful project begins with understanding the problem. In business analysis, this phase is called Strategy Analysis or Planning. In public health, it is the Needs Assessment or Community Health Assessment. Both seek to answer the same fundamental question: What problem are we solving, and for whom?",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Planning & Needs Assessment</span>"
    ]
  },
  {
    "objectID": "chapters/04-planning.html#planning-strategy-needs-assessment",
    "href": "chapters/04-planning.html#planning-strategy-needs-assessment",
    "title": "6  Planning & Needs Assessment",
    "section": "",
    "text": "6.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nBusiness Need\nPublic Health Challenge\n\n\nCurrent State Analysis\nEpidemiological Baseline\n\n\nFuture State Vision\nProgram Goals & Intended Outcomes\n\n\nStakeholder Identification\nCommunity Partner Mapping\n\n\nFeasibility Assessment\nResource & Capacity Analysis\n\n\n\n\n\n6.1.2 Start Translation Early\nMany health IT projects encounter translation challenges only when they reach software requirements definition. By then, both teams have invested time and developed expectations using their own terminology. Untangling miscommunication while simultaneously trying to make progress creates unnecessary friction.\nThe business case is the ideal moment to introduce translation.\nWhen building the justification for funding (whether a grant application, budget request, or vendor RFP), explicitly consider:\n\nWhat terminology gaps exist between the technical and programmatic teams?\nWhat training or facilitation will help teams communicate effectively?\nWho will serve as translators or bridges between domains?\nWhat documentation will help each team understand the other’s frameworks?\n\nIncluding these considerations in the business case accomplishes two things:\n\nResource discovery: The funding request captures the full scope of what the project needs, including human and process elements, not just technical deliverables\nEarly alignment: Teams enter requirements gathering with shared vocabulary, reducing costly clarification loops later\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nThe CancerSurv grant application included a budget line for “cross-domain facilitation workshops” and allocated time for the project manager to develop a terminology crosswalk document before vendor selection. When TechHealth Solutions came on board, the state team provided this crosswalk in the kickoff meeting, establishing shared language from day one.\n\n\n\n\n6.1.3 Defining the Problem\n\n6.1.3.1 Business Analysis Approach\nIn traditional BA, the business need emerges from organizational pain points:\n\nRevenue decline\nOperational inefficiency\nCompliance gaps\nCompetitive pressure\nTechnology obsolescence\n\nThe BA documents this in a Business Case that quantifies the problem, proposes solutions, and projects return on investment.\n\n\n6.1.3.2 Public Health Approach\nIn public health, the need emerges from population health data:\n\nDisease incidence and prevalence\nHealth disparities across demographics\nService access gaps\nOutbreak patterns\nUnmet community needs\n\nThe epidemiologist documents this in a Needs Assessment that quantifies health burden, identifies determinants, and prioritizes interventions.\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nBusiness Need (BA framing): The legacy mainframe system is approaching end-of-life. Maintenance costs have increased 40% over three years. The system cannot support modern interoperability requirements or remote work.\nPublic Health Challenge (PH framing): Cancer data completeness has declined to 89%, below the CDC target of 95%. Late-stage diagnoses are increasing in rural counties, suggesting gaps in early detection. The current system cannot support the real-time analytics needed to identify and address disparities.\nBoth framings describe the same project. The BA framing emphasizes operational efficiency; the PH framing emphasizes health outcomes. Effective planning addresses both.\n\n\n\n\n\n6.1.4 Current State Analysis\n\n6.1.4.1 Documenting What Exists\nBefore defining requirements, understand what currently exists. The approaches differ in emphasis but serve the same purpose:\nBA Current State Analysis:\n\nProcess maps (BPMN diagrams)\nSystem inventories\nPain point interviews\nPerformance metrics\nTechnical debt assessment\n\nPH Epidemiological Baseline:\n\nDisease surveillance data\nDemographic health profiles\nService utilization patterns\nHealth equity indicators\nEnvironmental and social determinants\n\n\n\n6.1.4.2 Data Sources for Current State\n\n\n\nBA Data Sources\nPH Data Sources\n\n\n\n\nSystem logs, usage analytics\nDisease registries, vital records\n\n\nUser surveys, interviews\nCommunity health surveys (BRFSS)\n\n\nProcess documentation\nClinical guidelines, protocols\n\n\nFinancial reports\nGrant reports, program evaluations\n\n\nVendor assessments\nCDC/state health department data\n\n\n\n\n\n\n6.1.5 Future State Vision\n\n6.1.5.1 Defining Success\nThe future state describes what success looks like. Again, the framing differs:\nBA Future State:\n\nSystem capabilities and features\nProcess improvements\nPerformance targets\nTechnical architecture\nIntegration landscape\n\nPH Program Goals:\n\nHealth outcome improvements\nDisparity reductions\nService access expansion\nQuality metrics\nPopulation health indicators\n\n\n\n6.1.5.2 SMART Objectives\nBoth domains benefit from SMART objective setting:\n\n\n\n\n\n\n\n\nComponent\nBA Example\nPH Example\n\n\n\n\nSpecific\nReduce case entry time\nIncrease early-stage cancer detection\n\n\nMeasurable\nFrom 15 to 8 minutes per case\nFrom 45% to 55% of cases\n\n\nAchievable\nBased on vendor benchmarks\nBased on peer state performance\n\n\nRelevant\nSupports registrar productivity\nSupports prevention targeting\n\n\nTime-bound\nWithin 6 months of go-live\nWithin 2 years of program launch\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSMART Objective (Dual Framing):\nBA: “Within 6 months of CancerSurv deployment, average case abstraction time will decrease from 15 minutes to 8 minutes, as measured by system audit logs.”\nPH: “Within 2 years of CancerSurv deployment, data completeness will increase from 89% to 95%, enabling accurate survival analysis and disparity identification for the state cancer plan.”\nBoth objectives are valid; both should appear in project documentation.\n\n\n\n\n\n6.1.6 Proving Impact: Defining Success Metrics That Matter\nDuring the Planning phase, both teams must establish not just technical deliverables but proof points that demonstrate public health value and return on investment. This is not an afterthought for the Evaluation phase; impact measurement must be designed into the project from the beginning.\n\n6.1.6.1 Why This Matters Now\nWhen funding is constrained, every technology investment must justify its worth. Programs that cannot demonstrate measurable health outcomes or cost savings risk elimination. Grant proposals that lack data-driven proof of impact are increasingly non-competitive.\nThis creates a shared responsibility between technical and public health teams:\n\nTechnical teams (BAs, developers, product managers) must design systems that capture and report impact metrics automatically\nPublic health teams (epidemiologists, program managers, evaluators) must define what “success” looks like in measurable, system-capturable terms\n\nNeither can succeed without the other. A system that tracks technical performance but not health outcomes cannot justify continued funding. A program with strong outcomes but no data to prove them cannot compete for resources.\n\n\n6.1.6.2 Focus on Outcomes, Not Just Outputs\nDuring planning, resist the temptation to define success solely through system features or process metrics. Instead, establish clear lines of sight from technical deliverables to health outcomes:\n\n\n\n\n\n\n\nDon’t Just Measure…\nInstead, Translate To…\n\n\n\n\nSystem processes 12,000 records annually\nFaster outbreak detection through real-time analytics\n\n\nAchieved 95% data quality scores\nEnabled accurate survival statistics that guide treatment protocols\n\n\nImplemented HL7 FHIR integration\nReduced registrar burden by 40%, allowing focus on complex cases\n\n\nBuilt analytics dashboard\nIdentified cancer disparities 6 months faster, enabling targeted screening\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nDuring the Planning phase, the CancerSurv project team established paired metrics that satisfy both technical and programmatic accountability:\nBaseline Metrics: - Legacy system processes 12,000 cases annually with 87% data completeness - 6-month lag between data collection and annual reporting - Manual processes consume 15 minutes per case for abstraction - Limited ability to identify geographic or demographic disparities\nTarget Metrics (Technical + Health Outcomes):\n\nData Completeness: Increase from 87% to 95% within 18 months\n\nTechnical benefit: Meets CDC/NPCR reporting standards\nHealth outcome: Approximately 960 additional cases fully documented, strengthening survival analyses and disparity assessments\n\nReporting Timeliness: Reduce lag from 6 months to 3 months\n\nTechnical benefit: Automated data pipelines and validation\nHealth outcome: Policy decisions about screening programs informed by more current data\n\nWorkflow Efficiency: Reduce abstraction time from 15 to 8 minutes per case\n\nTechnical benefit: Improved UI/UX and automated coding suggestions\nHealth outcome: Registrars can process more cases or dedicate time to complex case resolution\n\nDisparity Identification: Enable real-time geographic and demographic analysis\n\nTechnical benefit: Analytics dashboard with interactive mapping\nHealth outcome: Target screening and prevention resources to underserved populations faster\n\n\nThese metrics were not arbitrary technical goals. They directly map to CDC reporting requirements, state strategic health objectives, and competitive grant criteria. By establishing them during Planning, the team ensured that both the business case and the system requirements aligned around demonstrable impact.\n\n\n\n\n6.1.6.3 Making the Case to Multiple Audiences\nSuccess metrics serve different stakeholders with different priorities. Plan for multiple reporting formats:\nFor CDC/NPCR Program: - Annual performance reports demonstrating compliance with NPCR standards - Data quality metrics (completeness, timeliness, validity) - Comparison to national benchmarks\nFor State Legislature and Budget Offices: - Budget justifications showing public health value per dollar invested - Efficiency gains (e.g., “Automation saves 140 hours/month, equivalent to $X salary”) - Health outcomes (e.g., “Earlier detection in rural counties reduced late-stage diagnoses by 12%”)\nFor Hospital Partners and Data Submitters: - Evidence that data submission yields insights valuable to their quality improvement efforts - Feedback reports showing how their data contributes to statewide cancer prevention - Reduced burden through automated electronic reporting\nFor Registry Staff: - Tangible improvements in workflow efficiency and job satisfaction - Recognition of their work’s impact on community health - Career development opportunities through new technical skills\n\n\n\n\n\n\nImportantDesign for Impact from Day One\n\n\n\nProving impact is not solely the responsibility of the public health team’s evaluation unit. Business analysts, developers, and product managers must design impact measurement into the system architecture from requirements gathering forward. This means:\n\nIncluding “evaluation metrics capture” as functional requirements, not optional features\nDesigning data models that support both operational reporting and outcome analysis\nBuilding dashboards and export functions that generate grant-ready reports automatically\nArchitecting audit logs that track not just system usage but workflow improvements\n\nLikewise, epidemiologists and registry staff must collaborate with technical partners early to define what “success” looks like in measurable, system-capturable terms. Vague goals like “improve cancer outcomes” must translate to specific, quantifiable indicators the system can track.\n\n\n\n\n6.1.6.4 Building Impact into the Business Case\nThe business case or grant application developed during this Planning phase should explicitly address impact demonstration:\nResource Allocation: - Budget line items for evaluation tools, data visualization platforms, or analytics staff - Time allocated for defining metrics and establishing baselines - Training for staff on using system-generated reports for grant writing\nLong-term Sustainability: - How will the system prove its value over time to justify continued funding? - What ongoing performance indicators will be tracked and reported? - Who is responsible for translating technical metrics into health outcome narratives?\nCompetitive Positioning: - How does this project’s approach to impact measurement differentiate the proposal? - What evidence-based outcomes make the case more compelling than competing priorities?\nWhen both technical and public health teams embrace this shared responsibility for proving impact, technology investments do more than deliver features on time and on budget. They deliver demonstrable improvements in population health, which justifies existing funding and unlocks new opportunities. Public health leaders and elected officials are far more likely to protect and expand programs that show tangible results: reduced cancer mortality, eliminated disparities, dollars saved through prevention.\n\n\n\n6.1.7 Stakeholder / Community Partner Identification\n\n6.1.7.1 Mapping the Landscape\nIdentifying who participates in the project requires understanding both organizational and community perspectives:\n\n\n\n\n\n\nflowchart TB\n    subgraph Internal[\"Internal Partners\"]\n        A[Registry Director]\n        B[Cancer Registrars]\n        C[Epidemiologists]\n        D[IT Department]\n    end\n    \n    subgraph External[\"External Partners\"]\n        E[Hospitals]\n        F[Laboratories]\n        G[CDC/NPCR]\n        H[Vital Records]\n    end\n    \n    subgraph Community[\"Community\"]\n        I[Cancer Survivors]\n        J[Advocacy Groups]\n        K[Researchers]\n    end\n    \n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    E --&gt; B\n    F --&gt; B\n    G --&gt; A\n    H --&gt; C\n    I --&gt; J\n    J --&gt; A\n    K --&gt; C\n\n\n\n\nFigure 6.1: Stakeholder/Community Partner Landscape\n\n\n\n\n\n\n\n6.1.7.2 Power-Interest Analysis\nThe classic BA power-interest grid maps to PH community engagement levels:\n\n\n\nQuadrant\nBA Approach\nPH Approach\n\n\n\n\nHigh Power, High Interest\nManage closely\nActive partnership\n\n\nHigh Power, Low Interest\nKeep satisfied\nInform and consult\n\n\nLow Power, High Interest\nKeep informed\nEmpower and involve\n\n\nLow Power, Low Interest\nMonitor\nEnsure representation\n\n\n\n\n\n\n6.1.8 Feasibility Assessment\n\n6.1.8.1 Can We Do This?\nBoth domains assess feasibility before committing resources:\nBA Feasibility Dimensions:\n\nTechnical feasibility (Can we build it?)\nEconomic feasibility (Can we afford it?)\nOperational feasibility (Can we run it?)\nSchedule feasibility (Can we deliver on time?)\n\nPH Feasibility Dimensions:\n\nEvidence base (Does the intervention work?)\nResource availability (Do we have funding, staff?)\nPolitical will (Is there leadership support?)\nCommunity readiness (Will the population engage?)\nEthical considerations (Is it equitable?)\n\n\n\n\n\n\n\nTipBridging Tip\n\n\n\nWhen presenting feasibility to mixed audiences, address both technical and programmatic dimensions. A system that is technically feasible but lacks community buy-in will fail. An intervention with strong evidence but no technical infrastructure cannot scale.\n\n\n\n\n\n6.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nBusiness Case\nNeeds Assessment Report\nJustify the project\n\n\nStakeholder Register\nCommunity Partner Map\nIdentify participants\n\n\nCurrent State Analysis\nEpidemiological Baseline\nDocument starting point\n\n\nFuture State Vision\nProgram Goals\nDefine success\n\n\nFeasibility Study\nReadiness Assessment\nConfirm viability\n\n\n\n\n\n6.1.10 Common Pitfalls\nFor BA professionals entering PH:\n\nUnderestimating regulatory constraints (HIPAA, IRB)\nIgnoring health equity implications\nTreating clinical workflows like business processes\nMissing grant-cycle dependencies\n\nFor PH professionals working with BA:\n\nProviding needs assessments instead of requirements\nAssuming IT teams understand clinical context\nUnderspecifying data quality needs\nIgnoring change management complexity\n\n\n\n6.1.11 Moving Forward\nWith the problem defined and feasibility confirmed, the next phase focuses on Elicitation: gathering detailed information from stakeholders and community partners about their specific needs and constraints.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Planning & Needs Assessment</span>"
    ]
  },
  {
    "objectID": "chapters/05-elicitation.html",
    "href": "chapters/05-elicitation.html",
    "title": "7  Elicitation & Engagement",
    "section": "",
    "text": "7.1 Elicitation & Stakeholder Engagement\nOnce the problem is defined, we must gather detailed information about needs, constraints, and context. In business analysis, this is Elicitation. In public health, it is Stakeholder Engagement or Community-Based Participatory Research. Both involve systematic approaches to learning from people who will use, be affected by, or govern the solution.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Elicitation & Engagement</span>"
    ]
  },
  {
    "objectID": "chapters/05-elicitation.html#elicitation-stakeholder-engagement",
    "href": "chapters/05-elicitation.html#elicitation-stakeholder-engagement",
    "title": "7  Elicitation & Engagement",
    "section": "",
    "text": "7.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nElicitation Techniques\nCommunity Engagement Methods\n\n\nRequirements Workshops\nFocus Groups, Town Halls\n\n\nUser Interviews\nKey Informant Interviews\n\n\nDocument Analysis\nLiterature Review, Policy Analysis\n\n\nObservation\nEthnography, Site Visits\n\n\nPrototyping\nPilot Testing, Formative Research\n\n\n\n\n\n7.1.2 Elicitation Techniques Mapped\n\n7.1.2.1 Interviews\nBoth domains rely heavily on one-on-one conversations with knowledgeable individuals:\nBA User Interviews:\n\nFocus on workflow, pain points, desired features\nStructured around use cases or process steps\nDocument functional and non-functional requirements\n\nPH Key Informant Interviews:\n\nFocus on community needs, barriers, facilitators\nMay explore cultural context, health beliefs\nInform intervention design and implementation strategy\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nBA Interview with Registrar:\n“Walk me through your typical day. What tasks take the most time? Where do you encounter errors? What features would make your work easier?”\nPH Interview with Oncologist:\n“How do you currently receive staging information? What data quality issues affect your treatment decisions? How could better surveillance data support tumor board discussions?”\nBoth interviews inform CancerSurv requirements, but from different perspectives.\n\n\n\n\n\n7.1.3 Workshops and Focus Groups\nGroup sessions enable collaborative discovery:\nBA Requirements Workshop:\n\nFacilitated session with defined agenda\nUses techniques like brainstorming, affinity mapping\nProduces prioritized requirement lists\nResolves conflicts through negotiation\n\nPH Focus Group:\n\nSemi-structured group discussion\nExplores shared experiences and diverse perspectives\nMay surface unanticipated needs or concerns\nEmphasizes inclusive participation\n\n\n7.1.3.1 Document Analysis\nExisting documentation provides essential context:\n\n\n\nBA Documents\nPH Documents\n\n\n\n\nSystem specifications\nClinical protocols\n\n\nProcess manuals\nCDC guidelines\n\n\nVendor contracts\nGrant requirements\n\n\nTraining materials\nHealth education materials\n\n\nAudit reports\nProgram evaluations\n\n\n\n\n\n7.1.3.2 Observation\nWatching work happen reveals what interviews miss:\nBA Observation (Job Shadowing):\n\nWatch users perform tasks\nNote workarounds and inefficiencies\nIdentify undocumented processes\nTime critical workflows\n\nPH Observation (Site Visits, Ethnography):\n\nVisit clinics, community settings\nUnderstand context and constraints\nObserve patient-provider interactions\nIdentify environmental factors\n\n\n\n\n\n\n\nTipGemba Walks\n\n\n\nThe Lean concept of “Gemba” (going to the actual place where work happens) applies to both domains. For CancerSurv, this means spending time in the registry office watching abstractors work, not just interviewing them in a conference room.\n\n\n\n\n\n7.1.4 Engaging Diverse Voices\n\n7.1.4.1 The Equity Imperative\nPublic health emphasizes inclusive engagement, ensuring marginalized voices are heard. This principle benefits IT projects as well:\nQuestions to Ask:\n\nWho is missing from our stakeholder list?\nWhose needs might be overlooked by “typical” users?\nWhat barriers prevent participation (language, location, schedule)?\nHow do we ensure power imbalances do not silence important perspectives?\n\n\n\n7.1.4.2 Community-Based Participatory Research (CBPR)\nCBPR principles can strengthen BA elicitation:\n\n\n\n\n\n\n\nCBPR Principle\nBA Application\n\n\n\n\nCommunity as equal partner\nUsers co-design, not just provide input\n\n\nBuild on community strengths\nLeverage existing workflows that work\n\n\nBalance research and action\nDeliver incremental value during elicitation\n\n\nLong-term commitment\nMaintain relationships beyond project end\n\n\n\n\n\n\n7.1.5 Translating What You Hear\n\n7.1.5.1 From Needs to Requirements\nElicitation produces raw material that must be translated into actionable requirements:\n\n\n\n\n\n\nflowchart LR\n    A[Interviews&lt;br/&gt;Observations] --&gt; B[Elicitation Notes]\n    B --&gt; C[Analysis &&lt;br/&gt;Synthesis]\n    C --&gt; D[Requirements&lt;br/&gt;Documentation]\n    D --&gt; E[Validation with&lt;br/&gt;Stakeholders]\n    E --&gt; F[Approved&lt;br/&gt;Requirements]\n\n\n\n\nFigure 7.1: From Elicitation to Requirements\n\n\n\n\n\n\n\n7.1.5.2 Common Translation Challenges\n\n\n\n\n\n\n\n\nWhat Stakeholders Say\nWhat They Might Mean\nRequirement Implication\n\n\n\n\n“It should be easy to use”\nCurrent system requires too many clicks\nReduce clicks per task by 50%\n\n\n“We need real-time data”\nCurrent reports are weeks old\nDashboard updates within 24 hours\n\n\n“Make it like Excel”\nUsers are comfortable with Excel\nFamiliar grid-based interface\n\n\n“We need better reports”\nCurrent reports lack specific metrics\nAdd [specific metric] to [report]\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStakeholder statement: “We need the system to be faster.”\nProbing questions:\n\nWhich specific tasks feel slow?\nHow long do those tasks take now?\nWhat would be an acceptable time?\nWhat happens when the system is slow?\n\nRefined requirement: “The case search function shall return results within 3 seconds for queries returning up to 1,000 records, to support efficient case lookup during abstraction.”\n\n\n\n\n\n7.1.6 Documentation Approaches\n\n7.1.6.1 BA Requirements Documentation\n\nUser Stories (Agile)\nUse Cases (UML)\nRequirements Specifications (Waterfall)\nAcceptance Criteria\n\n\n\n7.1.6.2 PH Program Documentation\n\nLogic Models\nIntervention Protocols\nEvaluation Plans\nImplementation Guides\n\n\n\n7.1.6.3 Bridging the Formats\nFor hybrid projects, consider dual documentation:\n\n\n\n\n\n\n\n\nAudience\nFormat\nContent\n\n\n\n\nDevelopment team\nUser Stories\nFunctional requirements in Agile format\n\n\nClinical stakeholders\nService-User Scenarios\nNarrative descriptions of clinical workflows\n\n\nFunders (CDC, grants)\nLogic Model\nInputs, activities, outputs, outcomes\n\n\nGovernance\nRequirements Traceability Matrix\nLinks requirements to objectives\n\n\n\n\n\n7.1.6.4 When Standard User Stories Fall Short\nThe standard Agile user story format (“As a [user], I want [feature], so that [benefit]”) works well for many software contexts. However, it often fails to capture the nuances of clinical workflows, regulatory requirements, and public health scenarios.\nWhy standard user stories may not fit clinical contexts:\n\nClinical workflows involve conditional logic (“if this lab result, then that action”)\nRegulatory requirements mandate specific timeframes and actions\nPatient safety considerations require explicit protocols, not just user preferences\nPublic health surveillance involves system-initiated actions, not just user-initiated features\n\nAlternative formats worth considering:\nGiven-Person-Should (GPS) Format:\nThis format emphasizes context and obligation rather than desire:\n\n“Given [clinical/situational context], the [health worker role] should [specific action] to [health outcome].”\n\nExample:\n\n“Given a positive TB test result, the contact tracer should initiate household investigation within 48 hours to prevent secondary transmission.”\n\nSituational Protocol Format:\nThis format ties system behavior to clinical guidelines or regulatory requirements:\n\n“When [triggering event], the system shall [required action] within [timeframe].”\n\nExample:\n\n“When a laboratory reports a confirmed measles case, the system shall generate a contact list and notify the assigned epidemiologist within 4 hours.”\n\nService-User Scenario Format:\nThis narrative format describes a patient or client journey through the system:\n\n“Maria, a 45-year-old farmworker, visits a mobile clinic for diabetes screening. She speaks primarily Spanish and has no regular primary care provider. The system must support her preferred language, connect her to follow-up care, and track her screening results for population health reporting.”\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStandard user story:\n“As a registrar, I want to search for existing cases, so that I can avoid creating duplicates.”\nGPS format (adding clinical context):\n“Given a new pathology report for a patient who may already be in the registry, the registrar should be able to search by name, SSN, and diagnosis within 3 seconds to prevent duplicate case creation and ensure accurate incidence counts.”\nSituational protocol (system-initiated):\n“When a new case is entered, the system shall automatically search for potential duplicates using probabilistic matching and present candidates to the registrar for review before saving.”\n\n\nChoose the format that best communicates intent to your audience. Development teams may still translate these into standard user stories for sprint planning, but starting with clinical context ensures nothing gets lost in translation.\n\n\n\n7.1.7 Validation and Confirmation\n\n7.1.7.1 Ensuring Accuracy\nElicitation is iterative. Validate what you heard:\nBA Validation Techniques:\n\nRequirements reviews\nPrototype walkthroughs\nStructured walkthroughs\nSign-off meetings\n\nPH Validation Techniques:\n\nMember checking (returning findings to participants)\nCommunity review sessions\nPilot testing\nExpert review panels\n\n\n\n\n7.1.8 Managing Conflicting Needs\n\n7.1.8.1 When Stakeholders Disagree\nConflicts are inevitable. Resolution approaches include:\n\n\n\n\n\n\n\nApproach\nWhen to Use\n\n\n\n\nPrioritization\nWhen resources are limited; use MoSCoW or weighted scoring\n\n\nNegotiation\nWhen compromise is possible without losing value\n\n\nEscalation\nWhen authority must resolve; use governance structure\n\n\nPhasing\nWhen both needs are valid; address in different releases\n\n\nAlternatives Analysis\nWhen creative solutions can satisfy both parties\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nConflict: Registrars want a simple, streamlined interface. Epidemiologists want comprehensive data fields for analysis.\nResolution: Implement a tiered interface:\n\nCore fields (required): Streamlined view for registrars\nExtended fields (optional): Available when needed\nAnalytics fields: Populated from other sources, not requiring registrar entry\n\n\n\n\n\n\n7.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nElicitation Results\nEngagement Summary\nRaw findings documentation\n\n\nRequirements Document\nProgram Protocol\nDetailed specifications\n\n\nUser Stories / Use Cases\nService-User Scenarios\nActionable descriptions\n\n\nStakeholder Feedback Log\nCommunity Input Register\nTrack all input received\n\n\n\n\n\n7.1.10 Moving Forward\nWith needs elicited and documented, the next phase focuses on Requirements Analysis: organizing, prioritizing, and specifying the detailed requirements that will guide solution design.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Elicitation & Engagement</span>"
    ]
  },
  {
    "objectID": "chapters/06-requirements.html",
    "href": "chapters/06-requirements.html",
    "title": "8  Requirements & Data Analysis",
    "section": "",
    "text": "8.1 Requirements Analysis & Data Analysis\nRaw elicitation findings must be organized, analyzed, and specified in detail. In business analysis, this is Requirements Analysis and Design Definition. In public health, it maps to Data Analysis and Logic Model Development. Both processes transform unstructured input into structured, actionable specifications.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Requirements & Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/06-requirements.html#requirements-analysis-data-analysis",
    "href": "chapters/06-requirements.html#requirements-analysis-data-analysis",
    "title": "8  Requirements & Data Analysis",
    "section": "",
    "text": "8.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nRequirements Analysis\nData Analysis\n\n\nRequirements Specification\nLogic Model / Theory of Change\n\n\nFunctional Requirements\nProgram Activities\n\n\nNon-Functional Requirements\nImplementation Characteristics\n\n\nData Requirements\nCase Definitions, Data Dictionaries\n\n\nBusiness Rules\nClinical Guidelines, Protocols\n\n\n\n\n\n8.1.2 Types of Requirements\n\n8.1.2.1 Functional Requirements\nBA Definition: What the system must do. Capabilities, features, functions.\nPH Equivalent: Program activities, intervention components, service delivery specifications.\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nFunctional Requirement (BA format):\n\nFR-101: The system shall allow users to search for cases by patient name, medical record number, or social security number.\n\nProgram Activity (PH format):\n\nCancer registrars will abstract and code incident cases from hospital pathology reports within 6 months of diagnosis date.\n\nBoth describe “what happens” but at different levels of specificity.\n\n\n\n\n8.1.2.2 Non-Functional Requirements (NFRs)\nBA Definition: Quality attributes, constraints, performance characteristics.\nPH Equivalent: Implementation characteristics (per CFIR framework).\n\n\n\n\n\n\n\n\nNFR Category\nBA Focus\nPH Focus (CFIR Domain)\n\n\n\n\nPerformance\nResponse time, throughput\nEfficiency of intervention delivery\n\n\nSecurity\nAccess control, encryption\nHIPAA compliance, trust\n\n\nScalability\nGrowth capacity\nOutbreak surge response\n\n\nUsability\nUser interface design\nComplexity, ease of adoption\n\n\nReliability\nUptime, fault tolerance\nService continuity\n\n\nInteroperability\nAPI standards, data exchange\nHealth information exchange\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nNFR (BA format):\n\nNFR-201: The system shall maintain 99.9% uptime during business hours (8 AM to 6 PM Eastern).\n\nImplementation Characteristic (PH format):\n\nThe CancerSurv platform must demonstrate high reliability to maintain registrar confidence and ensure continuous data collection, critical during cancer awareness campaigns when reporting volumes increase.\n\n\n\n\n\n8.1.2.3 Data Requirements\nData specifications are central to both domains:\nBA Data Model:\n\nEntity-Relationship diagrams\nDatabase schemas\nData dictionaries\nValidation rules\n\nPH Case Definitions:\n\nDiagnostic criteria\nInclusion/exclusion criteria\nCoding standards (ICD-O-3, TNM)\nData quality metrics\n\n\n\n\n\n\n\nerDiagram\n    PATIENT ||--o{ TUMOR : has\n    TUMOR ||--o{ TREATMENT : receives\n    TUMOR ||--|| DIAGNOSIS : \"classified by\"\n    FACILITY ||--o{ TUMOR : reports\n    \n    PATIENT {\n        string patient_id PK\n        string name\n        date birth_date\n        string ssn\n        string address\n    }\n    \n    TUMOR {\n        string tumor_id PK\n        string patient_id FK\n        date diagnosis_date\n        string primary_site\n        string histology\n        string stage\n    }\n\n\n\n\nFigure 8.1: CancerSurv Simplified Data Model\n\n\n\n\n\n\n\n8.1.2.4 Business Rules / Clinical Guidelines\nRules governing system behavior and data processing:\n\n\n\n\n\n\n\nBA Business Rule\nPH Clinical Guideline\n\n\n\n\n“Order cannot be placed if credit limit exceeded”\n“Case is reportable if primary site is within state jurisdiction”\n\n\n“Discount applies if quantity &gt; 100”\n“Stage is unknown if pathology report unavailable within 4 months”\n\n\n“Manager approval required for refunds &gt; $500”\n“Multiple primary rules apply per SEER guidelines”\n\n\n\n\n\n\n8.1.3 Data Standards as Primary Requirements\nIn commercial software projects, data standards (file formats, API specifications, integration protocols) are often treated as technical details to be resolved by developers during implementation. In public health IT, this approach fails.\nData standards are primary business requirements, not optional technical details.\nHealth information systems operate within a regulatory and interoperability landscape where specific standards are mandated, not merely preferred. These standards should be identified and documented early, during requirements analysis, not deferred to design or implementation.\n\n\n\n\n\n\n\n\nStandard\nPurpose\nRequirement Implication\n\n\n\n\nHIPAA\nPrivacy and security\nSecurity architecture, access controls, audit logging\n\n\nHL7 v2\nMessage-based data exchange\nInterface specifications for lab results, ADT events\n\n\nHL7 FHIR\nModern API-based exchange\nRESTful API design for EHR integration\n\n\nUSCDI\nFederal data interoperability\nRequired data classes for ONC certification\n\n\nICD-10 / ICD-O-3\nDiagnosis and oncology coding\nValidation rules, lookup tables, code mapping\n\n\nSNOMED CT\nClinical terminology\nConcept mapping specifications\n\n\nLOINC\nLaboratory test coding\nInterface specifications for electronic lab reporting\n\n\nNAACCR\nCancer registry standards\nData dictionary, edit checks, submission formats\n\n\n\n\n\n\n\n\n\nWarningCommon Pitfall\n\n\n\nWhen data standards are not identified as requirements, projects encounter costly surprises during integration testing. A system that functions correctly in isolation may fail when connected to external systems that expect specific data formats, codes, or protocols.\nFor business analysts entering public health IT: treat data standards as \"Must Have\" requirements from day one. Interview stakeholders about external data exchanges early, and document the specific standards each interface requires.\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nStandards-Based Requirements for CancerSurv:\n\n\n\n\n\n\n\nStandard\nCancerSurv Requirement\n\n\n\n\nHIPAA\nAll PHI encrypted at rest and in transit; role-based access; 6-year audit log retention\n\n\nHL7 FHIR\nPatient, Condition, and Observation resources for hospital EHR integration\n\n\nNAACCR v24\nAll required data items; automated EDITS validation; annual submission file generation\n\n\nICD-O-3\nValidated primary site and histology codes with cross-validation rules\n\n\nLOINC\nMapping table for incoming electronic pathology reports\n\n\n\nThese standards-based requirements appeared in the CancerSurv requirements specification alongside functional requirements, with the same priority and traceability as any other \"Must Have\" item.\n\n\n\n\n8.1.4 The Logic Model as Requirements Framework\nPublic health uses the Logic Model to specify program components. This structure maps directly to requirements categories:\n\n\n\n\n\n\nflowchart LR\n    subgraph Inputs[\" \"]\n        I[\"**Inputs**&lt;br/&gt;(Resources)&lt;br/&gt;───────&lt;br/&gt;Funding&lt;br/&gt;Staff&lt;br/&gt;Data feeds&lt;br/&gt;Infrastructure\"]\n    end\n    \n    subgraph Activities[\" \"]\n        A[\"**Activities**&lt;br/&gt;(Functions)&lt;br/&gt;───────&lt;br/&gt;Case abstraction&lt;br/&gt;Data quality&lt;br/&gt;Reporting&lt;br/&gt;Analytics\"]\n    end\n    \n    subgraph Outputs[\" \"]\n        O[\"**Outputs**&lt;br/&gt;(Deliverables)&lt;br/&gt;───────&lt;br/&gt;Case records&lt;br/&gt;Quality reports&lt;br/&gt;NPCR submissions&lt;br/&gt;Dashboards\"]\n    end\n    \n    subgraph Outcomes[\" \"]\n        OC[\"**Outcomes**&lt;br/&gt;(Success Metrics)&lt;br/&gt;───────&lt;br/&gt;95% completeness&lt;br/&gt;Timely reporting&lt;br/&gt;User satisfaction\"]\n    end\n    \n    I --&gt; A --&gt; O --&gt; OC\n\n\n\n\nFigure 8.2: Logic Model Components Mapped to Requirements\n\n\n\n\n\n\n\n\nLogic Model Component\nRequirements Category\n\n\n\n\nInputs\nConstraints, Assumptions, Dependencies\n\n\nActivities\nFunctional Requirements\n\n\nOutputs\nSystem Deliverables, Features\n\n\nOutcomes\nSuccess Metrics, Acceptance Criteria\n\n\nImpact\nStrategic Objectives, Business Value\n\n\n\n\n\n8.1.5 Prioritization\n\n8.1.5.1 Methods for Ranking Requirements\nNot all requirements are equal. Prioritization ensures critical needs are addressed first:\nMoSCoW Method:\n\nMust have: Essential for go-live\nShould have: Important but not critical\nCould have: Desirable if time permits\nWon’t have: Out of scope for this release\n\nWeighted Scoring:\nAssign weights to criteria (business value, regulatory requirement, user impact) and score each requirement.\nKano Model:\n\nBasic needs (expected, cause dissatisfaction if missing)\nPerformance needs (more is better)\nDelighters (unexpected features that excite)\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nMust Have:\n\nCase entry and coding functionality\nHIPAA-compliant security\nNPCR data submission capability\n\nShould Have:\n\nReal-time analytics dashboard\nMobile-friendly interface\nAutomated duplicate detection\n\nCould Have:\n\nMachine learning for coding assistance\nPatient portal for self-reported outcomes\nIntegration with research databases\n\n\n\n\n\n\n8.1.6 Requirements Traceability\n\n8.1.6.1 Linking Requirements to Objectives\nTraceability ensures every requirement connects to a business need or program goal:\n\n\n\n\n\n\nflowchart TB\n    BN[Business Need /&lt;br/&gt;Program Goal] --&gt; FR[Functional&lt;br/&gt;Requirement]\n    BN --&gt; NFR[Non-Functional&lt;br/&gt;Requirement]\n    FR --&gt; TC[Test Case]\n    NFR --&gt; TC\n    FR --&gt; US[User Story]\n    TC --&gt; TR[Test Result]\n\n\n\n\nFigure 8.3: Requirements Traceability Structure\n\n\n\n\n\nTraceability Matrix Example:\n\n\n\n\n\n\n\n\n\n\nRequirement ID\nDescription\nSource\nPriority\nTest Case\n\n\n\n\nFR-101\nCase search functionality\nRegistrar interviews\nMust\nTC-101, TC-102\n\n\nFR-102\nICD-O-3 coding validation\nNAACCR standards\nMust\nTC-103\n\n\nNFR-201\n99.9% uptime\nSLA requirements\nMust\nTC-201\n\n\n\n\n\n\n8.1.7 Specification Formats\n\n8.1.7.1 Writing Good Requirements\nRegardless of format, good requirements share characteristics:\n\n\n\n\n\n\n\n\nCharacteristic\nDescription\nExample\n\n\n\n\nComplete\nContains all necessary information\nIncludes error handling, edge cases\n\n\nConsistent\nDoes not contradict other requirements\nUses standard terminology\n\n\nUnambiguous\nOnly one interpretation possible\n“Within 3 seconds” not “quickly”\n\n\nVerifiable\nCan be tested\nMeasurable acceptance criteria\n\n\nTraceable\nLinks to source and test\nIncludes requirement ID\n\n\n\n\n\n8.1.7.2 User Story Format\nFor Agile projects:\n\nAs a [role], I want [feature], so that [benefit].\n\nAcceptance Criteria:\n\nGiven [context], when [action], then [result]\n\n\n\n8.1.7.3 GPS Format for Clinical Contexts\n\nGiven [clinical context], the [health worker role] should [specific action] to [health outcome].\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nUser Story:\n\nAs a cancer registrar, I want to search for existing cases before creating a new record, so that I avoid creating duplicate entries.\n\nGPS Format:\n\nGiven a new pathology report, the registrar should search existing cases by patient identifiers before abstracting, to maintain data integrity and accurate incidence counts.\n\nAcceptance Criteria:\n\nGiven a patient name, when the registrar searches, then matching cases display within 3 seconds\nGiven a patient with no existing cases, when the registrar searches, then a “No matches found” message displays with option to create new case\n\n\n\n\n\n\n8.1.8 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nRequirements Specification\nLogic Model\nDocument what must be built\n\n\nData Dictionary\nCase Definition / Data Standards\nSpecify data structures\n\n\nBusiness Rules Catalog\nClinical Protocol\nDefine processing rules\n\n\nTraceability Matrix\nEvaluation Framework\nLink requirements to objectives\n\n\nPrioritized Backlog\nWorkplan\nOrder implementation work\n\n\n\n\n\n8.1.9 Moving Forward\nWith requirements analyzed, prioritized, and specified, the next phase focuses on Design: defining how the solution will be built to meet these requirements.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Requirements & Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/07-design.html",
    "href": "chapters/07-design.html",
    "title": "9  Design & Solution Definition",
    "section": "",
    "text": "9.1 Design & Solution Definition / Intervention Design\nWith requirements defined, we move to designing the solution. In business analysis, this is Solution Design or Design Definition. In public health, it parallels Intervention Design and Implementation Planning. Both involve translating requirements into a blueprint for action.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design & Solution Definition</span>"
    ]
  },
  {
    "objectID": "chapters/07-design.html#design-solution-definition-intervention-design",
    "href": "chapters/07-design.html#design-solution-definition-intervention-design",
    "title": "9  Design & Solution Definition",
    "section": "",
    "text": "9.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSolution Architecture\nIntervention Framework\n\n\nSystem Design\nProgram Design\n\n\nInterface Design\nService Delivery Model\n\n\nIntegration Design\nHealth Information Exchange\n\n\nChange Management Plan\nImplementation Strategy (CFIR)\n\n\n\n\n\n9.1.2 Architecture and Framework\n\n9.1.2.1 Solution Architecture\nBA solution architecture defines:\n\nSystem components and their relationships\nTechnology stack selection\nIntegration points with existing systems\nData flow between components\nSecurity architecture\n\n\n\n9.1.2.2 Intervention Framework\nPH intervention design defines:\n\nCore intervention components\nDelivery mechanisms\nTarget populations\nAdaptable vs. core elements\nContextual considerations\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSolution Architecture (BA):\n┌─────────────────────────────────────────────────────────┐\n│                    CancerSurv Platform                   │\n├──────────────┬──────────────┬──────────────┬────────────┤\n│  Web UI      │  API Layer   │  Analytics   │  Reporting │\n│  (React)     │  (REST/FHIR) │  (R/Python)  │  Engine    │\n├──────────────┴──────────────┴──────────────┴────────────┤\n│                    Core Services                         │\n│  Case Management │ Data Quality │ User Management        │\n├─────────────────────────────────────────────────────────┤\n│                    Data Layer                            │\n│  PostgreSQL │ Document Store │ Data Warehouse           │\n└─────────────────────────────────────────────────────────┘\nIntervention Framework (PH):\nCancerSurv delivers the surveillance intervention through:\n\nCore components: Case abstraction, data quality, NPCR reporting (non-negotiable)\nAdaptable elements: Dashboard customization, local report templates\nDelivery mechanism: Cloud-based SaaS with local training support\nTarget population: State cancer registries, hospital tumor registrars\n\n\n\n\n\n\n9.1.3 Design Patterns\n\n9.1.3.1 User Interface Design\nBoth domains emphasize user-centered design:\nBA UI/UX Approach:\n\nWireframes and mockups\nUser journey mapping\nUsability testing\nAccessibility compliance (WCAG)\n\nPH Service Design Approach:\n\nPatient journey mapping\nCultural competency review\nHealth literacy assessment\nEquity impact analysis\n\n\n\n9.1.3.2 Key Design Principles\n\n\n\n\n\n\n\n\nPrinciple\nBA Application\nPH Application\n\n\n\n\nSimplicity\nMinimize clicks, clear navigation\nReduce complexity for adoption\n\n\nConsistency\nStandard UI patterns\nConsistent with clinical workflows\n\n\nFeedback\nVisual confirmation of actions\nClear outcome indicators\n\n\nError Prevention\nValidation before submission\nBuilt-in clinical decision support\n\n\nFlexibility\nCustomizable views, workflows\nAdaptable to local context\n\n\n\n\n\n\n9.1.4 Integration Design\n\n9.1.4.1 Connecting Systems\nHealth IT projects require extensive integration:\n\n\n\n\n\n\nflowchart LR\n    subgraph External[\"External Systems\"]\n        H[Hospital EHRs]\n        L[Lab Systems]\n        V[Vital Records]\n        N[NPCR/CDC]\n    end\n    \n    subgraph CancerSurv[\"CancerSurv Platform\"]\n        API[Integration Layer]\n        Core[Core System]\n    end\n    \n    H --&gt;|HL7 FHIR| API\n    L --&gt;|HL7 v2| API\n    V --&gt;|Batch| API\n    API --&gt; Core\n    Core --&gt;|XML| N\n\n\n\n\nFigure 9.1: CancerSurv Integration Landscape\n\n\n\n\n\n\n\n9.1.4.2 Integration Standards\n\n\n\n\n\n\n\n\nStandard\nUse Case\nDesign Consideration\n\n\n\n\nHL7 FHIR\nReal-time EHR integration\nREST APIs, JSON payloads\n\n\nHL7 v2.x\nLegacy lab interfaces\nMessage parsing, acknowledgments\n\n\nNAACCR XML\nCancer registry exchange\nSchema validation, field mapping\n\n\nDirect Protocol\nSecure health messaging\nCertificate management\n\n\n\n\n\n\n9.1.5 Implementation Readiness Assessment\n\n9.1.5.1 CFIR for Design Validation\nThe Consolidated Framework for Implementation Research (CFIR) provides a lens for evaluating design decisions:\n\n\n\n\n\n\n\nCFIR Domain\nDesign Questions\n\n\n\n\nIntervention Characteristics\nIs the design evidence-based? Is it adaptable?\n\n\nOuter Setting\nDoes it meet regulatory requirements? Does it connect to external systems?\n\n\nInner Setting\nDoes it fit organizational workflows? Is infrastructure adequate?\n\n\nIndividuals\nWill users accept it? What training is needed?\n\n\nProcess\nHow will it be implemented? Who champions it?\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nCFIR-Informed Design Review:\n\n\n\n\n\n\n\n\nCFIR Construct\nCancerSurv Design Decision\nRationale\n\n\n\n\nRelative Advantage\nModern UI, mobile access\nClear improvement over mainframe\n\n\nComplexity\nPhased rollout, role-based views\nReduce cognitive load\n\n\nAdaptability\nConfigurable data fields\nSupport local registry needs\n\n\nAvailable Resources\nCloud-hosted, vendor support\nMinimize IT infrastructure burden\n\n\nSelf-Efficacy\nEmbedded training, help system\nBuild user confidence\n\n\n\n\n\n\n\n\n9.1.6 Prototyping and Validation\n\n9.1.6.1 Iterative Design\nDesign should be validated before full development:\nBA Prototyping:\n\nLow-fidelity wireframes for concept validation\nHigh-fidelity mockups for detailed feedback\nInteractive prototypes for workflow testing\nMVP (Minimum Viable Product) for market validation\n\nPH Piloting:\n\nFormative research with target population\nPilot testing in representative sites\nRapid cycle evaluation (PDSA)\nFidelity assessment\n\n\n\n9.1.6.2 Prototype Fidelity Levels\n\n\n\n\n\n\n\n\n\nLevel\nBA Artifact\nPH Artifact\nPurpose\n\n\n\n\nLow\nPaper sketches, Balsamiq\nConcept paper, draft protocol\nConcept validation\n\n\nMedium\nClickable mockups\nPilot at 1-2 sites\nWorkflow validation\n\n\nHigh\nWorking prototype\nMulti-site pilot\nFull process validation\n\n\n\n\n\n\n9.1.7 Change Management Planning\n\n9.1.7.1 Preparing for Transition\nDesign must include plans for organizational change:\nBA Change Management:\n\nStakeholder impact analysis\nCommunication plan\nTraining plan\nResistance management\nTransition strategy\n\nPH Implementation Planning:\n\nCapacity building\nTechnical assistance model\nSustainability planning\nScale-up strategy\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nChange Management Elements:\n\n\n\n\n\n\n\nElement\nPlan\n\n\n\n\nTraining\n3-tier approach: super-users (in-person), all users (webinar), ongoing (self-paced modules)\n\n\nCommunication\nMonthly newsletters, demo sessions at registrar conferences\n\n\nSupport\nHelp desk during business hours; online knowledge base; user community forum\n\n\nRollout\nPhase 1: High-volume hospitals; Phase 2: Remaining facilities; Phase 3: Full operation\n\n\n\n\n\n\n\n\n9.1.8 Design Documentation\n\n9.1.8.1 What to Document\nDesign documentation bridges requirements and implementation:\n\n\n\n\n\n\n\n\nDocument\nBA Content\nPH Content\n\n\n\n\nArchitecture Document\nSystem components, technology stack\nIntervention components, delivery model\n\n\nInterface Specifications\nScreen layouts, navigation flows\nService user touchpoints\n\n\nIntegration Specifications\nAPIs, message formats\nData exchange protocols\n\n\nData Design\nDatabase schema, data flows\nData collection instruments\n\n\nSecurity Design\nAccess controls, encryption\nPrivacy protections, consent\n\n\nTransition Plan\nDeployment, training, support\nImplementation, capacity building\n\n\n\n\n\n\n9.1.9 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nSolution Architecture\nIntervention Framework\nDefine solution structure\n\n\nUI/UX Design\nService Delivery Model\nSpecify user experience\n\n\nIntegration Design\nHIE Specifications\nDefine system connections\n\n\nPrototype\nPilot Protocol\nValidate design\n\n\nChange Management Plan\nImplementation Strategy\nPrepare organization\n\n\n\n\n\n9.1.10 Moving Forward\nWith design complete, the next phase focuses on Implementation: building, deploying, and rolling out the solution while managing the organizational change required for adoption.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design & Solution Definition</span>"
    ]
  },
  {
    "objectID": "chapters/08-implementation.html",
    "href": "chapters/08-implementation.html",
    "title": "10  Implementation & Execution",
    "section": "",
    "text": "10.1 Implementation & Program Execution\nDesign becomes reality through implementation. In business analysis, this phase involves Solution Delivery and Change Management. In public health, it is Program Implementation guided by frameworks like PDSA (Plan-Do-Study-Act). Both require managing complexity while maintaining focus on outcomes.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implementation & Execution</span>"
    ]
  },
  {
    "objectID": "chapters/08-implementation.html#implementation-program-execution",
    "href": "chapters/08-implementation.html#implementation-program-execution",
    "title": "10  Implementation & Execution",
    "section": "",
    "text": "10.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSprint/Iteration\nPDSA Cycle\n\n\nRelease Management\nPhased Rollout\n\n\nUser Acceptance Testing\nPilot Evaluation\n\n\nGo-Live\nProgram Launch\n\n\nDefect Management\nVariance/Adverse Event Tracking\n\n\n\n\n\n10.1.2 The Double Loop of Agile in Public Health\nStandard Agile focuses on product improvement: Build → Measure → Learn. Public health adds a second loop: Surveillance → Intervention → Evaluation.\n\n\n\n\n\n\nflowchart LR\n    subgraph Agile[\"Agile Loop\"]\n        A1[Plan Sprint] --&gt; A2[Develop]\n        A2 --&gt; A3[Demo/Review]\n        A3 --&gt; A4[Retrospective]\n        A4 --&gt; A1\n    end\n    \n    subgraph PH[\"Public Health Loop\"]\n        P1[Plan] --&gt; P2[Do]\n        P2 --&gt; P3[Study]\n        P3 --&gt; P4[Act]\n        P4 --&gt; P1\n    end\n    \n    A3 &lt;-.-&gt;|Sync| P3\n\n\n\n\nFigure 10.1: The Double Loop: Agile + Public Health\n\n\n\n\n\nThe BA must ensure both loops are synchronized: software releases should align with epidemiological reporting cycles.\n\n\n10.1.3 Agile Practices Adapted\n\n10.1.3.1 Sprint Planning\nTraditional Agile:\n\nProduct owner prioritizes backlog\nTeam selects stories for sprint\nStories estimated in points\nSprint goal defined\n\nPublic Health Adaptation:\n\nProgram manager prioritizes based on grant milestones\nTeam considers reporting deadlines\nStories linked to program objectives\nSprint goal aligned with public health impact\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nSprint Goal (BA): Complete case search functionality and duplicate detection module.\nProgram Alignment (PH): This sprint supports NPCR Milestone 2: “Data quality infrastructure operational.” Completion enables Q2 data submission with duplicate resolution.\nSprint Backlog:\n\n\n\nStory\nPoints\nGrant Milestone\n\n\n\n\nCase search by patient ID\n5\nM2\n\n\nCase search by name/DOB\n3\nM2\n\n\nDuplicate candidate display\n5\nM2\n\n\nMerge duplicate records\n8\nM2\n\n\nAudit log for merges\n3\nCompliance\n\n\n\n\n\n\n\n10.1.3.2 PDSA Cycles\nPDSA provides a structured approach to continuous improvement:\n\n\n\n\n\n\n\n\nPhase\nActivities\nCancerSurv Example\n\n\n\n\nPlan\nDefine change, predict outcomes\n“Adding auto-population of demographics will reduce entry time by 2 minutes”\n\n\nDo\nImplement on small scale\nEnable feature for 3 pilot registrars\n\n\nStudy\nAnalyze results\nCompare entry times before/after; gather feedback\n\n\nAct\nAdopt, adapt, or abandon\nFeature reduced time by 1.5 minutes; adopt with UI adjustments\n\n\n\n\n\n10.1.3.3 Mapping Sprints to PDSA\n\n\n\nSprint Element\nPDSA Element\n\n\n\n\nSprint Planning\nPlan\n\n\nDevelopment\nDo\n\n\nSprint Review\nStudy\n\n\nRetrospective\nAct\n\n\nBacklog Refinement\nNext Plan cycle\n\n\n\n\n\n\n10.1.4 Testing in Health IT\n\n10.1.4.1 Testing Levels\n\n\n\n\n\n\n\n\nLevel\nBA Focus\nPH Focus\n\n\n\n\nUnit Testing\nCode functions correctly\nN/A (technical)\n\n\nIntegration Testing\nSystems connect properly\nData flows between systems\n\n\nSystem Testing\nFull system works\nEnd-to-end workflows function\n\n\nUser Acceptance Testing\nUsers approve functionality\nClinical workflows validated\n\n\nOperational Testing\nSystem performs under load\nHandles reporting surge periods\n\n\n\n\n\n10.1.4.2 UAT for Clinical Systems\nUser Acceptance Testing in public health requires clinical validation:\nStandard UAT:\n\nDoes the system do what was specified?\nDo users accept the interface?\nAre performance requirements met?\n\nClinical UAT Additions:\n\nDo clinical workflows function correctly?\nDoes data quality meet standards?\nDo edits align with NAACCR rules?\nIs patient safety protected?\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nUAT Test Case:\n\n\n\n\n\n\n\n\n\n\nID\nScenario\nSteps\nExpected Result\nPH Validation\n\n\n\n\nUAT-101\nDuplicate detection\nEnter case matching existing patient\nSystem flags potential duplicate\nMatches NAACCR duplicate resolution rules\n\n\nUAT-102\nStage validation\nEnter invalid stage combination\nSystem prevents save with error message\nError references SEER staging manual\n\n\nUAT-103\nNPCR export\nGenerate submission file\nValid NAACCR XML produced\nPasses CDC validator tool\n\n\n\n\n\n\n\n\n10.1.5 Managing Change\n\n10.1.5.1 Organizational Readiness\nImplementation fails without organizational change management:\nReadiness Assessment:\n\nLeadership commitment\nStaff capacity\nInfrastructure availability\nWorkflow adaptability\nCultural alignment\n\nCommon Barriers:\n\n\n\n\n\n\n\n\nBarrier\nBA Perspective\nPH Perspective\n\n\n\n\nResistance\nUsers prefer old system\n“Not how we’ve always done it”\n\n\nCapacity\nTraining time unavailable\nStaff already overburdened\n\n\nInfrastructure\nHardware/network issues\nRural sites lack bandwidth\n\n\nWorkflow\nProcess changes required\nClinical protocols affected\n\n\n\n\n\n10.1.5.2 Training Approaches\n\n\n\n\n\n\n\n\nApproach\nWhen to Use\nCancerSurv Example\n\n\n\n\nTrain-the-Trainer\nLarge, distributed user base\nRegistry supervisors trained first\n\n\nJust-in-Time\nComplex, infrequent tasks\nContext-sensitive help for staging\n\n\nSimulation\nHigh-stakes processes\nPractice mode for case abstraction\n\n\nPeer Support\nOngoing questions\nSuper-user network\n\n\n\n\n\n\n10.1.6 Deployment Strategies\n\n10.1.6.1 Phased vs. Big Bang\n\n\n\n\n\n\n\n\n\nStrategy\nPros\nCons\nWhen to Use\n\n\n\n\nBig Bang\nSingle cutover, consistent\nHigh risk, no rollback\nSimple systems, urgent deadlines\n\n\nPhased\nLower risk, lessons learned\nLonger timeline, parallel systems\nComplex systems, distributed users\n\n\nPilot\nReal-world validation\nLimited initial impact\nNew interventions, uncertain adoption\n\n\nParallel\nSafety net available\nResource intensive\nCritical systems, high risk tolerance\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nDeployment Strategy: Phased with Pilot\n\n\n\n\n\n\n\n\n\nPhase\nScope\nDuration\nSuccess Criteria\n\n\n\n\nPilot\n3 high-volume hospitals\n8 weeks\n&gt;90% user satisfaction; &lt;5% error rate\n\n\nPhase 1\nRemaining hospitals (20)\n12 weeks\nAll hospitals submitting data\n\n\nPhase 2\nPhysician offices, labs\n8 weeks\nELR feeds operational\n\n\nPhase 3\nFull operation, legacy decommission\n4 weeks\nLegacy system retired\n\n\n\n\n\n\n\n\n10.1.7 Monitoring During Implementation\n\n10.1.7.1 What to Track\n\n\n\n\n\n\n\n\nCategory\nBA Metrics\nPH Metrics\n\n\n\n\nAdoption\nLogin counts, feature usage\nSites trained, go-live completion\n\n\nPerformance\nResponse times, error rates\nData submission timeliness\n\n\nQuality\nDefect counts, resolution time\nData completeness, accuracy\n\n\nSatisfaction\nUser surveys, support tickets\nRegistrar feedback, NPS scores\n\n\nOutcomes\nFeature delivery, velocity\nGrant milestone achievement\n\n\n\n\n\n10.1.7.2 Issue Escalation\n\n\n\n\n\n\nflowchart TB\n    I[Issue Identified] --&gt; T{Severity?}\n    T --&gt;|Low| S1[Support Team]\n    T --&gt;|Medium| S2[Project Team]\n    T --&gt;|High| S3[Steering Committee]\n    T --&gt;|Critical| S4[Executive Sponsor]\n    \n    S1 --&gt; R[Resolution]\n    S2 --&gt; R\n    S3 --&gt; R\n    S4 --&gt; R\n\n\n\n\nFigure 10.2: Issue Escalation Path\n\n\n\n\n\n\n\n\n10.1.8 Communication During Implementation\n\n10.1.8.1 Stakeholder Updates\n\n\n\n\n\n\n\n\n\nAudience\nFrequency\nContent\nChannel\n\n\n\n\nExecutive Sponsors\nBi-weekly\nMilestone status, risks, decisions needed\nMeeting, dashboard\n\n\nProject Team\nDaily\nProgress, blockers, coordination\nStandup, chat\n\n\nEnd Users\nWeekly during rollout\nTraining, go-live dates, support\nEmail, newsletter\n\n\nExternal Partners\nAs needed\nIntegration status, requirements\nMeeting, documentation\n\n\n\n\n\n\n10.1.9 Deliverables from This Phase\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nWorking Software\nOperational Program\nDeliver the solution\n\n\nTest Results\nPilot Evaluation\nValidate quality\n\n\nTraining Materials\nCapacity Building Resources\nEnable users\n\n\nRelease Notes\nImplementation Updates\nCommunicate changes\n\n\nSupport Documentation\nOperational Guides\nEnable ongoing use\n\n\n\n\n\n10.1.10 Moving Forward\nWith the solution implemented, the next phase focuses on Evaluation: measuring outcomes, assessing value delivered, and identifying opportunities for continuous improvement.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implementation & Execution</span>"
    ]
  },
  {
    "objectID": "chapters/09-evaluation.html",
    "href": "chapters/09-evaluation.html",
    "title": "11  Evaluation & Improvement",
    "section": "",
    "text": "11.1 Evaluation & Continuous Improvement\nDid we solve the problem? Are outcomes improving? What should we do differently? In business analysis, this phase encompasses Solution Evaluation and Continuous Improvement. In public health, it maps to Program Evaluation using frameworks like the CDC Evaluation Framework. Both seek to measure value delivered and inform future action.",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluation & Improvement</span>"
    ]
  },
  {
    "objectID": "chapters/09-evaluation.html#evaluation-continuous-improvement",
    "href": "chapters/09-evaluation.html#evaluation-continuous-improvement",
    "title": "11  Evaluation & Improvement",
    "section": "",
    "text": "11.1.1 The Dual Framework\n\n\n\nBA Perspective\nPH Perspective\n\n\n\n\nSolution Evaluation\nProgram Evaluation\n\n\nKPI Tracking\nHealth Indicator Monitoring\n\n\nROI Analysis\nCost-Effectiveness Analysis\n\n\nLessons Learned\nAfter-Action Review\n\n\nContinuous Improvement\nQuality Improvement (QI)\n\n\n\n\n\n11.1.2 The CDC Evaluation Framework\nPublic health evaluation follows a well-established framework that parallels BA evaluation practices:\n\n\n\n\n\n\nflowchart LR\n    A[Engage&lt;br/&gt;Stakeholders] --&gt; B[Describe the&lt;br/&gt;Program]\n    B --&gt; C[Focus the&lt;br/&gt;Evaluation]\n    C --&gt; D[Gather Credible&lt;br/&gt;Evidence]\n    D --&gt; E[Justify&lt;br/&gt;Conclusions]\n    E --&gt; F[Ensure Use &&lt;br/&gt;Share Lessons]\n    F -.-&gt; A\n\n\n\n\nFigure 11.1: CDC Evaluation Framework Steps\n\n\n\n\n\n\n\n\nCDC Step\nBA Equivalent\n\n\n\n\nEngage Stakeholders\nIdentify evaluation stakeholders\n\n\nDescribe the Program\nDocument solution scope and objectives\n\n\nFocus the Evaluation\nDefine evaluation questions and scope\n\n\nGather Credible Evidence\nCollect metrics and feedback\n\n\nJustify Conclusions\nAnalyze data, determine value delivered\n\n\nEnsure Use and Share Lessons\nCommunicate results, inform decisions\n\n\n\n\n\n11.1.3 Types of Evaluation\n\n11.1.3.1 Formative vs. Summative\n\n\n\n\n\n\n\n\n\n\nType\nWhen\nPurpose\nBA Example\nPH Example\n\n\n\n\nFormative\nDuring implementation\nImprove the intervention\nSprint reviews, usability testing\nPDSA cycles, pilot feedback\n\n\nSummative\nAfter implementation\nJudge overall value\nPost-implementation review\nAnnual program evaluation\n\n\n\n\n\n11.1.3.2 Process vs. Outcome\n\n\n\n\n\n\n\n\n\nType\nFocus\nQuestions\nMetrics\n\n\n\n\nProcess\nHow well did we implement?\nWas the solution delivered as designed?\nAdoption rates, fidelity measures\n\n\nOutcome\nWhat difference did it make?\nDid we achieve intended results?\nHealth indicators, KPIs\n\n\nImpact\nLong-term effects\nWhat is the lasting change?\nPopulation health trends\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nProcess Evaluation:\n\nWere all registrars trained? (Target: 100%)\nAre hospitals submitting data electronically? (Target: 90%)\nIs the system meeting uptime requirements? (Target: 99.9%)\n\nOutcome Evaluation:\n\nHas data completeness improved? (Target: 89% → 95%)\nHas case abstraction time decreased? (Target: 15 min → 8 min)\nAre NPCR submissions timely? (Target: 100% on-time)\n\nImpact Evaluation:\n\nAre survival statistics more accurate?\nCan disparities be identified at the county level?\nHas the data informed state cancer plan priorities?\n\n\n\n\n\n\n11.1.4 Defining Metrics\n\n11.1.4.1 KPIs and Health Indicators\nMetrics should be SMART (Specific, Measurable, Achievable, Relevant, Time-bound):\n\n\n\nBA KPI\nPH Health Indicator\nMeasurement\n\n\n\n\nSystem uptime\nService availability\n% time operational\n\n\nUser adoption\nProgram reach\n% target users active\n\n\nTask completion time\nEfficiency\nMinutes per case abstraction\n\n\nError rate\nData quality\n% records with errors\n\n\nUser satisfaction\nAcceptability\nSurvey scores\n\n\n\n\n\n11.1.4.2 Building a Balanced Scorecard\nConsider multiple dimensions of value:\n\n\n\n\n\n\n\n\nDimension\nBA Metrics\nPH Metrics\n\n\n\n\nFinancial\nCost savings, ROI\nCost per case, grant compliance\n\n\nCustomer\nUser satisfaction, NPS\nRegistrar satisfaction, partner feedback\n\n\nInternal Process\nEfficiency gains, quality\nData completeness, timeliness\n\n\nLearning & Growth\nSkill development, innovation\nWorkforce capacity, continuous improvement\n\n\n\n\n\n\n11.1.5 Data Collection for Evaluation\n\n11.1.5.1 Sources of Evidence\n\n\n\n\n\n\n\n\nSource\nBA Application\nPH Application\n\n\n\n\nSystem Logs\nUsage analytics, performance data\nData submission tracking\n\n\nSurveys\nUser satisfaction, feature requests\nRegistrar feedback, partner surveys\n\n\nInterviews\nDetailed user feedback\nKey informant perspectives\n\n\nDocument Review\nProject artifacts, change logs\nReports, protocols\n\n\nObservation\nUsability testing\nWorkflow observation\n\n\nAdministrative Data\nSupport tickets, defects\nProgram records, health data\n\n\n\n\n\n11.1.5.2 Evaluation Plan Components\n\n\n\n\n\n\n\n\nComponent\nDescription\nCancerSurv Example\n\n\n\n\nQuestions\nWhat do we want to know?\nHas data quality improved?\n\n\nIndicators\nHow will we measure?\n% records passing NAACCR edits\n\n\nData Sources\nWhere will we get data?\nCancerSurv quality reports\n\n\nMethods\nHow will we collect?\nAutomated monthly extraction\n\n\nTimeline\nWhen will we measure?\nBaseline, 6 months, 12 months\n\n\nResponsibilities\nWho will do it?\nRegistry data quality manager\n\n\n\n\n\n\n11.1.6 Analysis and Interpretation\n\n11.1.6.1 Comparing to Baseline\nEffective evaluation requires baseline data:\n\n\n\n\n\n\nxychart-beta\n    title \"CancerSurv Data Completeness\"\n    x-axis [Baseline, Q1, Q2, Q3, Q4]\n    y-axis \"Completeness (%)\" 85 --&gt; 100\n    line [89, 91, 93, 94, 96]\n    line [95, 95, 95, 95, 95]\n\n\n\n\nFigure 11.2: Data Completeness Trend\n\n\n\n\n\n\n\n11.1.6.2 Interpreting Results\n\n\n\n\n\n\n\n\nResult\nInterpretation\nAction\n\n\n\n\nExceeds target\nSuccess; potential to raise bar\nDocument best practices; set stretch goals\n\n\nMeets target\nSuccess; sustain performance\nContinue current approach; monitor\n\n\nBelow target, improving\nProgress; maintain effort\nIdentify accelerators; address barriers\n\n\nBelow target, flat/declining\nConcern; intervention needed\nRoot cause analysis; corrective action\n\n\n\n\n\n\n11.1.7 Communicating Results\n\n11.1.7.1 Tailoring Messages\n\n\n\n\n\n\n\n\n\nAudience\nInterest\nFormat\nContent Emphasis\n\n\n\n\nExecutive sponsors\nBottom line, strategic alignment\nExecutive summary, dashboard\nROI, milestone achievement\n\n\nFunders (CDC, grants)\nCompliance, outcomes\nFormal reports\nGrant objective progress\n\n\nProject team\nDetailed performance\nWorking reports, retrospectives\nSpecific metrics, lessons learned\n\n\nEnd users\nHow it helps them\nNewsletters, town halls\nEfficiency gains, new features\n\n\n\n\n\n11.1.7.2 Visualization Best Practices\n\nUse clear, simple charts\nShow trends, not just snapshots\nCompare to targets/benchmarks\nHighlight key takeaways\nMake data accessible\n\n\n\n\n11.1.8 Continuous Improvement\n\n11.1.8.1 The QI Cycle\nEvaluation feeds continuous improvement:\n\n\n\n\n\n\nflowchart LR\n    A[Identify&lt;br/&gt;Opportunity] --&gt; B[Analyze Root&lt;br/&gt;Cause]\n    B --&gt; C[Design&lt;br/&gt;Improvement]\n    C --&gt; D[Implement&lt;br/&gt;Change]\n    D --&gt; E[Evaluate&lt;br/&gt;Results]\n    E --&gt; F{Successful?}\n    F --&gt;|Yes| G[Standardize]\n    F --&gt;|No| A\n    G --&gt; H[Monitor]\n    H --&gt; A\n\n\n\n\nFigure 11.3: Continuous Improvement Cycle\n\n\n\n\n\n\n\n11.1.8.2 Retrospectives and After-Action Reviews\n\n\n\n\n\n\n\n\nElement\nAgile Retrospective\nPH After-Action Review\n\n\n\n\nWhat went well?\nSprint successes\nProgram strengths\n\n\nWhat could improve?\nSprint challenges\nProgram gaps\n\n\nWhat will we do differently?\nAction items for next sprint\nRecommendations\n\n\nWho is responsible?\nTeam member assignments\nAction owners\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\n12-Month Evaluation Summary:\n\n\n\nMetric\nBaseline\nTarget\nActual\nStatus\n\n\n\n\nData completeness\n89%\n95%\n96%\n✅ Exceeded\n\n\nAbstraction time\n15 min\n8 min\n9 min\n⚠️ Close\n\n\nUser satisfaction\nN/A\n80%\n85%\n✅ Exceeded\n\n\nNPCR submission\n85% on-time\n100%\n100%\n✅ Met\n\n\nSystem uptime\nN/A\n99.9%\n99.7%\n⚠️ Close\n\n\n\nKey Findings:\n\nData quality improvements exceeded expectations\nAbstraction time reduced but not to target; workflow analysis needed\nTwo outages impacted uptime; infrastructure improvements planned\n\nRecommendations:\n\nContinue current data quality processes\nConduct workflow study to identify remaining abstraction bottlenecks\nImplement redundant infrastructure for high availability\n\n\n\n\n\n\n11.1.9 Sustaining Value\n\n11.1.9.1 From Project to Operations\nEvaluation supports the transition from project mode to operations:\n\n\n\nProject Phase\nOperational Phase\n\n\n\n\nProject team manages\nOperations team manages\n\n\nChange requests\nEnhancement requests\n\n\nImplementation metrics\nOperational metrics\n\n\nGo-live success\nOngoing performance\n\n\nProject budget\nOperating budget\n\n\n\n\n\n11.1.9.2 Governance for Continuous Improvement\nEstablish ongoing governance:\n\nRegular metric reviews (monthly/quarterly)\nUser feedback channels\nEnhancement prioritization process\nPerformance monitoring\nPeriodic comprehensive evaluations\n\n\n\n\n11.1.10 Deliverables from This Phase\n\n\n\n\n\n\n\n\nBA Deliverable\nPH Deliverable\nPurpose\n\n\n\n\nSolution Evaluation Report\nProgram Evaluation Report\nDocument outcomes\n\n\nLessons Learned\nAfter-Action Review\nCapture knowledge\n\n\nPerformance Dashboard\nHealth Indicator Dashboard\nMonitor ongoing performance\n\n\nImprovement Recommendations\nQI Action Plan\nDrive continuous improvement\n\n\nTransition Documentation\nSustainability Plan\nEnable long-term success\n\n\n\n\n\n11.1.11 Moving Forward\nWith the core analysis process complete, the following chapters provide additional resources: tools comparison (Chapter 10), implementation science frameworks (Chapter 11), templates (Appendix A), and a comprehensive glossary (Appendix C).",
    "crumbs": [
      "The Analysis Process",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluation & Improvement</span>"
    ]
  },
  {
    "objectID": "chapters/10-tools.html",
    "href": "chapters/10-tools.html",
    "title": "12  Tools Comparison",
    "section": "",
    "text": "12.1 Commercial vs. Open Source/Public Health Tools\nPublic health agencies often operate with constrained budgets while managing sensitive health data. This chapter compares commercial enterprise tools with open source and public health-specific alternatives, helping you select the right tools for your context.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tools Comparison</span>"
    ]
  },
  {
    "objectID": "chapters/10-tools.html#commercial-vs.-open-sourcepublic-health-tools",
    "href": "chapters/10-tools.html#commercial-vs.-open-sourcepublic-health-tools",
    "title": "12  Tools Comparison",
    "section": "",
    "text": "12.1.1 Selection Criteria\nWhen evaluating tools, consider:\n\n\n\nCriterion\nCommercial Advantage\nOSS/PH Advantage\n\n\n\n\nCost\nPredictable licensing\nNo license fees\n\n\nSupport\nVendor SLAs\nCommunity + self-reliance\n\n\nFeatures\nPolished, integrated\nCustomizable, extensible\n\n\nCompliance\nOften pre-certified\nFull control over data\n\n\nData Sovereignty\nVendor-managed\nOrganization-controlled\n\n\nSustainability\nVendor roadmap\nCommunity-driven\n\n\n\n\n\n12.1.2 Tool Categories\n\n12.1.2.1 Project Management\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nFull PM Suite\nJira, Azure DevOps, MS Project\nOpenProject, Redmine, Taiga\n\n\nKanban Boards\nTrello (paid), Monday.com\nTrello (free tier), Wekan, Kanboard\n\n\nAgile Planning\nJira, Rally, VersionOne\nTaiga, OpenProject\n\n\nGrant Management\nSmartsheet, Asana\nOpenProject with custom fields\n\n\n\nRecommendation for Public Health:\n\nSmall teams (&lt;10): Trello free tier or Taiga for simple Kanban/Scrum\nLarger programs: OpenProject for full PM capabilities with data sovereignty\nCDC/Federal projects: Often require Azure DevOps or Jira per contract\n\n\n\n\n\n\n\nTipWhen to Choose OSS\n\n\n\nChoose open source when:\n\nBudget is constrained\nData sovereignty is critical (cannot store project data externally)\nTechnical staff can support installation and maintenance\nCustomization is needed beyond commercial options\n\n\n\n\n\n12.1.2.2 Requirements and Documentation\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nWiki/Docs\nConfluence, SharePoint\nBookStack, MediaWiki, GitHub Wiki\n\n\nRequirements Management\nJama, Helix RM, DOORS\nGitHub Issues, GitLab, Notion (free)\n\n\nCollaborative Editing\nMS 365, Google Workspace\nNextcloud, CryptPad, HedgeDoc\n\n\n\nRecommendation for Public Health:\n\nDocumentation: BookStack provides Confluence-like experience without licensing\nRequirements: GitHub Issues sufficient for most projects; integrates with development\nCollaboration: Consider data sensitivity; Nextcloud for on-premise control\n\n\n\n12.1.2.3 Diagramming\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nGeneral Diagramming\nVisio, Lucidchart\ndiagrams.net (draw.io), Mermaid\n\n\nProcess Modeling (BPMN)\nVisio, Bizagi\ndiagrams.net, Camunda Modeler\n\n\nArchitecture\nLucidchart, Visio\ndiagrams.net, PlantUML\n\n\n\nRecommendation for Public Health:\n\ndiagrams.net is the de facto standard in public sector: free, web-based, exports to multiple formats, works offline\nMermaid for diagrams in documentation (renders from text, version-controllable)\n\n\n\n12.1.2.4 Data Collection\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nSurveys\nQualtrics, SurveyMonkey\nLimeSurvey, KoBoToolbox\n\n\nClinical/Research Data\nREDCap (free for research)\nREDCap, ODK, DHIS2\n\n\nForms\nMicrosoft Forms, Google Forms\nKoBoToolbox, ODK Collect\n\n\nCase Management\nSalesforce\nDHIS2, CommCare\n\n\n\nREDCap: The Public Health Standard\nREDCap (Research Electronic Data Capture) deserves special mention:\n\nFree for non-profit research institutions\nHIPAA-compliant, 21 CFR Part 11 capable\nSupports complex branching logic, validation\nBuilt-in audit trails\nConsortium of 6,000+ institutions\nCDC and NIH approved\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nFor the CancerSurv project, data collection tools include:\n\nREDCap: Pilot site feedback surveys, user satisfaction assessments\nKoBoToolbox: Field data collection for mobile cancer screening events\nNative CancerSurv: Case abstraction (built into the platform)\n\n\n\n\n\n12.1.2.5 Data Analysis\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nStatistical Analysis\nSAS, SPSS, Stata\nR, Python (pandas, scipy)\n\n\nEpidemiological Analysis\nSAS, Stata\nR (epitools), Epi Info\n\n\nData Wrangling\nAlteryx, Trifacta\nR (tidyverse), Python (pandas)\n\n\nNotebooks\nDatabricks, SAS Studio\nJupyter, RStudio, Quarto\n\n\n\nEpi Info: CDC’s Free Epidemiology Tool\nEpi Info is developed by CDC specifically for outbreak investigation:\n\nFree download, no installation fees\nBuilt-in epidemiological statistics (odds ratios, relative risks)\nEpidemic curve generation\nGeographic mapping\nSurvey development and analysis\n7-day moving averages, case fatality rates\n\nR for Public Health\nR has become the standard for public health analytics:\n# Example: Calculate age-adjusted incidence rate\nlibrary(epitools)\nlibrary(tidyverse)\n\ncancer_data %&gt;%\n  group_by(county, year) %&gt;%\n  summarize(\n    cases = n(),\n    population = first(population),\n    crude_rate = cases / population * 100000\n  ) %&gt;%\n  # Age adjustment using standard population\n  ageadjust.direct(count = cases, pop = population, stdpop = us_std_pop)\n\n\n12.1.2.6 Visualization\n\n\n\n\n\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nDashboards\nTableau, Power BI\nR Shiny, Dash (Python), Apache Superset\n\n\nStatic Visualization\nTableau, Excel\nR (ggplot2), Python (matplotlib, plotly)\n\n\nInteractive Charts\nTableau, Power BI\nPlotly, Highcharts (free for non-commercial)\n\n\n\nR Shiny for Public Health Dashboards\nR Shiny enables interactive dashboards without JavaScript expertise:\n\nFree and open source\nIntegrates with R analysis pipelines\nCan be deployed on-premise or Shinyapps.io\nMany public health templates available\n\n\n\n12.1.2.7 GIS and Mapping\n\n\n\nCapability\nCommercial Options\nOSS/PH Options\n\n\n\n\nDesktop GIS\nArcGIS Pro\nQGIS\n\n\nWeb Mapping\nArcGIS Online, Mapbox\nLeaflet, OpenLayers\n\n\nSpatial Analysis\nArcGIS, ESRI\nQGIS, R (sf package), PostGIS\n\n\nGeocoding\nGoogle, ESRI\nNominatim, US Census Geocoder\n\n\n\nQGIS for Disease Mapping\nQGIS is essential for spatial epidemiology:\n\nFree, cross-platform\nFull-featured GIS capabilities\nDisease mapping and cluster detection\nIntegrates with R for spatial statistics\nActive public health user community\n\n\n\n\n\n\n\nNoteCancerSurv Example\n\n\n\nCancerSurv analytics stack:\n\n\n\n\n\n\n\n\nFunction\nTool\nRationale\n\n\n\n\nCase data storage\nPostgreSQL\nOpen source, HIPAA-capable\n\n\nETL/Data pipeline\nApache Airflow\nOrchestration of data flows\n\n\nStatistical analysis\nR (tidyverse, survival)\nStandard for cancer epidemiology\n\n\nDashboards\nR Shiny\nInteractive, deployable on-premise\n\n\nGeographic mapping\nQGIS + Leaflet\nCancer cluster visualization\n\n\nAd-hoc queries\nApache Superset\nSelf-service for epidemiologists\n\n\n\n\n\n\n\n12.1.2.8 Data Standards and Interoperability\n\n\n\nStandard\nCommercial Tools\nOSS Tools\n\n\n\n\nHL7 FHIR\nRhapsody, Corepoint\nHAPI FHIR, LinuxForHealth\n\n\nHL7 v2.x\nRhapsody, Mirth\nMirth Connect (open source), HAPI\n\n\nCDA/C-CDA\nVarious EHR vendors\nMDHT, Reference CDA\n\n\n\nMirth Connect\nMirth Connect is widely used in public health for health information exchange:\n\nOpen source (NextGen Healthcare)\nHL7 v2, FHIR, CDA support\nVisual interface builder\nUsed by many state health departments\n\n\n\n\n12.1.3 Building Your Stack\n\n12.1.3.1 Small Public Health Program\n\n\n\nFunction\nRecommended Tool\nNotes\n\n\n\n\nProject Management\nTrello or Taiga\nFree tier sufficient\n\n\nDocumentation\nGitHub Wiki or BookStack\nVersion-controlled\n\n\nDiagramming\ndiagrams.net\nFree, export to any format\n\n\nData Collection\nREDCap\nStandard for research\n\n\nAnalysis\nR + RStudio\nFree, extensive packages\n\n\nVisualization\nR Shiny or Excel\nDepends on technical capacity\n\n\n\n\n\n12.1.3.2 Large State Health Department\n\n\n\nFunction\nRecommended Tool\nNotes\n\n\n\n\nProject Management\nAzure DevOps or OpenProject\nEnterprise scale\n\n\nDocumentation\nConfluence or BookStack\nTeam collaboration\n\n\nRequirements\nJira or GitHub\nIntegrated with development\n\n\nData Collection\nREDCap + DHIS2\nResearch + program monitoring\n\n\nData Platform\nPostgreSQL + Airflow\nScalable, HIPAA-capable\n\n\nAnalysis\nR + Python\nComprehensive capabilities\n\n\nVisualization\nR Shiny + Superset\nDashboards + self-service\n\n\nGIS\nQGIS + PostGIS\nFull spatial capabilities\n\n\nIntegration\nMirth Connect\nHL7/FHIR integration\n\n\n\n\n\n\n12.1.4 Considerations for Tool Selection\n\n12.1.4.1 Total Cost of Ownership\nFree software is not always cheaper:\n\n\n\nCost Factor\nCommercial\nOpen Source\n\n\n\n\nLicense fees\nYes\nNo\n\n\nImplementation\nVendor/partner\nInternal/consultant\n\n\nTraining\nOften included\nSelf-directed or purchased\n\n\nSupport\nIncluded in license\nCommunity or purchased\n\n\nCustomization\nLimited\nUnlimited but costly\n\n\nInfrastructure\nCloud included or on-prem\nYou manage\n\n\n\n\n\n12.1.4.2 Compliance and Security\n\n\n\n\n\n\n\n\nConsideration\nCommercial\nOpen Source\n\n\n\n\nHIPAA compliance\nOften certified\nYour responsibility to configure\n\n\nSOC 2 certification\nCommon\nRare; your responsibility\n\n\nSecurity updates\nVendor manages\nYou monitor and apply\n\n\nAudit trails\nBuilt-in\nMay require configuration\n\n\n\n\n\n12.1.4.3 Sustainability\nConsider long-term viability:\n\nCommercial: Vendor may be acquired, change pricing, sunset product\nOpen Source: Community may lose momentum; check activity levels\nHybrid: Consider tools with both commercial and open source options\n\n\n\n\n12.1.5 Summary\nThe choice between commercial and open source tools depends on your context: budget, technical capacity, data sensitivity, and compliance requirements. Public health has excellent open source options, particularly for data collection (REDCap), analysis (R), mapping (QGIS), and integration (Mirth Connect). Evaluate total cost of ownership, not just license fees.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tools Comparison</span>"
    ]
  },
  {
    "objectID": "chapters/11-implementation-science.html",
    "href": "chapters/11-implementation-science.html",
    "title": "13  Implementation Science",
    "section": "",
    "text": "13.1 CFIR and Implementation Frameworks\nWhy do evidence-based interventions fail to achieve expected outcomes when deployed in the real world? Implementation science provides the answer: the gap between efficacy (works under ideal conditions) and effectiveness (works in practice) is bridged by understanding implementation context. This chapter introduces key frameworks that help business analysts anticipate and address adoption barriers.",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implementation Science</span>"
    ]
  },
  {
    "objectID": "chapters/11-implementation-science.html#cfir-and-implementation-frameworks",
    "href": "chapters/11-implementation-science.html#cfir-and-implementation-frameworks",
    "title": "13  Implementation Science",
    "section": "",
    "text": "13.1.1 Why Implementation Science Matters for BA\nTraditional requirements focus on what a system must do. Implementation science asks: Will people actually use it? This question is critical because:\n\n70% of change initiatives fail to achieve their objectives\nClinical guidelines take an average of 17 years to become standard practice\nTechnology adoption depends on factors beyond functionality\n\nFor the business analyst, implementation science provides:\n\nA structured way to assess organizational readiness\nLanguage for discussing non-technical barriers with stakeholders\nFrameworks for designing implementation strategies\nMetrics for measuring adoption, not just deployment\n\n\n\n13.1.2 The Consolidated Framework for Implementation Research (CFIR)\nCFIR is the most widely used implementation science framework. It organizes factors influencing implementation into five domains:\n\n\n\n\n\n\nflowchart TB\n    subgraph CFIR[\"CFIR Framework\"]\n        A[Intervention&lt;br/&gt;Characteristics]\n        B[Outer&lt;br/&gt;Setting]\n        C[Inner&lt;br/&gt;Setting]\n        D[Characteristics&lt;br/&gt;of Individuals]\n        E[Process]\n    end\n    \n    A --&gt; F[Implementation&lt;br/&gt;Outcome]\n    B --&gt; F\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n\n\n\n\nFigure 13.1: CFIR Domains\n\n\n\n\n\n\n13.1.2.1 Domain 1: Intervention Characteristics\nProperties of the intervention itself that influence adoption:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nIntervention Source\nPerception of whether intervention is externally vs internally developed\nInvolve users in design; customize for local context\n\n\nEvidence Strength\nStakeholders’ perception of evidence supporting the intervention\nDocument benefits; reference standards (CDC, NAACCR)\n\n\nRelative Advantage\nPerception that the intervention is better than current practice\nQuantify improvements; demonstrate in pilot\n\n\nAdaptability\nDegree to which intervention can be modified for local needs\nBuild configurability; separate core from periphery\n\n\nTrialability\nAbility to test on a small scale\nSupport pilot deployments; sandbox environments\n\n\nComplexity\nPerceived difficulty of implementation\nSimplify UI; phase rollout; provide training\n\n\nDesign Quality\nPerceived excellence in how intervention is presented\nInvest in UX; professional appearance\n\n\nCost\nCosts of implementation and ongoing operation\nDocument TCO; demonstrate ROI\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example: Intervention Characteristics\n\n\n\n\n\n\n\n\n\n\n\nConstruct\nAssessment\nDesign Response\n\n\n\n\nRelative Advantage\nHigh: modern UI, remote access, better analytics\nEmphasize in training; demonstrate side-by-side\n\n\nComplexity\nMedium: new workflows, new interface\nPhased training; role-based simplified views\n\n\nAdaptability\nMedium: some local customization needed\nConfigurable report templates; custom fields\n\n\nTrialability\nHigh: pilot sites planned\n8-week pilot with 3 hospitals\n\n\n\n\n\n\n\n13.1.2.2 Domain 2: Outer Setting\nExternal context influencing the implementing organization:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nPatient/Community Needs\nExtent to which needs are known and prioritized\nGather community input; equity analysis\n\n\nCosmopolitanism\nDegree of networking with external organizations\nPlan for interoperability; support data sharing\n\n\nPeer Pressure\nCompetitive pressure from peer organizations\nReference successful implementations elsewhere\n\n\nExternal Policies\nExternal mandates, regulations, guidelines\nDocument compliance requirements early\n\n\n\nFor public health IT projects, outer setting often includes:\n\nCDC reporting requirements\nHIPAA regulations\nState health information exchange policies\nGrant funder expectations\nNAACCR standards (for cancer registries)\n\n\n\n13.1.2.3 Domain 3: Inner Setting\nInternal organizational context:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nStructural Characteristics\nOrganization size, maturity, structure\nAssess readiness; tailor approach\n\n\nNetworks & Communications\nInformation flow within organization\nPlan communication strategy\n\n\nCulture\nNorms, values, assumptions\nAlign with organizational culture\n\n\nImplementation Climate\nReceptivity to change\nAssess readiness; address resistance\n\n\nReadiness for Implementation\nTangible indicators of commitment\nSecure resources, leadership support\n\n\n\nAssessing Implementation Climate:\n\nIs there leadership commitment?\nAre resources allocated?\nIs there a sense of urgency?\nAre staff held accountable for adoption?\nAre early adopters rewarded?\n\n\n\n13.1.2.4 Domain 4: Characteristics of Individuals\nAttributes of people involved in implementation:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nKnowledge & Beliefs\nAttitudes toward the intervention\nEducation, demonstration, testimonials\n\n\nSelf-Efficacy\nConfidence in ability to use intervention\nTraining, support resources, simplification\n\n\nIndividual Stage of Change\nReadiness to adopt\nTailored engagement by readiness level\n\n\nIndividual Identification\nRelationship with organization\nLeverage organizational loyalty\n\n\n\n\n\n13.1.2.5 Domain 5: Process\nThe implementation process itself:\n\n\n\n\n\n\n\n\nConstruct\nDefinition\nBA/Requirements Implication\n\n\n\n\nPlanning\nDegree to which implementation is planned\nDetailed implementation plan\n\n\nEngaging\nAttracting and involving appropriate people\nStakeholder engagement strategy\n\n\nExecuting\nCarrying out implementation as planned\nProject management, monitoring\n\n\nReflecting & Evaluating\nFeedback about progress\nPDSA cycles, metrics, retrospectives\n\n\n\nKey Roles in Process:\n\nChampions: Individuals who advocate for the intervention\nOpinion Leaders: Respected individuals who influence peers\nImplementation Leaders: Those formally responsible\nExternal Change Agents: Consultants, vendors supporting implementation\n\n\n\n\n13.1.3 Mapping NFRs to CFIR\nA practical application of CFIR is translating non-functional requirements into implementation characteristics:\n\n\n\n\n\n\n\n\nNFR Category\nCFIR Mapping\nRequirement Example\n\n\n\n\nPerformance\nComplexity, Design Quality\n“Response time &lt;3 seconds to maintain workflow efficiency”\n\n\nUsability\nComplexity, Self-Efficacy\n“Interface requires &lt;4 hours training for basic proficiency”\n\n\nReliability\nRelative Advantage\n“99.9% uptime to maintain user confidence”\n\n\nScalability\nAdaptability\n“Support 2x current case volume for outbreak surge”\n\n\nSecurity\nExternal Policies\n“HIPAA-compliant access controls”\n\n\nInteroperability\nCosmopolitanism\n“HL7 FHIR APIs for health information exchange”\n\n\nAccessibility\nSelf-Efficacy\n“WCAG 2.1 AA compliance; support for screen readers”\n\n\n\n\n\n13.1.4 RE-AIM Framework\nRE-AIM provides a complementary framework focused on public health impact1,2:\n\n\n\n\n\n\n\n\nDimension\nDefinition\nMetric Examples\n\n\n\n\nReach\nProportion of target population participating\n% registrars using system; % facilities connected\n\n\nEffectiveness\nImpact on outcomes\nData completeness; abstraction time\n\n\nAdoption\nProportion of settings/staff adopting\n% hospitals submitting electronically\n\n\nImplementation\nFidelity to protocol; consistency\nAdherence to data standards; training completion\n\n\nMaintenance\nSustainability over time\nContinued use at 12 months; staff turnover impact\n\n\n\n\n\n\n\n\n\nNoteCancerSurv Example: RE-AIM Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nIndicator\nTarget\nActual (12 mo)\n\n\n\n\nReach\n% registrars trained\n100%\n98%\n\n\nEffectiveness\nData completeness\n95%\n96%\n\n\nAdoption\n% hospitals on ELR\n90%\n87%\n\n\nImplementation\nTraining completion rate\n95%\n92%\n\n\nMaintenance\nActive users at 12 mo\n90%\n94%\n\n\n\n\n\n\n\n13.1.5 Applying Implementation Science in Practice\n\n13.1.5.1 During Planning\n\nConduct CFIR-based readiness assessment\nIdentify potential barriers across all domains\nDesign implementation strategies to address barriers\n\n\n\n13.1.5.2 During Design\n\nEnsure intervention characteristics support adoption\nBuild in adaptability for local context\nMinimize complexity; maximize relative advantage\n\n\n\n13.1.5.3 During Implementation\n\nEngage champions and opinion leaders\nMonitor adoption, not just deployment\nUse PDSA cycles to address emerging barriers\n\n\n\n13.1.5.4 During Evaluation\n\nAssess both implementation outcomes and intervention outcomes\nUse RE-AIM dimensions for comprehensive evaluation\nDocument lessons for future implementations\n\n\n\n\n13.1.6 Implementation Strategies\nWhen barriers are identified, select appropriate implementation strategies:\n\n\n\n\n\n\n\n\nBarrier\nStrategy Category\nExample Strategies\n\n\n\n\nLack of knowledge\nTraining & Education\nWorkshops, e-learning, job aids\n\n\nLow self-efficacy\nSupport\nHelp desk, super-users, mentoring\n\n\nResistance to change\nStakeholder Engagement\nChampions, leadership messaging\n\n\nWorkflow disruption\nPlanning\nPhased rollout, parallel operation\n\n\nResource constraints\nInfrastructure\nDedicated staff, protected time\n\n\nComplexity\nIntervention Modification\nSimplified views, guided workflows\n\n\n\n\n\n13.1.7 Summary\nImplementation science provides business analysts with frameworks to anticipate and address the human and organizational factors that determine whether a technically sound solution actually achieves its intended outcomes. By incorporating CFIR assessment into requirements gathering and using RE-AIM for evaluation, hybrid BA/PH projects can bridge the gap between deployment and adoption.\nKey takeaways:\n\nRequirements must address adoption, not just functionality\nCFIR provides a comprehensive lens for assessing implementation context\nNFRs should map to implementation characteristics\nRE-AIM offers a framework for evaluating public health impact\nImplementation strategies should target specific barriers\n\n\n\n\n\n\n\n1. RE-AIM – Home – Reach Effectiveness Adoption Implementation Maintenance [Internet]. [cited 2026 Jan 9]. Available from: https://re-aim.org/\n\n\n2. 1999 – RE-AIM [Internet]. [cited 2026 Jan 9]. Available from: https://re-aim.org/1999/",
    "crumbs": [
      "Putting It Into Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implementation Science</span>"
    ]
  },
  {
    "objectID": "chapters/A-templates.html",
    "href": "chapters/A-templates.html",
    "title": "14  Templates & Checklists",
    "section": "",
    "text": "14.1 Templates and Checklists\nThis appendix provides ready-to-use templates for hybrid BA/PH projects. Each template is presented in both BA and PH framings where appropriate.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Templates & Checklists</span>"
    ]
  },
  {
    "objectID": "chapters/A-templates.html#templates-and-checklists",
    "href": "chapters/A-templates.html#templates-and-checklists",
    "title": "14  Templates & Checklists",
    "section": "",
    "text": "14.1.1 Stakeholder / Community Partner Matrix\nUse this template to identify and analyze project participants:\n\n\n\n\n\n\n\n\n\n\n\nName/Group\nBA Role\nPH Role\nInterest Level\nInfluence Level\nEngagement Strategy\n\n\n\n\nRegistry Director\nExecutive Sponsor\nState Epidemiologist\nHigh\nHigh\nMonthly briefings, decision authority\n\n\nCancer Registrars\nEnd Users\nFrontline Data Collectors\nHigh\nMedium\nTraining, feedback sessions, super-users\n\n\nHospital IT\nTechnical SMEs\nData Partners\nMedium\nMedium\nIntegration meetings, testing\n\n\nCDC/NPCR\nGovernance\nFunder/Standards Body\nHigh\nHigh\nFormal reporting, compliance reviews\n\n\nCancer Survivors\nIndirect Stakeholders\nRights Holders/Beneficiaries\nLow\nLow\nAdvisory input, outcome focus\n\n\n\n\n14.1.1.1 Analysis Questions\n\nWho might be missing from this list?\nAre marginalized voices represented?\nWho has formal authority vs. informal influence?\nWhat are the potential conflicts of interest?\n\n\n\n\n14.1.2 Logic Model Template\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              LOGIC MODEL                                     │\n│  Program: _______________________________________________                    │\n│  Date: __________________________________________________                    │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│  INPUTS              ACTIVITIES           OUTPUTS            OUTCOMES        │\n│  (Resources)         (What we do)         (Products)         (Changes)       │\n│                                                                              │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐       ┌──────────┐     │\n│  │          │       │          │        │          │       │ Short-   │     │\n│  │ Funding  │──────▶│ Develop  │───────▶│ Working  │──────▶│ term     │     │\n│  │          │       │ system   │        │ software │       │          │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                   │          │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐              ▼          │\n│  │          │       │          │        │          │       ┌──────────┐     │\n│  │ Staff    │──────▶│ Train    │───────▶│ Trained  │──────▶│ Medium-  │     │\n│  │          │       │ users    │        │ users    │       │ term     │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                   │          │\n│  ┌──────────┐       ┌──────────┐        ┌──────────┐              ▼          │\n│  │          │       │          │        │          │       ┌──────────┐     │\n│  │ Data     │──────▶│ Process  │───────▶│ Quality  │──────▶│ Long-    │     │\n│  │ feeds    │       │ data     │        │ reports  │       │ term     │     │\n│  └──────────┘       └──────────┘        └──────────┘       └──────────┘     │\n│                                                                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  ASSUMPTIONS:                          EXTERNAL FACTORS:                     │\n│  _________________________________     _________________________________     │\n│  _________________________________     _________________________________     │\n│  _________________________________     _________________________________     │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n14.1.2.1 CancerSurv Logic Model Example\n\n\n\n\n\n\n\n\n\n\nInputs\nActivities\nOutputs\nShort-term Outcomes\nLong-term Impact\n\n\n\n\nNPCR grant funding\nSystem development\nOperational platform\nImproved workflow efficiency\nAccurate survival statistics\n\n\nRegistry staff\nUser training\nTrained registrars\nReduced abstraction time\nTargeted prevention\n\n\nHospital data feeds\nData integration\nElectronic data capture\nIncreased data completeness\nHealth disparity identification\n\n\nNAACCR standards\nQuality assurance\nValidated records\nTimely NPCR submission\nEvidence-based cancer policy\n\n\n\n\n\n\n14.1.3 Requirements Specification Template\n\n14.1.3.1 Functional Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nFR-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nDescription\nThe system shall [action] [object] [condition/constraint]\n\n\nRationale\n[Why this requirement exists; link to business need/program goal]\n\n\nSource\n[Stakeholder, document, or session where identified]\n\n\nPriority\nMust / Should / Could / Won’t (this release)\n\n\nAcceptance Criteria\nGiven [context], when [action], then [expected result]\n\n\nDependencies\n[Other requirements this depends on or enables]\n\n\n\n\n\n14.1.3.2 Non-Functional Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nNFR-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nCategory\nPerformance / Security / Usability / Reliability / Scalability / Compliance\n\n\nDescription\nThe system shall [quality attribute] [measurable target]\n\n\nRationale\n[Why this matters; CFIR domain if applicable]\n\n\nMeasurement\n[How compliance will be verified]\n\n\nPriority\nMust / Should / Could\n\n\n\n\n\n14.1.3.3 Example: CancerSurv Requirement\n\n\n\n\n\n\n\nField\nContent\n\n\n\n\nID\nFR-105\n\n\nTitle\nDuplicate Case Detection\n\n\nDescription\nThe system shall identify potential duplicate case records based on patient identifiers (name, DOB, SSN) and tumor characteristics\n\n\nRationale\nPrevents double-counting of cancer incidence; required for accurate epidemiological analysis\n\n\nSource\nRegistrar interviews; NAACCR standards\n\n\nPriority\nMust\n\n\nAcceptance Criteria\nGiven a new case entry, when patient identifiers match an existing case with confidence &gt;80%, then the system displays potential duplicates for review before saving\n\n\nDependencies\nFR-101 (Patient search)\n\n\n\n\n\n\n14.1.4 CFIR Readiness Assessment Checklist\nUse this checklist to assess implementation readiness:\n\n14.1.4.1 Intervention Characteristics\n\nEvidence of effectiveness documented and communicated\nRelative advantage over current practice clearly articulated\nCore vs. adaptable components identified\nComplexity minimized; training plan addresses remaining complexity\nPilot/trialability planned\n\n\n\n14.1.4.2 Outer Setting\n\nRegulatory requirements identified (HIPAA, CDC, state)\nExternal stakeholder needs understood\nInteroperability requirements documented\nPeer organization experiences reviewed\n\n\n\n14.1.4.3 Inner Setting\n\nLeadership commitment secured\nResources allocated (budget, staff, time)\nImplementation team identified\nCommunication plan developed\nOrganizational culture alignment assessed\n\n\n\n14.1.4.4 Characteristics of Individuals\n\nUser attitudes toward change assessed\nTraining needs identified\nChampions identified and engaged\nSelf-efficacy barriers addressed\n\n\n\n14.1.4.5 Process\n\nDetailed implementation plan developed\nKey milestones and decision points defined\nMonitoring and feedback mechanisms established\nContingency plans for common risks\n\n\n\n\n14.1.5 User Story Templates\n\n14.1.5.1 Standard Format\n\nAs a [role], I want [feature/capability], so that [benefit/value].\n\n\n\n14.1.5.2 GPS Format (Clinical Contexts)\n\nGiven [clinical context/trigger], the [health worker role] should [specific action] to [health outcome].\n\n\n\n14.1.5.3 Service-User Scenario Format\n\n[Name], a [demographic description], [presents with situation]. The system must [support their need] while [addressing constraints]. Success means [outcome].\n\n\n\n14.1.5.4 Example Set: CancerSurv\nStandard: &gt; As a cancer registrar, I want to search existing cases before creating a new record, so that I avoid creating duplicate entries.\nGPS: &gt; Given a new pathology report, the registrar should search for existing patient records using at least two identifiers, to prevent duplicate case creation and ensure accurate incidence counts.\nService-User Scenario: &gt; Maria is a senior registrar abstracting cases from a high-volume hospital. She receives 50 pathology reports daily and needs to quickly determine if each represents a new case or an existing patient. The system must support rapid searching with fuzzy matching while flagging potential duplicates. Success means Maria can process her daily queue in under 4 hours with less than 1% duplicate creation rate.\n\n\n\n14.1.6 Test Case Template\n\n\n\nField\nContent\n\n\n\n\nID\nTC-XXX\n\n\nTitle\n[Brief descriptive title]\n\n\nRequirement(s)\n[FR-XXX, NFR-XXX]\n\n\nPreconditions\n[System state before test]\n\n\nTest Data\n[Specific data needed]\n\n\nSteps\n1. [Action] 2. [Action] 3. [Action]\n\n\nExpected Result\n[What should happen]\n\n\nActual Result\n[Fill in during testing]\n\n\nPass/Fail\n[ ]\n\n\nNotes\n[Observations, defect IDs if failed]\n\n\n\n\n\n14.1.7 Project Charter Template (Dual-Frame)\n\n\n\n\n\n\n\n\nSection\nBA Content\nPH Content\n\n\n\n\nProject Name\n[System name]\n[Program name]\n\n\nBusiness Need\n[Operational driver]\nPublic Health Challenge: [Health need]\n\n\nObjectives\n[Business objectives]\nProgram Goals: [Health objectives]\n\n\nScope\n[In/out of scope]\nIntervention Boundaries: [Target population, geography]\n\n\nStakeholders\n[Stakeholder list]\nCommunity Partners: [Partner list]\n\n\nSuccess Metrics\n[KPIs]\nHealth Indicators: [Outcome measures]\n\n\nTimeline\n[Project milestones]\nGrant Milestones: [Funder deadlines]\n\n\nBudget\n[Project budget]\nFunding Source: [Grant, appropriation]\n\n\nRisks\n[Project risks]\nImplementation Barriers: [CFIR-informed risks]\n\n\nGovernance\n[Decision structure]\nOversight: [Funder, advisory board]\n\n\n\n\n\n14.1.8 Evaluation Plan Template\n\n\n\nComponent\nContent\n\n\n\n\nEvaluation Questions\nWhat do we want to know?\n\n\nType\nFormative / Summative / Process / Outcome\n\n\nIndicators\nWhat will we measure?\n\n\nData Sources\nWhere will data come from?\n\n\nMethods\nHow will we collect and analyze?\n\n\nTimeline\nWhen will we measure?\n\n\nResponsibilities\nWho will conduct evaluation?\n\n\nUse of Findings\nHow will results inform decisions?\n\n\n\n\n14.1.8.1 RE-AIM Evaluation Matrix\n\n\n\nDimension\nIndicator\nData Source\nTarget\nActual\n\n\n\n\nReach\n\n\n\n\n\n\nEffectiveness\n\n\n\n\n\n\nAdoption\n\n\n\n\n\n\nImplementation\n\n\n\n\n\n\nMaintenance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Templates & Checklists</span>"
    ]
  },
  {
    "objectID": "chapters/B-development-tools.html",
    "href": "chapters/B-development-tools.html",
    "title": "15  Development Tools",
    "section": "",
    "text": "15.1 Development Tools for Toolkit Contributors\nThis appendix documents the tools used to build and maintain the Bridgeframe Toolkit. This information is for contributors who want to modify or extend the toolkit, not for practitioners using the toolkit content.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Development Tools</span>"
    ]
  },
  {
    "objectID": "chapters/B-development-tools.html#development-tools-for-toolkit-contributors",
    "href": "chapters/B-development-tools.html#development-tools-for-toolkit-contributors",
    "title": "15  Development Tools",
    "section": "",
    "text": "15.1.1 Technology Stack\nThe Bridgeframe Toolkit is built with:\n\n\n\nTool\nPurpose\nVersion\n\n\n\n\nQuarto\nDocument publishing system\n1.4+\n\n\nVS Code\nCode editor\nCurrent\n\n\nGitHub Copilot\nAI-assisted development\nCurrent\n\n\nGit/GitHub\nVersion control and hosting\nN/A\n\n\nGitHub Actions\nAutomated publishing\nN/A\n\n\nMermaid\nDiagram generation\nBuilt into Quarto\n\n\n\n\n\n15.1.2 AI-Assisted Development\nGitHub Copilot is used throughout the development of this toolkit for:\n\nRefining ideas: Brainstorming content structure and terminology mappings\nAnalysis: Reviewing and improving BA-PH framework alignments\nCode development: Generating Mermaid diagrams, R scripts, and configuration files\n\n\n15.1.2.1 LLM Models Used\nGitHub Copilot provides access to multiple large language models:\n\n\n\n\n\n\n\n\nProvider\nModels\nPrimary Use\n\n\n\n\nAnthropic\nClaude (Sonnet, Opus)\nComplex analysis, long-form content\n\n\nGoogle\nGemini\nResearch, fact-checking\n\n\nOpenAI\nGPT-4, GPT-4o\nCode generation, quick edits\n\n\n\n\n\n15.1.2.2 Copilot Features\n\nChat: Interactive conversation for complex tasks\nInline suggestions: Real-time code and content completion\nAgent mode: Multi-step tasks with file creation and terminal access\n\n\n\n15.1.2.3 Deep Research Tools\nResearch and analysis is additionally supported by deep research features in:\n\nGoogle Gemini: Extended research capabilities for comprehensive literature review and framework analysis\nOpenAI ChatGPT: Deep research mode for exploring complex BA-PH terminology mappings and implementation science concepts\n\nThese tools complement GitHub Copilot by providing broader research context beyond the immediate codebase.\n\n\n15.1.2.4 Project Context\nThe .github/copilot-instructions.md file provides Copilot with project-specific context including:\n\nRepository structure and conventions\nBA-PH terminology mappings\nCancerSurv case study details\nFormatting standards and style guidelines\n\n\n\n\n15.1.3 Local Development Setup\n\n15.1.3.1 Prerequisites\n\nInstall Quarto: Download from quarto.org\nInstall VS Code: Download from code.visualstudio.com\nInstall Git: Download from git-scm.com\nInstall Quarto VS Code Extension: Search “Quarto” in VS Code extensions\n\n\n\n15.1.3.2 Clone the Repository\ngit clone https://github.com/andre-inter-collab-llc/Bridgeframe-Toolkit.git\ncd Bridgeframe-Toolkit\n\n\n15.1.3.3 Preview Locally\nquarto preview\nThis starts a local server and opens the book in your browser. Changes to .qmd files automatically refresh.\n\n\n15.1.3.4 Render the Book\nquarto render\nOutput is generated in the _book/ directory.\n\n\n\n15.1.4 Repository Structure\nBridgeframe-Toolkit/\n├── _quarto.yml           # Book configuration\n├── _brand.yml            # Branding (colors, fonts, logo)\n├── index.qmd             # Landing page\n├── preface.qmd           # Author preface\n├── references.qmd        # Bibliography\n├── chapters/             # Book chapters\n│   ├── 01-introduction.qmd\n│   ├── ...\n│   └── C-glossary.qmd\n├── assets/\n│   ├── branding/         # Logos, icons, templates\n│   ├── references/       # Bibliography files\n│   └── styles/           # Custom SCSS\n├── .github/\n│   ├── copilot-instructions.md  # AI assistant context\n│   └── workflows/\n│       └── publish.yml   # GitHub Actions workflow\n└── _book/                # Generated output (gitignored)\n\n\n15.1.5 Configuration Files\n\n15.1.5.1 _quarto.yml\nThe main configuration file controls:\n\nBook metadata (title, author, date)\nChapter organization\nOutput formats (HTML, DOCX)\nTheme and styling\nBibliography settings\n\nKey sections:\nproject:\n  type: book\n  output-dir: _book\n\nbook:\n  title: \"Bridgeframe\"\n  chapters:\n    - index.qmd\n    - part: \"Foundations\"\n      chapters:\n        - chapters/01-introduction.qmd\n        # ...\n\nformat:\n  html:\n    theme:\n      - brand\n      - assets/styles/custom.scss\n  docx:\n    reference-doc: assets/branding/templates/IntersectCollab-reference-doc.docx\n\n\n15.1.5.2 _brand.yml\nControls visual branding:\ncolor:\n  palette:\n    blue: \"#2494f7\"\n    teal: \"#00a4bb\"\n    # ...\n  primary: blue\n  secondary: teal\n\ntypography:\n  base: \"Inter\"\n  headings:\n    family: \"Inter\"\n    weight: 800\n  monospace: \"Fira Code\"\n\nlogo:\n  medium: assets/branding/logos/intersect-logo.png\n\n\n\n15.1.6 Writing Content\n\n15.1.6.1 Quarto Markdown Basics\nQuarto uses extended Markdown. Key features:\nHeadings:\n# Chapter Title (H1)\n## Section (H2)\n### Subsection (H3)\nCallout Boxes:\n::: {.callout-note title=\"CancerSurv Example\"}\nContent here...\n:::\n\n::: {.callout-tip}\nTip content...\n:::\n\n::: {.callout-warning}\nWarning content...\n:::\nTables:\n| Column 1 | Column 2 |\n|:---------|:---------|\n| Data 1   | Data 2   |\nCitations:\nAccording to the BABOK [@babok2015], requirements should be...\n\n\n15.1.6.2 Mermaid Diagrams\nDiagrams are created with Mermaid syntax in code blocks:\n```{mermaid}\n%%| label: fig-example\n%%| fig-cap: \"Example Diagram\"\nflowchart LR\n    A[Start] --&gt; B[Process]\n    B --&gt; C[End]\n```\nCommon diagram types:\n\nflowchart: Process flows, architecture\nsequenceDiagram: Interactions over time\nerDiagram: Data models\ngantt: Project timelines\n\n\n\n15.1.6.3 Cross-References\nReference figures, tables, and sections:\nSee @fig-example for the diagram.\nAs discussed in @sec-planning...\n\n\n\n15.1.7 Publishing\n\n15.1.7.1 GitHub Pages Deployment\nThe book automatically publishes to GitHub Pages when changes are pushed to main. The workflow (.github/workflows/publish.yml) handles rendering and deployment.\n\n\n15.1.7.2 Manual Publishing\nTo publish manually:\nquarto publish gh-pages\nThis creates/updates the gh-pages branch with rendered content.\n\n\n15.1.7.3 Initial Setup\nFor first-time setup:\n\nCreate .nojekyll file in repository root (empty file)\nRun quarto publish gh-pages once locally\nCommit the generated _publish.yml file\nEnable GitHub Pages in repository settings (source: gh-pages branch)\n\n\n\n\n15.1.8 Contributing\n\n15.1.8.1 Branch Strategy\n\nmain: Production-ready content\nFeature branches: For new content or significant changes\nPull requests: For review before merging to main\n\n\n\n15.1.8.2 Content Guidelines\n\nFollow the BA-PH dual framing throughout\nInclude CancerSurv examples in callout boxes\nUse tables for terminology mapping\nAvoid em dashes and en dashes; rewrite sentences instead\nEvery .qmd file needs YAML frontmatter with at least a title\n\n\n\n15.1.8.3 Style Guidelines\n\nFirst-person voice only in Preface and personal sections\nProfessional/instructional tone for toolkit content\nAcademic yet accessible language\nGround claims in evidence where possible\n\n\n\n\n15.1.9 Troubleshooting\n\n15.1.9.1 Common Issues\nQuarto preview not updating: - Check for syntax errors in YAML frontmatter - Restart preview with quarto preview\nMermaid diagram not rendering: - Verify syntax at mermaid.live - Check for unsupported features\nGitHub Pages not updating: - Check Actions tab for workflow failures - Verify gh-pages branch exists - Check repository Pages settings\nBibliography not working: - Verify references.bib exists at specified path - Check citation key matches bib entry - Ensure bibliography field in _quarto.yml is correct\n\n\n\n15.1.10 Resources\n\nQuarto Documentation\nQuarto Books Guide\nMermaid Documentation\nGitHub Pages Documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Development Tools</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html",
    "href": "chapters/C-glossary.html",
    "title": "16  Glossary",
    "section": "",
    "text": "16.1 A\nThis glossary provides comprehensive terminology mapping between Business Analysis (BA) and Public Health (PH) domains. Terms are organized alphabetically with cross-references.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#a",
    "href": "chapters/C-glossary.html#a",
    "title": "16  Glossary",
    "section": "",
    "text": "Acceptance Criteria (BA)\n\nConditions that a solution must meet to be accepted by stakeholders. PH equivalent: Evaluation protocol measures, success criteria.\n\nAcceptance Testing (BA)\n\nFormal testing to verify a solution meets acceptance criteria. PH equivalent: Pilot evaluation, field testing.\n\nActivity (PH)\n\nAn action taken as part of a program or intervention. BA equivalent: Functional requirement, use case step.\n\nAdaptability (Implementation Science)\n\nDegree to which an intervention can be modified for local context. BA equivalent: Configurability, customization capability.\n\nAfter-Action Review (PH)\n\nStructured review of what happened, why, and how to improve. BA equivalent: Lessons learned, retrospective.\n\nAgile (BA)\n\nIterative development methodology emphasizing flexibility and stakeholder collaboration. PH parallel: Adaptive management, PDSA cycles.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#b",
    "href": "chapters/C-glossary.html#b",
    "title": "16  Glossary",
    "section": "16.2 B",
    "text": "16.2 B\n\nBacklog (BA/Agile)\n\nPrioritized list of work items. PH equivalent: Workplan, action item list.\n\nBaseline (Both)\n\nStarting point measurement for comparison. In BA, current state metrics. In PH, epidemiological baseline data.\n\nBeneficiary (PH)\n\nPerson or group intended to benefit from a program. BA equivalent: End user, customer.\n\nBPMN (Business Process Model and Notation) (BA)\n\nStandard for process diagrams. PH equivalent: Intervention flowchart, workflow diagram.\n\nBug (BA)\n\nDefect in software. PH equivalent: Adverse event, variance from protocol.\n\nBusiness Case (BA)\n\nJustification for a project based on expected benefits. PH equivalent: Needs assessment, funding proposal.\n\nBusiness Need (BA)\n\nProblem or opportunity driving a project. PH equivalent: Public health challenge, health need.\n\nBusiness Rule (BA)\n\nConstraint governing system behavior. PH equivalent: Clinical guideline, protocol rule.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#c",
    "href": "chapters/C-glossary.html#c",
    "title": "16  Glossary",
    "section": "16.3 C",
    "text": "16.3 C\n\nCapacity Building (PH)\n\nDeveloping skills and resources in individuals and organizations. BA equivalent: Training, organizational readiness.\n\nCase Definition (PH)\n\nCriteria for identifying disease cases. BA equivalent: Data validation rules, entity definition.\n\nCFIR (Consolidated Framework for Implementation Research) (PH)\n\nFramework for understanding implementation context. BA application: Assessing organizational readiness, NFR refinement.\n\nChampion (Both)\n\nPerson who advocates for and promotes an initiative. Usage similar in both domains.\n\nChange Management (BA)\n\nApproach for transitioning organizations to new systems/processes. PH equivalent: Implementation strategy, adoption support.\n\nCommunity Health Assessment (PH)\n\nSystematic examination of health status and needs. BA equivalent: Current state analysis, needs assessment.\n\nCommunity Partner (PH)\n\nExternal organization collaborating on health initiatives. BA equivalent: Stakeholder, vendor, integration partner.\n\nComplexity (Implementation Science)\n\nPerceived difficulty of implementing an intervention. BA equivalent: Usability concerns, training requirements.\n\nCompliance (Both)\n\nAdherence to regulations, standards, or requirements. In PH, often HIPAA, CDC standards. In BA, often regulatory NFRs.\n\nConstraint (BA)\n\nLimitation on solution options. PH equivalent: Policy constraint, resource limitation.\n\nCurrent State (BA)\n\nExisting situation before change. PH equivalent: Baseline, pre-intervention status.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#d",
    "href": "chapters/C-glossary.html#d",
    "title": "16  Glossary",
    "section": "16.4 D",
    "text": "16.4 D\n\nData Dictionary (Both)\n\nDocumentation of data elements, definitions, and formats. Used similarly in both domains.\n\nData Quality (Both)\n\nAccuracy, completeness, and reliability of data. Critical in both domains.\n\nDefect (BA)\n\nFlaw in a deliverable. PH equivalent: Protocol deviation, adverse event.\n\nDeliverable (BA)\n\nOutput of project work. PH equivalent: Program output, product.\n\nDemo (BA/Agile)\n\nPresentation of completed work. PH equivalent: Progress presentation, milestone review.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#e",
    "href": "chapters/C-glossary.html#e",
    "title": "16  Glossary",
    "section": "16.5 E",
    "text": "16.5 E\n\nEffectiveness (PH)\n\nDegree to which an intervention achieves intended outcomes. BA equivalent: Solution value, ROI.\n\nElicitation (BA)\n\nTechniques for gathering requirements from stakeholders. PH equivalent: Community engagement, data collection.\n\nEpic (BA/Agile)\n\nLarge user story spanning multiple sprints. PH equivalent: Grant objective, program goal.\n\nEpidemiological Baseline (PH)\n\nPre-intervention disease/health status data. BA equivalent: Current state metrics.\n\nEvaluation (Both)\n\nAssessment of value, outcomes, or quality. In BA, solution evaluation. In PH, program evaluation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#f",
    "href": "chapters/C-glossary.html#f",
    "title": "16  Glossary",
    "section": "16.6 F",
    "text": "16.6 F\n\nFeasibility (Both)\n\nAssessment of whether something can be done. Technical, economic, operational (BA) or evidence-based, resource, political (PH).\n\nFidelity (PH)\n\nDegree to which an intervention is delivered as designed. BA equivalent: Conformance to specifications.\n\nFocus Group (Both)\n\nGroup discussion for gathering perspectives. Used similarly in both domains.\n\nFunctional Requirement (BA)\n\nWhat a system must do. PH equivalent: Program activity, intervention component.\n\nFuture State (BA)\n\nDesired situation after change. PH equivalent: Program goals, intended outcomes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#g",
    "href": "chapters/C-glossary.html#g",
    "title": "16  Glossary",
    "section": "16.7 G",
    "text": "16.7 G\n\nGemba (Lean/Both)\n\nGoing to the actual place where work happens. Applicable in both BA observation and PH site visits.\n\nGo-Live (BA)\n\nSystem deployment to production. PH equivalent: Program launch, intervention rollout.\n\nGovernance (Both)\n\nDecision-making structure and authority. Similar usage in both domains.\n\nGPS Format (PH-adapted)\n\n“Given [context], Person [role] Should [action]” user story format for clinical contexts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#h",
    "href": "chapters/C-glossary.html#h",
    "title": "16  Glossary",
    "section": "16.8 H",
    "text": "16.8 H\n\nHealth Indicator (PH)\n\nMeasurable characteristic of population health. BA equivalent: KPI, metric.\n\nHealth Information Exchange (HIE) (PH)\n\nElectronic sharing of health data. BA equivalent: System integration, data exchange.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#i",
    "href": "chapters/C-glossary.html#i",
    "title": "16  Glossary",
    "section": "16.9 I",
    "text": "16.9 I\n\nImpact (PH)\n\nLong-term effects of an intervention. BA equivalent: Business value, strategic outcomes.\n\nImplementation (Both)\n\nPutting a solution or intervention into practice. Similar usage.\n\nImplementation Climate (CFIR)\n\nOrganizational receptivity to change. BA equivalent: Organizational readiness.\n\nImplementation Science (PH)\n\nStudy of methods to promote adoption of evidence-based practices. BA application: Change management, adoption strategy.\n\nIndicator (PH)\n\nMeasurable variable reflecting status or change. BA equivalent: Metric, KPI.\n\nInner Setting (CFIR)\n\nInternal organizational context. BA equivalent: Organizational environment, culture.\n\nInput (PH Logic Model)\n\nResources invested in a program. BA equivalent: Resources, constraints.\n\nIntegration (BA)\n\nConnecting systems to work together. PH equivalent: Interoperability, HIE.\n\nInterest Holder (PH)\n\nPerson or group with interest in a program. BA equivalent: Stakeholder.\n\nIntervention (PH)\n\nAction taken to improve health. BA equivalent: Solution, system, process change.\n\nIteration (BA/Agile)\n\nFixed time period for development work. PH equivalent: PDSA cycle, program phase.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#k",
    "href": "chapters/C-glossary.html#k",
    "title": "16  Glossary",
    "section": "16.10 K",
    "text": "16.10 K\n\nKey Informant (PH)\n\nPerson with specialized knowledge consulted for input. BA equivalent: Subject matter expert (SME).\n\nKPI (Key Performance Indicator) (BA)\n\nMetric measuring success. PH equivalent: Health indicator, outcome measure.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#l",
    "href": "chapters/C-glossary.html#l",
    "title": "16  Glossary",
    "section": "16.11 L",
    "text": "16.11 L\n\nLessons Learned (BA)\n\nKnowledge gained from experience. PH equivalent: After-action review findings.\n\nLogic Model (PH)\n\nVisual representation of program theory (inputs → activities → outputs → outcomes). BA equivalent: Requirements traceability, value chain.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#m",
    "href": "chapters/C-glossary.html#m",
    "title": "16  Glossary",
    "section": "16.12 M",
    "text": "16.12 M\n\nMaintenance (RE-AIM)\n\nSustainability of an intervention over time. BA equivalent: Operational sustainability.\n\nMilestone (Both)\n\nSignificant point in project timeline. In PH, often aligned with grant reporting.\n\nMVP (Minimum Viable Product) (BA/Agile)\n\nSmallest useful version of a solution. PH equivalent: Pilot intervention, proof of concept.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#n",
    "href": "chapters/C-glossary.html#n",
    "title": "16  Glossary",
    "section": "16.13 N",
    "text": "16.13 N\n\nNeeds Assessment (PH)\n\nSystematic identification of needs and gaps. BA equivalent: Business analysis, current state assessment.\n\nNFR (Non-Functional Requirement) (BA)\n\nQuality attribute or constraint. PH equivalent: Implementation characteristic (CFIR).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#o",
    "href": "chapters/C-glossary.html#o",
    "title": "16  Glossary",
    "section": "16.14 O",
    "text": "16.14 O\n\nOutcome (PH)\n\nChange resulting from an intervention. BA equivalent: Benefit, value delivered.\n\nOuter Setting (CFIR)\n\nExternal context (regulations, networks, peer pressure). BA equivalent: External environment, market forces.\n\nOutput (PH Logic Model)\n\nDirect products of program activities. BA equivalent: Deliverables, features.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#p",
    "href": "chapters/C-glossary.html#p",
    "title": "16  Glossary",
    "section": "16.15 P",
    "text": "16.15 P\n\nPDSA (Plan-Do-Study-Act) (PH)\n\nQuality improvement cycle. BA equivalent: Sprint/iteration, continuous improvement cycle.\n\nPilot (Both)\n\nSmall-scale test of a solution or intervention. Similar usage.\n\nProcess Evaluation (PH)\n\nAssessment of how an intervention was implemented. BA equivalent: Implementation review.\n\nProgram (PH)\n\nOrganized set of activities to achieve health objectives. BA equivalent: Solution, system, project.\n\nProtocol (PH)\n\nStandardized procedure or guideline. BA equivalent: Business rule, procedure specification.\n\nPrototype (BA)\n\nEarly model for testing concepts. PH equivalent: Pilot, formative testing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#q",
    "href": "chapters/C-glossary.html#q",
    "title": "16  Glossary",
    "section": "16.16 Q",
    "text": "16.16 Q\n\nQuality Assurance (QA) (BA)\n\nSystematic quality activities. PH equivalent: Quality Improvement (QI).\n\nQuality Improvement (QI) (PH)\n\nContinuous improvement of processes and outcomes. BA equivalent: Continuous improvement, QA.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#r",
    "href": "chapters/C-glossary.html#r",
    "title": "16  Glossary",
    "section": "16.17 R",
    "text": "16.17 R\n\nRE-AIM (PH)\n\nFramework for evaluating public health impact (Reach, Effectiveness, Adoption, Implementation, Maintenance).\n\nReach (RE-AIM)\n\nProportion of target population participating. BA equivalent: Adoption rate, market penetration.\n\nReadiness (Both)\n\nPreparedness for change. Implementation readiness (PH) or organizational readiness (BA).\n\nRelative Advantage (CFIR)\n\nPerception that intervention is better than current practice. BA equivalent: Value proposition.\n\nRequirements (BA)\n\nConditions a solution must satisfy. PH equivalent: Program specifications, protocols.\n\nRetrospective (BA/Agile)\n\nMeeting to reflect on past work. PH equivalent: After-action review.\n\nRights Holder (PH)\n\nPerson with inherent claims (alternative to “stakeholder”). BA equivalent: Stakeholder (with different connotation).\n\nRisk (Both)\n\nPotential for adverse outcomes. Similar usage; PH may emphasize community risk.\n\nROI (Return on Investment) (BA)\n\nFinancial value relative to cost. PH equivalent: Cost-effectiveness, cost-benefit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#s",
    "href": "chapters/C-glossary.html#s",
    "title": "16  Glossary",
    "section": "16.18 S",
    "text": "16.18 S\n\nScope (Both)\n\nBoundaries of what is included. Similar usage in both domains.\n\nService-User Scenario (PH-adapted)\n\nNarrative description of user journey. BA equivalent: User story, use case narrative.\n\nSME (Subject Matter Expert) (BA)\n\nPerson with domain expertise. PH equivalent: Key informant, clinical expert.\n\nSolution (BA)\n\nSystem or process addressing a business need. PH equivalent: Intervention, program.\n\nSprint (BA/Agile)\n\nFixed-length iteration. PH equivalent: PDSA cycle, work period.\n\nStakeholder (BA)\n\nPerson with interest in a project. PH alternatives: Interest holder, community partner, rights holder.\n\nSummative Evaluation (PH)\n\nAssessment of overall outcomes after implementation. BA equivalent: Post-implementation review.\n\nSustainability (PH)\n\nAbility to maintain intervention over time. BA equivalent: Operational viability.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#t",
    "href": "chapters/C-glossary.html#t",
    "title": "16  Glossary",
    "section": "16.19 T",
    "text": "16.19 T\n\nTest Case (BA)\n\nSpecification for verifying a requirement. PH equivalent: Evaluation measure, data collection protocol.\n\nTraceability (BA)\n\nLinking requirements to sources and tests. PH equivalent: Theory of change alignment.\n\nTraining (Both)\n\nBuilding user/staff capability. Similar usage; PH may use “capacity building.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#u",
    "href": "chapters/C-glossary.html#u",
    "title": "16  Glossary",
    "section": "16.20 U",
    "text": "16.20 U\n\nUAT (User Acceptance Testing) (BA)\n\nStakeholder verification of solution. PH equivalent: Pilot evaluation, field testing.\n\nUse Case (BA)\n\nDescription of system-user interaction. PH equivalent: Clinical scenario, patient journey.\n\nUser Story (BA/Agile)\n\nBrief requirement from user perspective. PH equivalents: Service-user scenario, GPS format.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#v",
    "href": "chapters/C-glossary.html#v",
    "title": "16  Glossary",
    "section": "16.21 V",
    "text": "16.21 V\n\nValidation (Both)\n\nConfirming the right thing is built. Similar usage.\n\nVariance (PH)\n\nDeviation from expected outcome. BA equivalent: Defect, exception.\n\nVerification (Both)\n\nConfirming the thing is built right. Similar usage.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#w",
    "href": "chapters/C-glossary.html#w",
    "title": "16  Glossary",
    "section": "16.22 W",
    "text": "16.22 W\n\nWorkflow (Both)\n\nSequence of tasks to accomplish work. Similar usage.\n\nWorkplan (PH)\n\nDetailed plan of activities. BA equivalent: Project plan, backlog.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/C-glossary.html#quick-reference-most-common-translations",
    "href": "chapters/C-glossary.html#quick-reference-most-common-translations",
    "title": "16  Glossary",
    "section": "16.23 Quick Reference: Most Common Translations",
    "text": "16.23 Quick Reference: Most Common Translations\n\n\n\n\n\n\n\n\nWhen You Hear…\nBA Meaning\nPH Meaning\n\n\n\n\n“Requirements”\nSystem specifications\nProgram protocols\n\n\n“Stakeholder”\nAnyone with interest\nCommunity partner, rights holder\n\n\n“Sprint”\n2-week development cycle\nPDSA cycle\n\n\n“User story”\nFeature description\nService-user scenario\n\n\n“KPI”\nBusiness metric\nHealth indicator\n\n\n“MVP”\nMinimal product\nPilot intervention\n\n\n“Go-live”\nSystem deployment\nProgram launch\n\n\n“Bug”\nSoftware defect\nProtocol variance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "chapters/D-workforce-development.html",
    "href": "chapters/D-workforce-development.html",
    "title": "17  Workforce Development",
    "section": "",
    "text": "17.1 Building Hybrid Professionals and Teams\nThe translation gap between IT business analysis and public health is not just a process problem or a terminology problem. It is fundamentally a workforce problem. Both government agencies and private industry need professionals who can operate fluently in both worlds, yet most training programs still operate in silos.\nThis appendix explores the challenge, highlights emerging resources, and suggests strategies for building hybrid capacity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Workforce Development</span>"
    ]
  },
  {
    "objectID": "chapters/D-workforce-development.html#building-hybrid-professionals-and-teams",
    "href": "chapters/D-workforce-development.html#building-hybrid-professionals-and-teams",
    "title": "17  Workforce Development",
    "section": "",
    "text": "17.1.1 The Training Silo Problem\nConsider the typical career paths:\nPublic Health Path:\n\nMPH or equivalent graduate training\nCoursework in epidemiology, biostatistics, health policy\nRarely covers: Agile methodology, BABOK, software development lifecycle, requirements engineering\n\nBusiness Analysis Path:\n\nBusiness or IT undergraduate degree\nCBAP, PMI-PBA, or similar certification\nRarely covers: Epidemiology, CDC frameworks, public health ethics, community engagement methods\n\nWhen these professionals meet on a health IT project, each brings deep expertise in their domain but limited fluency in the other’s language, frameworks, and assumptions. The result is the translation gap this book addresses.\n\n\n\n\n\n\nNoteInsight from the Field\n\n\n\nThe lack of skilled hybrid workforce and limited resources to hire such professionals are two significant limiting factors in bridging this gap. Organizations often recognize the need but struggle to find candidates or justify positions that span traditional departmental boundaries.\n\n\n\n\n17.1.2 Emerging Training Programs\nSeveral initiatives are working to close this gap:\n\n17.1.2.1 CDC Public Health Informatics Fellowship Program (PHIFP)\nThe Public Health Informatics Fellowship Program trains professionals specifically for this intersection. Fellows work on real public health informatics projects while developing competencies in both public health practice and information systems.\nThe program recruits from diverse backgrounds and provides structured mentorship bridging technical and programmatic domains. Since its establishment, PHIFP has produced many of the hybrid professionals now leading health IT initiatives across state and local health departments.\n\n\n17.1.2.2 PHIT Workforce Development Program\nThe Public Health Informatics & Technology (PHIT) Workforce Development Program, funded by the Office of the National Coordinator for Health IT (ONC), focuses on training diverse professionals in health informatics. The program emphasizes recruiting from underrepresented communities, recognizing that the hybrid workforce should reflect the populations public health serves.\n\n\n17.1.2.3 AMIA Public Health Informatics Working Group\nThe American Medical Informatics Association (AMIA) hosts an active Public Health Informatics Working Group with members from state health departments, academic institutions, nonprofits, and consulting firms. The working group:\n\nDevelops competency frameworks for public health informatics\nShares best practices across organizations\nAdvocates for workforce development resources\nConnects professionals navigating similar challenges\n\nFor those seeking community and professional development in this space, AMIA membership provides access to peers tackling the same translation challenges.\n\n\n17.1.2.4 Other Resources\n\n\n\nResource\nFocus\nAccess\n\n\n\n\nCDC TRAIN\nFree public health courses, including informatics fundamentals\ntrain.org\n\n\nAMIA 10x10\nIntensive health informatics courses\namia.org\n\n\nCAHIMS / CPHIMS\nHealth IT certifications from HIMSS\nhimss.org\n\n\nCoursera / edX\nPublic health informatics specializations (Johns Hopkins, UCSF)\nOnline\n\n\n\n\n\n\n17.1.3 Organizational Strategies for Building Hybrid Capacity\nBeyond individual training, organizations can take structural steps to build translation capability:\n\n17.1.3.1 1. Create Explicit Bridge Roles\nRather than expecting traditional BAs or PH staff to develop hybrid skills on their own, create positions specifically designed for translation:\n\nHealth Informatics Liaison: Embedded in IT teams but reporting to public health leadership\nTechnical Program Analyst: Embedded in public health programs but with explicit BA responsibilities\nImplementation Specialist: Focused on adoption and change management across both domains\n\nThese roles should have:\n\nDual reporting or matrix accountability\nPerformance metrics that span technical and programmatic outcomes\nProfessional development budgets for cross-training\n\n\n\n17.1.3.2 2. Invest in Onboarding Crosswalks\nWhen new team members join hybrid projects, provide structured onboarding that covers both domains:\n\nTerminology glossaries (like Chapter 3 of this book)\nFramework overviews (BABOK basics for PH staff; CDC evaluation basics for BA staff)\nProject-specific crosswalks mapping deliverables to both languages\n\nThis upfront investment reduces the clarification loops that consume project time later.\n\n\n17.1.3.3 3. Include Translation Capacity in Grant Budgets\nAs discussed in Chapter 1, the business case stage is the ideal time to secure resources for cross-domain facilitation. When writing grant applications or project proposals:\n\nInclude line items for terminology alignment workshops\nBudget for cross-training or professional development\nAllocate facilitation time for mixed-team meetings\nConsider consultants or contractors with hybrid backgrounds\n\n\n\n17.1.3.4 4. Build Communities of Practice\nInternal communities of practice can connect professionals across organizational silos:\n\nRegular brown-bag sessions where IT and PH staff present to each other\nShared Slack/Teams channels for quick translation questions\nJoint retrospectives that surface communication challenges\nRotating assignments that expose staff to the other domain\n\n\n\n\n17.1.4 Competencies for the Hybrid Professional\nWhat should a hybrid BA/PH professional be able to do? Based on established competency frameworks and practical experience, key capabilities include:\n\n17.1.4.1 Core Translation Competencies\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nDual-framework thinking\nMap Agile artifacts to Logic Model outcomes and vice versa\n\n\nTerminology fluency\nSpeak both languages without constant translation pauses\n\n\nAudience adaptation\nAdjust communication style for technical vs. programmatic audiences\n\n\nFramework selection\nKnow when to use BA tools vs. PH tools vs. hybrid approaches\n\n\n\n\n\n17.1.4.2 Technical Competencies (BA Side)\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nRequirements engineering\nElicit, analyze, specify, and validate requirements\n\n\nProcess modeling\nCreate BPMN, UML, or similar diagrams\n\n\nAgile practices\nParticipate effectively in sprints, stand-ups, retrospectives\n\n\nData modeling\nUnderstand ERDs, data dictionaries, database concepts\n\n\n\n\n\n17.1.4.3 Technical Competencies (PH Side)\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nEpidemiological thinking\nUnderstand surveillance, case definitions, health indicators\n\n\nProgram evaluation\nApply CDC evaluation framework, logic models, PDSA\n\n\nRegulatory knowledge\nNavigate HIPAA, IRB, grant compliance requirements\n\n\nHealth equity lens\nIdentify disparities, ensure inclusive engagement\n\n\n\n\n\n17.1.4.4 Implementation Science Competencies\n\n\n\n\n\n\n\nCompetency\nDescription\n\n\n\n\nCFIR application\nAssess implementation context using CFIR domains\n\n\nChange management\nPlan for adoption, not just deployment\n\n\nStakeholder engagement\nBuild relationships across organizational boundaries\n\n\nSustainability planning\nDesign for long-term operation, not just project completion\n\n\n\n\n\n\n17.1.5 Career Pathways\nFor professionals seeking to develop hybrid capabilities, several pathways exist:\n\n17.1.5.1 From Public Health to Hybrid\n\nSeek projects involving health IT systems (surveillance, registries, EHRs)\nRequest assignment to vendor-managed implementations\nPursue CAHIMS or similar health IT certification\nTake online courses in Agile, requirements engineering, or project management\nJoin AMIA or similar professional communities\n\n\n\n17.1.5.2 From Business Analysis to Hybrid\n\nSeek health sector clients or employers\nLearn public health fundamentals (CDC TRAIN offers free courses)\nStudy regulatory context (HIPAA, CDC reporting requirements)\nUnderstand grant-driven funding cycles and constraints\nDevelop cultural competency for community engagement\n\n\n\n17.1.5.3 Academic Pathways\n\nDual degrees: MPH + MBA, MPH + MIS\nHealth informatics programs: Specifically designed for this intersection\nGraduate certificates: Add informatics credentials to existing degrees\n\n\n\n\n17.1.6 The Long View\nBuilding hybrid workforce capacity is a long-term investment. Individual professionals take years to develop fluency in both domains. Organizational cultures take even longer to change.\nBut the need is urgent. Public health IT projects will continue to grow in scope and complexity. The professionals and organizations that build translation capacity now will be better positioned to deliver systems that actually improve health outcomes.\nBridgeframe is one contribution to this effort: a crosswalk that teams can use while the workforce develops. But crosswalks are not substitutes for people. The field needs more hybrid professionals, more training programs, and more organizational commitment to building this capacity.\n\n\n\n\n\n\nTipGet Involved\n\n\n\nIf you are working on workforce development for health informatics, or have experience building hybrid teams, consider contributing to this conversation. The challenges described in this appendix are shared across many organizations, and solutions developed in one context often transfer to others.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Workforce Development</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "18  References",
    "section": "",
    "text": "References\n\n\n1. Iiba. Babok: A Guide\nto the Business Analysis Body of\nKnowledge. International Institute of Business Analysis;\n2015. \n\n\n2. CDC.\nCDC Program Evaluation\nFramework [Internet]. CDC Approach to Program Evaluation.\n2025 [cited 2026 Jan 9]. Available from: https://www.cdc.gov/evaluation/php/evaluation-framework/index.html\n\n\n3. Kidder DP. CDC\nProgram Evaluation Framework,\n2024. MMWR Recommendations and Reports [Internet]. 2024 [cited 2026 Jan\n9];73. Available from: https://www.cdc.gov/mmwr/volumes/73/rr/rr7306a1.htm\n\n\n4. 1999\n– RE-AIM [Internet]. [cited 2026 Jan 9].\nAvailable from: https://re-aim.org/1999/\n\n\n5. RE-AIM –\nHome – Reach Effectiveness\nAdoption Implementation\nMaintenance [Internet]. [cited 2026 Jan 9]. Available from:\nhttps://re-aim.org/",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>References</span>"
    ]
  }
]